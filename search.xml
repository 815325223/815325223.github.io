<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kubernetes HA]]></title>
    <url>%2F2019%2F05%2F07%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2Kubernetes-HA%2F</url>
    <content type="text"><![CDATA[&#9742; 软件安装版本预览 Kubernetes v1.13.5 (v1.13.4有kubectl cp的bug)CNI v0.7.5Etcd v3.2.24Flannel v0.11.0 or Calico v3.4Docker CE 18.06.03 在官方的支持版本里，1.13.5并未支持18.09，为此这里使用的是18.06.03。 网络信息 Cluster IP CIDR: 10.244.0.0/16Service Cluster IP CIDR: 10.96.0.0/12Service DNS IP: 10.96.0.10DNS DN: cluster.localKubernetes API VIP: 10.0.6.155Kubernetes Ingress VIP: 10.0.6.156 节点信息 IP Hostname CPU Memory 10.0.6.166 k8s-m1 2 2G 10.0.6.167 k8s-m2 2 2G 10.0.6.168 k8s-m3 2 2G 10.0.6.169 k8s-n1 2 3G 10.0.6.170 k8s-n2 2 3G 另外vip为10.0.6.155，由所有master节点的keepalived+haproxy来选择vip的归属保持高可用。 高可用一般建议为3、5、7台的奇数台 准备工作 所有机器网络互通，并且k8s-m1可以免密码登陆。 所有防火墙和selinux关闭，否则k8s挂载目录时会报错Permission denied。 123systemctl disable --now firewalld NetworkManagersetenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config 如果为GUI环境的Linux，dns server地址为127.0.0.1，会导致docker无法解析域名。 1systemctl disable --now dnsmasq Kubernetes v1.8后，要求关闭系统swap，若不关闭的话，则需要修改kubelet参数来忽略swap on，–fail-swap-on=false，关闭所有机器的swap。 12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab 选择1：升级至保守内核 123yum install epel-release -yyum update -yreboot 选择2：升级至较新的内核（市面上发行版linux内核版本普遍较低，4.15都存在一些bug） 1kernel:unregister_netdevice: waiting for lo to become free. Usage count = 1 安装elrepo源，导入key和查看可安装的内核版本 123rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpmyum --disablerepo="*" --enablerepo="elrepo-kernel" list available --showduplicates 显示mainline为最新版本的内核。 最新版本内核安装 12yum --disablerepo="*" --enablerepo="elrepo-kernel" list available --showduplicates | grep -Po '^kernel-ml.x86_64\s+\K\S+(?=.el7)'yum --disablerepo="*" --enablerepo=elrepo-kernel install -y kernel-ml&#123;,-devel&#125; 自定义版本内核安装 123export Kernel_Version=4.18.9-1wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml&#123;,-devel&#125;-$&#123;Kernel_Version&#125;.el7.elrepo.x86_64.rpmyum localinstall -y kernel-ml* 修改内核启动顺序，并验证是否为默认内核 12grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfggrubby --default-kernel 开启user namespace，并重启加载新内核 12grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"reboot 安装ipvs 1yum install ipvsadm ipset sysstat conntrack libseccomp -y 加载内核模块并设置开机自动加载 ipvs依赖于nf_conntrack_ipv4内核模块，4.19包括之后内核里改名为nf_conntrack，1.13.1之前的 kube-proxy的代码里没有加判断，一直用的nf_conntrack_ipv4，1.13.1后的kube-proxy代码里增加了判断，可以顺利加载nf_conntrack，使用ipvs正常。 12345678910111213:&gt; /etc/modules-load.d/ipvs.confmodule=(ip_vsip_vs_rrip_vs_wrrip_vs_shnf_conntrackbr_netfilter)for kernel_module in $&#123;module[@]&#125;;do/sbin/modinfo -F filename $kernel_module |&amp; grep -qv ERROR &amp;&amp; echo $kernel_module &gt;&gt; /etc/modules-load.d/ipvs.conf || :donesystemctl enable --now systemd-modules-load.service 针对Kubernetes的参数调优 12345678910111213141516171819202122232425262728293031323334cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.tcp_keepalive_time = 600# 修复ipvs模式下长连接timeout问题 小于900即可net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1net.ipv4.neigh.default.gc_stale_time = 120net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.default.arp_announce = 2net.ipv4.conf.lo.arp_announce = 2net.ipv4.conf.all.arp_announce = 2net.ipv4.ip_forward = 1net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 1024net.ipv4.tcp_synack_retries = 2# 要求iptables不对bridge的数据进行处理net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-arptables = 1net.netfilter.nf_conntrack_max = 2310720fs.inotify.max_user_watches=89100fs.may_detach_mounts = 1fs.file-max = 52706963fs.nr_open = 52706963vm.swappiness = 0vm.overcommit_memory=1vm.panic_on_oom=0EOFsysctl --system 检查Docker安装必要条件 12 curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh &gt; check-config.shbash ./check-config.sh 安装Docker 在Kubernetes官方https://github.com/kubernetes/kubernetes/ 的对应版本changelog里搜索The list of validated docker versions remain查看支持的docker版本。 12export VERSION=18.06curl -fsSL "https://get.docker.com/" | bash -s -- --mirror Aliyun 配置Docker国内加速源并设置systemd启动参数 12345678910111213141516mkdir -p /etc/docker/cat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; "exec-opts": ["native.cgroupdriver=systemd"], "registry-mirrors": ["https://fz5yth0r.mirror.aliyuncs.com"], "storage-driver": "overlay2", "storage-opts": [ "overlay2.override_kernel_check=true" ], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m", "max-file": "3" &#125;&#125;EOF 设置Docker开机启动和命令补全 123yum install -y bash-completioncp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/systemctl enable --now docker 申明集群环境变量 为了后续安装方便，避免大量重复，根据自己环境声明环境变量，建议写入文件，避免ssh断开导致变量丢失后可以source。默认kubelet向集群注册时，使用hostname或者–hostname-override选项去注册信息。haproxy占用每台机器的8443端口去负载到每台master上的api-server的6443端口，keepalived保证vip飘在可用的master上所有管理组件和kubelet都会去访问vip:8443，即使某一台master宕机，也能访问api-server。如果云上环境建议使用LB代替haproxy和keepalived，vip选择同一个局域网未使用的ip。12345678910111213141516171819202122232425# 声明集群成员信息declare -A MasterArray otherMaster NodeArray AllNode OtherMasterArray=(['k8s-m1']=10.0.6.166 ['k8s-m2']=10.0.6.167 ['k8s-m3']=10.0.6.168)otherMaster=(['k8s-m2']=10.0.6.167 ['k8s-m3']=10.0.6.168)NodeArray=(['k8s-n1']=10.0.6.169 ['k8s-n2']=10.0.6.170)# 下面复制上面的信息粘贴即可AllNode=(['k8s-m1']=10.0.6.166 ['k8s-m2']=10.0.6.167 ['k8s-m3']=10.0.6.168 ['k8s-n1']=10.0.6.169 ['k8s-n2']=10.0.6.170)Other=(['k8s-m2']=10.0.6.167 ['k8s-m3']=10.0.6.168 ['k8s-n1']=10.0.6.169 ['k8s-n2']=10.0.6.170)export VIP=10.0.6.155[ "$&#123;#MasterArray[@]&#125;" -eq 1 ] &amp;&amp; export VIP=$&#123;MasterArray[@]&#125; || export API_PORT=8443export KUBE_APISERVER=https://$&#123;VIP&#125;:$&#123;API_PORT:=6443&#125;#声明需要安装的的k8s版本export KUBE_VERSION=v1.13.5# 网卡名export interface=eth0# cniexport CNI_URL="https://github.com/containernetworking/plugins/releases/download"export CNI_VERSION=v0.7.5# etcdexport ETCD_version=v3.2.24 设置所有机器的hostname 1234for name in $&#123;!AllNode[@]&#125;;do echo "--- $name $&#123;AllNode[$name]&#125; ---" ssh $&#123;AllNode[$name]&#125; "hostnamectl set-hostname $name"done 在k8s-m1通过git获取相关二进制配置文件和yaml 12git clone https://github.com/zhangguanzhang/k8s-manual-files.git ~/k8s-manual-files -b v1.13.4cd ~/k8s-manual-files/ 无需翻墙获取所有二进制文件 12345cd ~/k8s-manual-files/docker pull zhangguanzhang/k8s_bin:$KUBE_VERSION-fulldocker run --rm -d --name temp zhangguanzhang/k8s_bin:$KUBE_VERSION-full sleep 10docker cp temp:/kubernetes-server-linux-amd64.tar.gz .tar -zxvf kubernetes-server-linux-amd64.tar.gz --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; 分发master相关组件的二进制文件到其他master上 1234for NODE in "$&#123;!otherMaster[@]&#125;"; do echo "--- $NODE $&#123;otherMaster[$NODE]&#125; ---" scp /usr/local/bin/kube&#123;let,ctl,-apiserver,-controller-manager,-scheduler,-proxy&#125; $&#123;otherMaster[$NODE]&#125;:/usr/local/bin/ done 分发node相关组件的二进制文件到所有node上 1234for NODE in "$&#123;!NodeArray[@]&#125;"; do echo "--- $NODE $&#123;NodeArray[$NODE]&#125; ---" scp /usr/local/bin/kube&#123;let,-proxy&#125; $&#123;NodeArray[$NODE]&#125;:/usr/local/bin/ done 分发CNI到其他机器 12345678mkdir -p /opt/cni/binwget "$&#123;CNI_URL&#125;/$&#123;CNI_VERSION&#125;/cni-plugins-amd64-$&#123;CNI_VERSION&#125;.tgz" tar -zxf cni-plugins-amd64-$&#123;CNI_VERSION&#125;.tgz -C /opt/cni/binfor NODE in "$&#123;!Other[@]&#125;"; do echo "--- $NODE $&#123;Other[$NODE]&#125; ---" ssh $&#123;Other[$NODE]&#125; 'mkdir -p /opt/cni/bin' scp /opt/cni/bin/* $&#123;Other[$NODE]&#125;:/opt/cni/bin/done 建立集群CA keys 与Certificates在这个部分，需要生成多个组件的Certificates，包含etcd、kubernetes组件等。并且每个集群都会有一个根数位凭证认证机构（Root Certificate Authority）被用在认证API Server与kubelet端的凭证。 注意的是CA中的CN（Common Name）与O（Organization）等内容是会影响Kubernetes组件认证的。CN，apiserver会从证书中提取该字段作为请求的用户名（UserName）O，apiserver会从证书中提取该字段作为请求用户所属的组（Group）CA是自签名的根证书，用来签名后续创建的其他证书以下所有证书都由openssl创建 etcd可以使用官方提供的在线工具生成，http://play.etcd.io 准备openssl证书的配置文件 注入IP信息 123456mkdir -p /etc/kubernetes/pki/etcdsed -i "/IP.2/a IP.3 = $VIP" ~/k8s-manual-files/pki/openssl.cnfsed -ri '/IP.3/r '&lt;( paste -d '' &lt;(seq -f 'IP.%g = ' 4 $[$&#123;#AllNode[@]&#125;+3]) &lt;(xargs -n1&lt;&lt;&lt;$&#123;AllNode[@]&#125; | sort) ) ~/k8s-manual-files/pki/openssl.cnfsed -ri '$r '&lt;( paste -d '' &lt;(seq -f 'IP.%g = ' 2 $[$&#123;#MasterArray[@]&#125;+1]) &lt;(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort) ) ~/k8s-manual-files/pki/openssl.cnfcp ~/k8s-manual-files/pki/openssl.cnf /etc/kubernetes/pki/cd /etc/kubernetes/pki 生成证书 path Default CN description ca.crt,key kubernetes-ca kubernetes general CA etcd/ca.crt,key etcd-ca For all etcd-related functions front-proxy-ca.crt,key kubernetes-front-proxy-ca For the front-end proxy kubernetes-ca 12openssl genrsa -out ca.key 2048openssl req -x509 -new -nodes -key ca.key -config openssl.cnf -subj "/CN=kubernetes-ca" -extensions v3_ca -out ca.crt -days 10000 etcd-ca 12openssl genrsa -out etcd/ca.key 2048openssl req -x509 -new -nodes -key etcd/ca.key -config openssl.cnf -subj "/CN=etcd-ca" -extensions v3_ca -out etcd/ca.crt -days 10000 front-proxy-ca 12openssl genrsa -out front-proxy-ca.key 2048openssl req -x509 -new -nodes -key front-proxy-ca.key -config openssl.cnf -subj "/CN=kubernetes-ca" -extensions v3_ca -out front-proxy-ca.crt -days 10000 生成所有的证书信息 Default CN Parent CA O（in Subject） kind kube-etcd etcd-ca server,client kube-etcd-peer etcd-ca server,client kube-etcd-healthcheck-client etcd-ca client kube-apiserver-etcd-client etcd-ca system:masters client kube-apiserver kubernetes-ca server kube-apiserver-kubelet-client kubernetes-ca system:masters client front-proxy-client kubernetes-front-proxy-ca client 证书路径 Default CN recommend key path recommended command key argument cert argument etcd-ca etcd/ca.crt kube-apiserver -etcd-cafile etcd-client apiserver-etcd-client.key apiserver-etcd-client.crt kube-apiserver -etcd-keyfile -etcd-certfile kubernetes-ca ca.crt kube-apiserver -client-ca-file kube-apiserver apiserver.key apiserver.crt kube-apiserver –tls-private-key-file –tls-cert-file apiserver-kubelet-client apiserver-kubelet-client.crt kube-apiserver –kubelet-client-certificate front-proxy-ca front-proxy-ca.crt kube-apiserver –requestheader-client-ca-file front-proxy-client front-proxy-client.key front-proxy-client.crt kube-apiserver –proxy-client-key-file –proxy-client-cert-file etcd-ca etcd/ca.crt etcd –trusted-ca-file,–peer-trusted-ca-file kube-etcd etcd/server.key etcd/server.crt etcd –key-file –cert-file kube-etcd-peer etcd/peer.key etcd/peer.crt etcd –peer-key-file –peer-cert-file etcd-ca etcd/ca.crt etcdctl[2] –cacert kube-etcd-healthcheck-client etcd/healthcheck-client.key etcd/healthcheck-client.crt etcdctl[2] –key –cert 生成证书 apiserver-etcd-client123openssl genrsa -out apiserver-etcd-client.key 2048openssl req -new -key apiserver-etcd-client.key -subj "/CN=apiserver-etcd-client/O=system:masters" -out apiserver-etcd-client.csropenssl x509 -in apiserver-etcd-client.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out apiserver-etcd-client.crt -days 10000 kube-etcd123openssl genrsa -out etcd/server.key 2048openssl req -new -key etcd/server.key -subj "/CN=etcd-server" -out etcd/server.csropenssl x509 -in etcd/server.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/server.crt -days 10000 kube-etcd-peer123openssl genrsa -out etcd/peer.key 2048openssl req -new -key etcd/peer.key -subj "/CN=etcd-peer" -out etcd/peer.csropenssl x509 -in etcd/peer.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/peer.crt -days 10000 kube-etcd-healthcheck-client123openssl genrsa -out etcd/healthcheck-client.key 2048openssl req -new -key etcd/healthcheck-client.key -subj "/CN=etcd-client" -out etcd/healthcheck-client.csropenssl x509 -in etcd/healthcheck-client.csr -req -CA etcd/ca.crt -CAkey etcd/ca.key -CAcreateserial -extensions v3_req_etcd -extfile openssl.cnf -out etcd/healthcheck-client.crt -days 10000 kube-apiserver123openssl genrsa -out apiserver.key 2048openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -config openssl.cnf -out apiserver.csropenssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req_apiserver -extfile openssl.cnf -out apiserver.crt apiserver-kubelet-client123openssl genrsa -out apiserver-kubelet-client.key 2048openssl req -new -key apiserver-kubelet-client.key -subj "/CN=apiserver-kubelet-client/O=system:masters" -out apiserver-kubelet-client.csropenssl x509 -req -in apiserver-kubelet-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req_client -extfile openssl.cnf -out apiserver-kubelet-client.crt front-proxy-client123openssl genrsa -out front-proxy-client.key 2048openssl req -new -key front-proxy-client.key -subj "/CN=front-proxy-client" -out front-proxy-client.csropenssl x509 -req -in front-proxy-client.csr -CA front-proxy-ca.crt -CAkey front-proxy-ca.key -CAcreateserial -days 10000 -extensions v3_req_client -extfile openssl.cnf -out front-proxy-client.crt kube-scheduler123openssl genrsa -out kube-scheduler.key 2048openssl req -new -key kube-scheduler.key -subj "/CN=system:kube-scheduler" -out kube-scheduler.csropenssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req_client -extfile openssl.cnf -out kube-scheduler.crt sa.pub sa.key12345openssl genrsa -out sa.key 2048openssl ecparam -name secp521r1 -genkey -noout -out sa.keyopenssl ec -in sa.key -outform PEM -pubout -out sa.pubopenssl req -new -sha256 -key sa.key -subj "/CN=system:kube-controller-manager" -out sa.csropenssl x509 -req -in sa.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req_client -extfile openssl.cnf -out sa.crt admin123openssl genrsa -out admin.key 2048openssl req -new -key admin.key -subj "/CN=kubernetes-admin/O=system:masters" -out admin.csropenssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req_client -extfile openssl.cnf -out admin.crt 清理 csr srl(csr只要key不变那每次生成都是一样的,所以可以删除,如果后期根据ca重新生成证书来添加ip的话可以此处不删除)1find . -name "*.csr" -o -name "*.srl"|xargs rm -f 利用证书生成组件的kubeconfig filename credential name Default CN O（in Subject） admin.conf default-admin kubernetes-admin system:masters kubelet.conf default-auth system:node: (see note) system:nodes controller-manager.conf default-controller-manager system:kube-controller-manager scheduler.conf default-manager system:kube-scheduler kubectl的参数意义为 -certificate-authority: 验证根证书 -client-certificate, -client-key: 生成的组件证书和私钥，连接kube-apiserver时会用到 -embed-certs=true: 将ca.pem和组件.pem证书内容嵌入生成的kubeconfig文件中（不添加该参数，默认写入证书文件路径） kube-controller-manager123456789101112131415161718192021222324252627282930CLUSTER_NAME="kubernetes"KUBE_USER="system:kube-controller-manager"KUBE_CERT="sa"KUBE_CONFIG="controller-manager.kubeconfig"# 设置集群参数kubectl config set-cluster $&#123;CLUSTER_NAME&#125; \ --certificate-authority=/etc/kubernetes/pki/ca.crt \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置客户端认证参数kubectl config set-credentials $&#123;KUBE_USER&#125; \ --client-certificate=/etc/kubernetes/pki/$&#123;KUBE_CERT&#125;.crt \ --client-key=/etc/kubernetes/pki/$&#123;KUBE_CERT&#125;.key \ --embed-certs=true \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置上下文参数kubectl config set-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; \ --cluster=$&#123;CLUSTER_NAME&#125; \ --user=$&#123;KUBE_USER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置当前使用的上下文kubectl config use-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 查看生成的配置文件kubectl config view --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125; kube-scheduler123456789101112131415161718192021222324252627282930CLUSTER_NAME="kubernetes"KUBE_USER="system:kube-scheduler"KUBE_CERT="kube-scheduler"KUBE_CONFIG="scheduler.kubeconfig"# 设置集群参数kubectl config set-cluster $&#123;CLUSTER_NAME&#125; \ --certificate-authority=/etc/kubernetes/pki/ca.crt \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置客户端认证参数kubectl config set-credentials $&#123;KUBE_USER&#125; \ --client-certificate=/etc/kubernetes/pki/$&#123;KUBE_CERT&#125;.crt \ --client-key=/etc/kubernetes/pki/$&#123;KUBE_CERT&#125;.key \ --embed-certs=true \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置上下文参数kubectl config set-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; \ --cluster=$&#123;CLUSTER_NAME&#125; \ --user=$&#123;KUBE_USER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置当前使用的上下文kubectl config use-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 查看生成的配置文件kubectl config view --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125; admin(kubectl)123456789101112131415161718192021222324252627282930CLUSTER_NAME="kubernetes"KUBE_USER="kubernetes-admin"KUBE_CERT="admin"KUBE_CONFIG="admin.kubeconfig"# 设置集群参数kubectl config set-cluster $&#123;CLUSTER_NAME&#125; \ --certificate-authority=/etc/kubernetes/pki/ca.crt \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置客户端认证参数kubectl config set-credentials $&#123;KUBE_USER&#125; \ --client-certificate=/etc/kubernetes/pki/$&#123;KUBE_CERT&#125;.crt \ --client-key=/etc/kubernetes/pki/$&#123;KUBE_CERT&#125;.key \ --embed-certs=true \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置上下文参数kubectl config set-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; \ --cluster=$&#123;CLUSTER_NAME&#125; \ --user=$&#123;KUBE_USER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置当前使用的上下文kubectl config use-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 查看生成的配置文件kubectl config view --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125; 分发kubeconfig及证书到其它master节点1234for NODE in "$&#123;!otherMaster[@]&#125;"; do echo "--- $NODE $&#123;otherMaster[$NODE]&#125; ---" scp -r /etc/kubernetes $&#123;otherMaster[$NODE]&#125;:/etcdone 配置etcd &#9730; https://github.com/etcd-io/etcd/releases etcd用来保存集群所有状态key/value的存储系统，所有kubernetes组件会通过 apiserver会跟etcd进行沟通，从而保存或读取资源状态。一般etcd跟master在一起，也可以单独做集群，但必须得配置apiserver指向etcd集群。 下载etcd二进制文件，提供两种方法12345678ETCD_version=v3.1.9wget https://github.com/etcd-io/etcd/releases/download/$&#123;ETCD_version&#125;/etcd-$&#123;ETCD_version&#125;-linux-amd64.tar.gztar -zxvf etcd-$&#123;ETCD_version&#125;-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-$&#123;ETCD_version&#125;-linux-amd64/etcd&#123;,ctl&#125;docker pull quay.io/coreos/etcd:$ETCD_versiondocker run --rm -d --name temp quay.io/coreos/etcd:$ETCD_version sleep 10docker cp temp:/usr/local/bin/etcd /usr/local/bindocker cp temp:/usr/local/bin/etcdctl /usr/local/bin 分发etcd的二进制文件到其他master上1234for NODE in "$&#123;!otherMaster[@]&#125;"; do echo "--- $NODE $&#123;otherMaster[$NODE]&#125; ---" scp /usr/local/bin/etcd* $&#123;otherMaster[$NODE]&#125;:/usr/local/bin/done 在k8s-m1上配置etcd配置文件并分发相关文件,配置文件路径为/etc/etcd/etcd.config.yml,参考官方 https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample 注入基础变量1234cd ~/k8s-manual-files/master/etcd_servers=$( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#https://#;s#$#:2379#;$s#\n##' | paste -d, -s - )etcd_initial_cluster=$( for i in $&#123;!MasterArray[@]&#125;;do echo $i=https://$&#123;MasterArray[$i]&#125;:2380; done | sort | paste -d, -s - )sed -ri "/initial-cluster:/s#'.+'#'$&#123;etcd_initial_cluster&#125;'#" etc/etcd/config.yml 分发systemd和配置文件到其他master上123456789for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $&#123;MasterArray[$NODE]&#125; "mkdir -p /etc/etcd /var/lib/etcd" scp systemd/etcd.service $&#123;MasterArray[$NODE]&#125;:/usr/lib/systemd/system/etcd.service scp etc/etcd/config.yml $&#123;MasterArray[$NODE]&#125;:/etc/etcd/etcd.config.yml ssh $&#123;MasterArray[$NODE]&#125; "sed -i "s/&#123;HOSTNAME&#125;/$NODE/g" /etc/etcd/etcd.config.yml" ssh $&#123;MasterArray[$NODE]&#125; "sed -i "s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g" /etc/etcd/etcd.config.yml" ssh $&#123;MasterArray[$NODE]&#125; 'systemctl daemon-reload'done 在k8s-m1上启动所有etcd，etcd进程首次启动时会等待其它节点的etcd加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象，可以全部启动后，使用etcdctl命令验证健康状态。12345for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $&#123;MasterArray[$NODE]&#125; 'systemctl enable --now etcd' &amp;donewait 查看etcd集群状态和集群的键值12345678910111213141516171819etcdctl \ --cert-file /etc/kubernetes/pki/etcd/healthcheck-client.crt \ --key-file /etc/kubernetes/pki/etcd/healthcheck-client.key \ --ca-file /etc/kubernetes/pki/etcd/ca.crt \ --endpoints $etcd_servers cluster-health...下面是输出member 4f15324b6756581c is healthy: got healthy result from https://10.0.6.166:2379member cce1303a6b6dd443 is healthy: got healthy result from https://10.0.6.167:2379member ead42f3e6c9bb295 is healthy: got healthy result from https://10.0.6.168:2379cluster is healthyETCDCTL_API=3 \ etcdctl \ --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt \ --key /etc/kubernetes/pki/etcd/healthcheck-client.key \ --cacert /etc/kubernetes/pki/etcd/ca.crt \ --endpoints $etcd_servers get / --prefix --keys-only 如果想了解更多etcdctl操作，可以访问官网的etcdctl command文章 Kubernetes Master本部分将说明如何建立与设定Kubernetes Master角色，过程中会部署以下组件： kubelet 负责管理容器的生命周期，定期从API Server获取节点上的预期状态(如网络、存储等配置)资源，并让对应的容器插件(CRI、CNI等)来达成这个状态。任何 Kubernetes节点(node)都会拥有这个组件。关闭只读端口，在安全端口10250接收https请求，对请求进行认证和授权，拒绝匿名访问和非授权访问。使用 kubeconfig访问apiserver的安全端口。 kube-apiserver 以REST APIs提供Kubernetes资源的CRUD，如授权、认证、存取控制与API 注册等机制。关闭非安全端口，在安全端口6443接收https请求。严格的认证和授权策略(x509、token、RBAC)。开启bootstrap token认证，支持kubelet TLS bootstrapping。使用https访问kubelet、etcd，加密通信。 kube-controller-manager 通过核心控制循环(Core Control Loop)监听Kubernetes API的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、Namespace Controller等等。而这些控制器会处理着自动扩展、滚动更新等功能。关闭非安全端口，在安全端口10252接收https请求。使用kubeconfig访问apiserver的安全端口。 kube-scheduler 负责将一个或多个容器依据调度策略分配到对应节点上让容器引擎(如 Docker)执行。而调度受到QoS要求、软硬性约束、亲和性(Affinity)等因素影响。 HAProxy 提供多个API Server的负载均衡(Load Balance)，确保haproxy的端口负载到所有的apiserver的6443端口。 Keepalived 提供虚拟IP位址(vip)，来让vip落在可用的master主机上供所有组件都能访问到可用的master，结合haproxy能访问到master上的apiserver的6443端口。 部署和设定 export interface=eth0，改为宿主机的网卡名。若cluster dns或domain有改变的话，需要修改kubelet-conf.yml。 在master上安装haproxy+keepalived12345for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $&#123;MasterArray[$NODE]&#125; 'yum install haproxy keepalived -y' &amp;donewait 在k8s-m1节点下把相关配置文件配置后再分发123456789101112131415161718cd ~/k8s-manual-files/master/etc# 修改haproxy.cfg配置文件sed -i '$r '&lt;(paste &lt;( seq -f' server k8s-api-%g' $&#123;#MasterArray[@]&#125; ) &lt;( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#$#:6443 check#')) haproxy/haproxy.cfg# 修改keepalived(网卡和VIP写进去,使用下面命令)sed -ri "s#\&#123;\&#123; VIP \&#125;\&#125;#$&#123;VIP&#125;#" keepalived/*sed -ri "s#\&#123;\&#123; interface \&#125;\&#125;#$&#123;interface&#125;#" keepalived/keepalived.conf sed -i '/unicast_peer/r '&lt;(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#\t#') keepalived/keepalived.conf# 分发文件for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" scp -r haproxy/ $&#123;MasterArray[$NODE]&#125;:/etc scp -r keepalived/ $&#123;MasterArray[$NODE]&#125;:/etc ssh $&#123;MasterArray[$NODE]&#125; 'systemctl enable --now haproxy keepalived'done 等待keepalived和haproxy服务启动后，尝试ping下vip1ping $VIP 如果失败的话，可以在节点上查看/etc/keepalived/keepalived.conf里网卡名和ip是否注入成功或者尝试restart服务1234for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $&#123;MasterArray[$NODE]&#125; 'systemctl restart haproxy keepalived'done Master组件 在k8s-m1节点下把相关配置文件配置后再分发12345678910111213141516cd ~/k8s-manual-files/master/etcd_servers=$( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#https://#;s#$#:2379#;$s#\n##' | paste -d, -s - )# 注入VIP和etcd_servers,apiserver数量sed -ri '/--etcd-servers/s#=.+#='"$etcd_servers"' \\#' systemd/kube-apiserver.servicesed -ri '/apiserver-count/s#=[^\]+#='"$&#123;#MasterArray[@]&#125;"' #' systemd/kube-apiserver.service# 分发文件for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $&#123;MasterArray[$NODE]&#125; 'mkdir -p /etc/kubernetes/manifests /var/lib/kubelet /var/log/kubernetes' scp systemd/kube-*.service $&#123;MasterArray[$NODE]&#125;:/usr/lib/systemd/system/ #注入网卡ip ssh $&#123;MasterArray[$NODE]&#125; "sed -ri '/bind-address/s#=[^\]+#=$&#123;MasterArray[$NODE]&#125; #' /usr/lib/systemd/system/kube-apiserver.service &amp;&amp; sed -ri '/--advertise-address/s#=[^\]+#=$&#123;MasterArray[$NODE]&#125; #' /usr/lib/systemd/system/kube-apiserver.service"done 在k8s-m1上给所有master机器启动kubelet服务并设置kubectl补全脚本1234567for NODE in "$&#123;!MasterArray[@]&#125;"; do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $&#123;MasterArray[$NODE]&#125; 'systemctl enable --now kube-apiserver kube-controller-manager kube-scheduler; mkdir -p ~/.kube/ cp /etc/kubernetes/admin.kubeconfig ~/.kube/config; kubectl completion bash &gt; /etc/bash_completion.d/kubectl'done 验证组件 完成后,在任意一台master节点通过简单指令验证1234567891011$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-2 Healthy &#123;"health": "true"&#125; etcd-0 Healthy &#123;"health": "true"&#125; etcd-1 Healthy &#123;"health": "true"&#125; $ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 36s 配置bootstrap 由于本次安装启用了TLS认证,因此每个节点的kubelet都必须使用kube-apiserver的CA的凭证后，才能与kube-apiserver进行沟通,而该过程需要手动针对每台节点单独签署凭证是一件繁琐的事情，而且一旦节点增加会延伸出管理不易问题。而TLS bootstrapping目标就是解决该问题，通过让kubelet先使用一个预定低权限使用者连接到kube-apiserver，然后在对kube-apiserver申请凭证签署，当授权Token一致时，Node节点的kubelet凭证将由kube-apiserver动态签署提供。具体做法可以参考TLS Bootstrapping与Authenticating with Bootstrap Tokens。 首先在k8s-m1建立一个变数来生产BOOTSTRAP_TOKEN，并建立bootstrap的kubeconfig文件，然后在k8s-m1建立TLS bootstrap secret来提供自动签证使用。（只需在任何一台master执行即可）1234567891011TOKEN_PUB=$(openssl rand -hex 3)TOKEN_SECRET=$(openssl rand -hex 8)BOOTSTRAP_TOKEN="$&#123;TOKEN_PUB&#125;.$&#123;TOKEN_SECRET&#125;"kubectl -n kube-system create secret generic bootstrap-token-$&#123;TOKEN_PUB&#125; \ --type 'bootstrap.kubernetes.io/token' \ --from-literal description="cluster bootstrap token" \ --from-literal token-id=$&#123;TOKEN_PUB&#125; \ --from-literal token-secret=$&#123;TOKEN_SECRET&#125; \ --from-literal usage-bootstrap-authentication=true \ --from-literal usage-bootstrap-signing=true 建立bootstrap的kubeconfig文件123456789101112131415161718192021222324252627CLUSTER_NAME="kubernetes"KUBE_USER="kubelet-bootstrap"KUBE_CONFIG="bootstrap.kubeconfig"# 设置集群参数kubectl config set-cluster $&#123;CLUSTER_NAME&#125; \ --certificate-authority=/etc/kubernetes/pki/ca.crt \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置上下文参数kubectl config set-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; \ --cluster=$&#123;CLUSTER_NAME&#125; \ --user=$&#123;KUBE_USER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置客户端认证参数kubectl config set-credentials $&#123;KUBE_USER&#125; \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 设置当前使用的上下文kubectl config use-context $&#123;KUBE_USER&#125;@$&#123;CLUSTER_NAME&#125; --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;# 查看生成的配置文件kubectl config view --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125; 若想要用手动签署凭证来进行授权的话，可以参考Certificate。 授权 kubelet可以创建csr12kubectl create clusterrolebinding kubeadm:kubelet-bootstrap \ --clusterrole system:node-bootstrapper --group system:bootstrappers 批准csr请求 允许system:bootstrappers组的所有csr 123456789101112131415cat &lt;&lt;EOF | kubectl apply -f -# Approve all CSRs for the group "system:bootstrappers"kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: auto-approve-csrs-for-groupsubjects:- kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.ioEOF 允许 kubelet 能够更新自己的证书123456789101112131415cat &lt;&lt;EOF | kubectl apply -f -# Approve renewal CSRs for the group "system:nodes"kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: auto-approve-renewals-for-nodessubjects:- kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.ioEOF Kubernetes Nodes本部分将说明如何建立与设定Kubernetes Node角色，Node是主要执行Pod的工作节点。 在k8s-m1将需要的文件分发到其他节点上，这里值得一提的是，官方建议大多数写一个yaml里用–config指定kubelet配置。https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration1234567for NODE in "$&#123;!Other[@]&#125;"; do echo "--- $NODE $&#123;Other[$NODE]&#125; ---" ssh $&#123;Other[$NODE]&#125; "mkdir -p /etc/kubernetes/pki /etc/kubernetes/manifests /var/lib/kubelet/" for FILE in /etc/kubernetes/pki/ca.crt /etc/kubernetes/bootstrap.kubeconfig; do scp $&#123;FILE&#125; $&#123;Other[$NODE]&#125;:$&#123;FILE&#125; donedone 部署与设定 这边建议healthzBindAddresskubeadm生成的是127，我建议设置成网卡ip方便后续检测curl http://10.0.6.166:10248/healthz 在k8s-m1节点分发kubelet.service文件和配置文件到每台上去管理kubelet12345678cd ~/k8s-manual-files/for NODE in "$&#123;!AllNode[@]&#125;"; do echo "--- $NODE $&#123;AllNode[$NODE]&#125; ---" scp master/systemd/kubelet.service $&#123;AllNode[$NODE]&#125;:/lib/systemd/system/kubelet.service scp master/etc/kubelet/kubelet-conf.yml $&#123;AllNode[$NODE]&#125;:/etc/kubernetes/kubelet-conf.yml ssh $&#123;AllNode[$NODE]&#125; "sed -ri '/0.0.0.0/s#\S+\$#$&#123;MasterArray[$NODE]&#125;#' /etc/kubernetes/kubelet-conf.yml" ssh $&#123;AllNode[$NODE]&#125; "sed -ri '/127.0.0.1/s#\S+\$#$&#123;MasterArray[$NODE]&#125;#' /etc/kubernetes/kubelet-conf.yml"done 最后在k8s-m1上去启动每个node节点的kubelet服务1234for NODE in "$&#123;!AllNode[@]&#125;"; do echo "--- $NODE $&#123;AllNode[$NODE]&#125; ---" ssh $&#123;AllNode[$NODE]&#125; 'systemctl enable --now kubelet.service'done 验证集群 完成后，在任意一台master节点并通过简单指令验证123456789101112131415$ kubectl get nodesNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-m1 NotReady &lt;none&gt; 22s v1.13.4 10.0.6.166 &lt;none&gt; CentOS Linux 7 (Core) 4.20.13-1.el7.elrepo.x86_64 docker://18.6.3k8s-m2 NotReady &lt;none&gt; 24s v1.13.4 10.0.6.167 &lt;none&gt; CentOS Linux 7 (Core) 4.20.13-1.el7.elrepo.x86_64 docker://18.6.3k8s-m3 NotReady &lt;none&gt; 21s v1.13.4 10.0.6.168 &lt;none&gt; CentOS Linux 7 (Core) 4.20.13-1.el7.elrepo.x86_64 docker://18.6.3k8s-n1 NotReady &lt;none&gt; 22s v1.13.4 10.0.6.169 &lt;none&gt; CentOS Linux 7 (Core) 4.20.13-1.el7.elrepo.x86_64 docker://18.6.3k8s-n2 NotReady &lt;none&gt; 22s v1.13.4 10.0.6.170 &lt;none&gt; CentOS Linux 7 (Core) 4.20.13-1.el7.elrepo.x86_64 docker://18.6.3# csr自动被授权$ kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-4fCDrNulc_btdBiRgev0JO4EorZ0rMuyJ756wrn9NqQ 27s system:bootstrap:e860ec Approved,Issuednode-csr-P3Y_knryQNaQWDDYFObtcdfXB4XAl9IB2Be2YJ-b-dA 27s system:bootstrap:e860ec Approved,Issuednode-csr-r_4ZDnanqBw2HPTSn6bSL50r-kJkTPjix6SY1n9UmjY 28s system:bootstrap:e860ec Approved,Issuednode-csr-vy-6tgMI9vUiIzR3Ogv6bPVGA2_gZrd7aMIWMSuHrME 27s system:bootstrap:e860ec Approved,Issuednode-csr-zOvVxSaY1iMco2LnOHmwqiBDwPUaLix7cSqUfZWTGFo 26s system:bootstrap:e860ec Approved,Issued 设定master节点加上污点Taint不让(没有声明容忍该污点的)pod跑在master节点上12345kubectl taint nodes $&#123;!MasterArray[@]&#125; node-role.kubernetes.io/master="":NoSchedule# 下面是输出node "k8s-m1" taintednode "k8s-m2" taintednode "k8s-m3" tainted node打标签声明role12kubectl label node $&#123;!MasterArray[@]&#125; node-role.kubernetes.io/master=""kubectl label node $&#123;!NodeArray[@]&#125; node-role.kubernetes.io/worker=worker Kubernetes Core Addons完成master与nodes的部署后，kubernetes dns与kubernetes proxy等addons也是极为重要的。 Kubernetes Proxy kube-proxy是实现Service的关键插件，kube-proxy会在每台节点上执行，然后监听API Server的Service与Endpoint资源的改变，然后来依据变化执行iptables来实现网路的转发。 二进制部署 创建一个kube-proxy的service account 1kubectl -n kube-system create serviceaccount kube-proxy 将kube-proxy的serviceaccount绑定到clusterrole system:node-proxier以允许RBAC 123kubectl create clusterrolebinding kubeadm:kube-proxy \ --clusterrole system:node-proxier \ --serviceaccount kube-system:kube-proxy 创建kube-proxy的kubeconfig 1234567891011121314151617181920212223242526CLUSTER_NAME="kubernetes"KUBE_CONFIG="kube-proxy.kubeconfig"SECRET=$(kubectl -n kube-system get sa/kube-proxy \ --output=jsonpath='&#123;.secrets[0].name&#125;')JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET \ --output=jsonpath='&#123;.data.token&#125;' | base64 -d)kubectl config set-cluster $&#123;CLUSTER_NAME&#125; \ --certificate-authority=/etc/kubernetes/pki/ca.crt \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;kubectl config set-context $&#123;CLUSTER_NAME&#125; \ --cluster=$&#123;CLUSTER_NAME&#125; \ --user=$&#123;CLUSTER_NAME&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;kubectl config set-credentials $&#123;CLUSTER_NAME&#125; \ --token=$&#123;JWT_TOKEN&#125; \ --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;kubectl config use-context $&#123;CLUSTER_NAME&#125; --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125;kubectl config view --kubeconfig=/etc/kubernetes/$&#123;KUBE_CONFIG&#125; 在k8s-m1分发kube-proxy的相关文件到所有节点 123456789101112cd ~/k8s-manual-files/for NODE in "$&#123;!Other[@]&#125;"; do echo "--- $NODE $&#123;Other[$NODE]&#125; ---" scp /etc/kubernetes/kube-proxy.kubeconfig $&#123;Other[$NODE]&#125;:/etc/kubernetes/kube-proxy.kubeconfigdonefor NODE in "$&#123;!AllNode[@]&#125;"; do echo "--- $NODE $&#123;AllNode[$NODE]&#125; ---" scp addons/kube-proxy/kube-proxy.conf $&#123;AllNode[$NODE]&#125;:/etc/kubernetes/kube-proxy.conf scp addons/kube-proxy/kube-proxy.service $&#123;AllNode[$NODE]&#125;:/usr/lib/systemd/system/kube-proxy.service ssh $&#123;AllNode[$NODE]&#125; "sed -ri '/0.0.0.0/s#\S+\$#$&#123;MasterArray[$NODE]&#125;#' /etc/kubernetes/kube-proxy.conf"done 在k8s-m1上启动所有节点的kube-proxy服务 1234for NODE in "$&#123;!AllNode[@]&#125;"; do echo "--- $NODE $&#123;AllNode[$NODE]&#125; ---" ssh $&#123;AllNode[$NODE]&#125; 'systemctl enable --now kube-proxy'done daemonSet方式部署 12345678910cd ~/k8s-manual-files# 注入变量sed -ri "/server:/s#(: ).+#\1$&#123;KUBE_APISERVER&#125;#" addons/kube-proxy/kube-proxy.ymlsed -ri "/image:.+kube-proxy/s#:[^:]+\$#:$KUBE_VERSION#" addons/kube-proxy/kube-proxy.ymlkubectl apply -f addons/kube-proxy/kube-proxy.yml# 下面是输出serviceaccount "kube-proxy" createdclusterrolebinding.rbac.authorization.k8s.io "system:kube-proxy" createdconfigmap "kube-proxy" createddaemonset.apps "kube-proxy" created 正常输出 123456$ kubectl -n kube-system get po -l k8s-app=kube-proxyNAME READY STATUS RESTARTS AGEkube-proxy-dd2m7 1/1 Running 0 8mkube-proxy-fwgx8 1/1 Running 0 8mkube-proxy-kjn57 1/1 Running 0 8mkube-proxy-vp47w 1/1 Running 0 8m 通过ipvsadm查看proxy规则 123456$ ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.96.0.1:443 rr-&gt; 10.0.6.155:6443 Masq 1 0 0 确认使用ipvs模式 12$ curl localhost:10249/proxyModeipvs 集群网络 Kubernetes在默认情況下与Docker的网络有所不同。在Kubernetes中有四个问题是需要被解決的，分別为： 高耦合的容器到容器通信：通过Pods内localhost的來解決。 Pod到Pod的通信：通过实现网络模型来解决。 Pod到Service通信：由Service objects结合kube-proxy解決。 外部到Service 通信：一样由Service objects结合kube-proxy解決。 而Kubernetes对于任何网络的实现都需要满足以下基本要求(除非是有意调整的网络分段策略)： 所有容器能够在沒有NAT的情況下与其他容器通信。 所有节点能夠在沒有 NAT 情況下与所有容器通信(反之亦然)。 容器看到的IP与其他人看到的IP是一样的。 目前Kubernetes已经有非常多种的网络模型作为网络插件(Network Plugins)方式被实现，因此可以选用满足自己需求的网络功能来使用。另外Kubernetes中的网络插件有以下两种形式： CNI plugins：以appc/CNI标准规范所实现的网络，详细可以阅读CNI Specification Kubenet plugin：使用CNI plugins的bridge与host-local来实现基本的cbr0。这通常被用在公有云服务上的Kubernetes集群网络。 如果想了解如何选择可以如阅读Chris Love的Choosing a CNI Network Provider for Kubernetes文章。 网络部署与设定(flannel或者calico任选其一)如果是公有云不在一个vpc里建议用flannel，因为公有云是SDN，只有vxlan才能到达目标，每个node上的flannel.1充当了vtep身份。另外完成到集群可以使用后会发现只有pod所在的node能访问到它这台上面的clusterIP，是因为kubelet上报的节点的node public IP是取网卡的ip，公有云网卡ip都是内网ip，所以当flannel包要发到目标机器的flannel上的时候会发到目标机器的内网ip上，根本发不出去。 flannel flannel使用vxlan技术为各节点创建一个可以互通的Pod网络，使用的端口为 UDP 8472，需要开放该端口（如公有云 AWS 等）。 flannel第一次启动时，从etcd获取Pod网段信息，为本节点分配一个未使用的 /24段地址，然后创建flannel.1（也可能是其它名称，如 flannel1等）接口。]]></content>
  </entry>
  <entry>
    <title><![CDATA[构建标准且人性化镜像]]></title>
    <url>%2F2019%2F05%2F02%2F%E6%9E%84%E5%BB%BA%E6%A0%87%E5%87%86%E4%B8%94%E4%BA%BA%E6%80%A7%E5%8C%96%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[正确的FROM合适的镜像很多新手一上来就是FROM centos然后RUN 一堆yum install的，这样还停留在虚拟机的角度。可以FROM alpine或者干脆拿官方的改，alpine初期的时候问题蛮多的，很多人建议使用alpine做基础镜像最好是测试好再上线，现在alpine的快速发展，这种现象很少了。 不要用imageID或者latest标签id的话不便于长期发展，而latest标签无法回滚。 不要重复造轮子现在dockerhub上有很多的镜像了，很多人还是喜欢造轮子，造出来的镜像层又多，无用的文件又停留在层理，主进程还不是业务进程，还不支持传入环境变量来让用户选择场景和传入配置信息启动。 如果你的是一个java应用，那么你应该使用java作为基础应用，如果你是tomcat应用，你应该使用tomcat作为基础应用，而不是按照虚拟机的思维，把Java装好，然后装应用；tomcat也一样，装java，装tomcat，装应用。 镜像大小之前我举例的ADD添加源码包和RUN rm -f删掉ADD的源码包，虽说最终起来的容器看不到源码包。实际上文件还停留在镜像的层里，所以尽量合并和减少层防止层保持住文件。 最后一些零散的建议和常见错误 编写entrypoint脚本让启动更人性化 同时如果是初期上docker到生产，考虑到排错啥的，可以在官方dockerfile里添加一些常见的排错命 尽量使用ENV和ARG让人不改或者少改Dockerfile即可做构建对应版本的镜像 容器时间不对的话可以安装包tzdate，声明变量TZ即可声明时区，或者构建的时候带上/etc/localtime或者运行的时候挂载宿主机的/etc/localtime。 如果是编译型语言，妥善利用多阶段构建（后面容器无法运行排错的时候会讲解多阶构建） 代码里应该要注意优雅退出。收到信号的时候释放东西啥的。 代码，war，jar，go编译的二进制到底应不应该放在镜像里？其实现在的java和php，还有go啥的依赖的运行环境基本不会变，变更发布新版本也就只有代码，war，jar和go编译的二进制，为此可以两种做法: 全部打包到镜像里 不变的层做个镜像，启动利用entrypoint脚本接受传入的git分支或者war包啥的内网下载直链下载到容器里或者启动直接挂载nfs里的war包或者代码啥的启动 很多人都是传统的第一种思维，看到第二种的时候直接张口说这样不行。如果后续接触到了k8s会发现k8s有个initContainers，谷歌也说了可以利用initContainers去初始化或者克隆git代码。 其实两种均可，例如第一种，在没有gc原生docker下，每一次发布都会老版本镜像存在，虽说层共享，但是最后的代码层的容量还是占据了宿主机容量的。 第二种每次启动都需要下载，需要网速，如果是内网可以尝试，代码或者war包啥的都是在容器层，不会吃宿主机多大容量。实在接受不了可以运维给研发做个这种通用镜像给他们用。 最后是推荐一个漠然大佬的示例，漠然大佬的github上很多镜像下载量很多，可以去他github看，这里我放下他的java的应用示例 https://github.com/Gozap/dockerfile]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[exec与entrypoint使用脚本]]></title>
    <url>%2F2019%2F05%2F02%2Fexec%E4%B8%8Eentrypoint%E4%BD%BF%E7%94%A8%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[现在很多有状态的官方镜像的ENTRYPOINT都是使用了一个脚本。例如redis12345COPY docker-entrypoint.sh /usr/local/bin/ENTRYPOINT ["docker-entrypoint.sh"]EXPOSE 6379CMD ["redis-server"] 12345678910111213141516#!/bin/shset -e# first arg is `-f` or `--some-option`# or first arg is `something.conf`if [ "$&#123;1#-&#125;" != "$1" ] || [ "$&#123;1%.conf&#125;" != "$1" ]; then set -- redis-server "$@"fi# allow the container to be started with `--user`if [ "$1" = 'redis-server' -a "$(id -u)" = '0' ]; then find . \! -user redis -exec chown redis '&#123;&#125;' + exec gosu redis "$0" "$@"fiexec "$@" 最终运行的是docker-entrypoint.sh redis-server 第一个if的逻辑是如果docker run 选项 redis -x 或者–xx或者xxx.conf，就把脚本收到的$@改编成redis-server $@,例如我们可同docker run -d redis –port 7379修改启动的容器里的redis端口。如果我们传入的command不是-开头的也不是.conf结尾的字符，例如是date，则会跑到最后的逻辑执行我们的date命令不会启动redis-server 第二个if这里，如果满足第一个if或者直接默认的cmd下而且容器里用户uid是0，则把属主不是redis的文件改成redis用户，然后切成redis用户去启动redis-server。 我们可以看到entrypoint能在业务进程启动前做很多事情。而且优秀的镜像都离不开entrypoint脚本，能够根据用户传入的变量和command来切换启动的场景和配置。 前面说了，主进程一定要是业务进程，这里怎么是个脚本呢，那业务进程不就不是pid为1了吗？ 这里用了exec来退位让贤，最终redis-server还是pid为1的。可以简单几个命令讲解下exec的作用。 写个test.sh脚本，在脚本里用pstree -p，运行脚本bash test.sh查看进程层次 发现pstree是在我们脚本bash(1998)的子进程 然后在脚本最后面加一行exec pstree -p看看输出 我们发现bash进程运行的时候pid是2022，然后第二个pstree上升到了2022这一层次了，假设pid为a的命令或者二进制exec执行了命令b，那b就接替了a的pid。如果说我们entrypoint或者cmd使用脚本，那么我们一定要在脚本最后启动业务进程的时候前面加个exec让脚本退位让贤。 最后环境变量写配置文件涉及到修改，还有一些判断是否初次启动的有下面一些工具或者套路。 xmlstarlet 处理xml pip安装shyaml 处理yaml jq读取json nodejs的npm安装json可以修改json文件 处理excel或者csv使用in2csv，csvkit 提供了 in2csv，csvcut，csvjoin，csvgrep touch -d “@0”写在构建的最后一个RUN里把时间戳设置为1970-1-1，然后用stat命令判断 if [ “$(stat -c “%Y” “${CONF_INSTALL}/conf/server.xml”)” -eq “0” ]; then 另外entrypoint脚本COPY进去的时候注意可执行权限，如果Windows上传到Linux构建会因为entrpoint脚本没带权限无法运行]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile STOPSIGNAL]]></title>
    <url>%2F2019%2F05%2F02%2FDockerfile-STOPSIGNAL%2F</url>
    <content type="text"><![CDATA[格式，缺省信号为SIGTERM1234STOPSIGNAL signal------STOPSIGNAL SIGTERMSTOPSIGNAL 9 可以是kill -l的信号名字也可以信号数字:1234567891011121314kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX docker run的选项可以覆盖镜像定义的STOPSIGNAL信号1--stop-signal string Signal to stop a container (default "SIGTERM") 在docker stop停止运行容器的时候指定发送给容器里pid为1角色的信号。默认超时10秒，超时则发送kill强杀进程。一般业务进程都是pid为1，所有官方的进程都会处理收到的SIGTERM信号进行优雅收尾退出。 前面说过了如果CMD是/bin/sh格式的话，主进程是一个sh -c的进程，shell不用trap处理的话是无法转发信号的。下面我举个例子 例子是是网上找的，两种CMD方式启动的redis123456789FROM ubuntu:14.04RUN apt-get update &amp;&amp; apt-get -y install redis-server &amp;&amp; rm -rf /var/lib/apt/lists/*EXPOSE 6379CMD /usr/bin/redis-server----------------------------FROM ubuntu:14.04RUN apt-get update &amp;&amp; apt-get -y install redis-server &amp;&amp; rm -rf /var/lib/apt/lists/*EXPOSE 6379CMD ["/usr/bin/redis-server"] 构建两种镜像，然后docker run -d img_name，然后docker stop这俩镜像启动的容器会发现exec的redis能在docker stop的时候收到信号优雅退出Received SIGTERM, scheduling shutdown123456[1] 11 Feb 08:13:01.633 * The server is now ready to accept connections on port 6379[1 | signal handler] (1455179074) Received SIGTERM, scheduling shutdown...[1] 11 Feb 08:24:34.259 # User requested shutdown...[1] 11 Feb 08:24:34.259 * Saving the final RDB snapshot before exiting.[1] 11 Feb 08:24:34.262 * DB saved on disk[1] 11 Feb 08:24:34.262 # Redis is now ready to exit, bye bye... 而/bin/sh的形式的redis在docker stop后去docker logs看日志会发现根本没有优雅退出，类似于强制杀掉一样。1[5] 11 Feb 08:12:40.109 * The server is now ready to accept connections on port 6379 这是因为/bin/sh形式启动的redis主进程是一个sh，shell不会转发信号，所以最后sh被超时的docker stop发送了kill信号杀掉，整个容器生存周期结束，redis没有触发signal handler。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile VOLUME]]></title>
    <url>%2F2019%2F05%2F02%2FDockerfile-VOLUME%2F</url>
    <content type="text"><![CDATA[VOLUME两种写法，无区别12VOLUME ["/data","/mysql"]VOLUME /var/log /var/db 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中。 为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ENTRYPOINT]]></title>
    <url>%2F2019%2F05%2F02%2FDockerfile-ENTRYPOINT%2F</url>
    <content type="text"><![CDATA[ENTRYPOINT和CMD用法也一样两种格式，唯一要注意的就是区别，CMD和ENTRYPOINT只有一个或者两者都有都可以，容器最终运行的命令为：1&lt;ENTRYPOINT&gt; &lt;CMD&gt; alpine的root目录是没有文件的，所以ls /root没有输出，我们用选项去覆盖住entrypoint可以看到输出了date。注意一点是覆盖entrypoint的时候镜像的CMD会被忽略，我们真要调试的时候需要加command的话，可以在docker run的镜像后面加command和arg。 上面例子可以很形象的证明了是这个关系，最终运行的是 ，同时不光在docker run的时候覆盖掉CMD，也可以覆盖掉默认的entrypoint。很多时候我们可以主进程bash或者sh进去手动启动看看。老版本接触不多，不确定老版本有没有–entrypoint的选项。 最后如果是/bin/sh的entrypoint会忽略掉CMD和docker run的command参数]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建私有YUM源]]></title>
    <url>%2F2019%2F05%2F01%2F%E5%88%9B%E5%BB%BA%E7%A7%81%E6%9C%89YUM%E6%BA%90%2F</url>
    <content type="text"><![CDATA[安装httpd1yum -y install httpd 安装所需软件1yum -y install rsync createrepo 创建相关目录1mkdir -p /var/www/repos/centos/7/&#123;os,updates,extras&#125;/x86_64 赋予读写权限1chmod -R 755 /var/www/repos 从清华源同步1234567891011rsync -avz --delete --exclude='repodata' \rsync://mirrors.tuna.tsinghua.edu.cn/centos/7/os/x86_64/ \/var/www/repos/centos/7/os/x86_64/ rsync -avz --delete --exclude='repodata' \rsync://mirrors.tuna.tsinghua.edu.cn/centos/7/updates/x86_64/ \/var/www/repos/centos/7/updates/x86_64/ rsync -avz --delete --exclude='repodata' \rsync://mirrors.tuna.tsinghua.edu.cn/centos/7/extras/x86_64/ \/var/www/repos/centos/7/extras/x86_64/ 创建 metadata repositories123createrepo /var/www/repos/centos/7/os/x86_64/ createrepo /var/www/repos/centos/7/updates/x86_64/ createrepo /var/www/repos/centos/7/extras/x86_64/ 设置定时任务，每天同步1234567891011# vi /etc/cron.daily/update-repo#!/bin/bashVER='7'ARCH='x86_64'REPOS=(os updates extras)for REPO in $&#123;REPOS[@]&#125;do rsync -avz --delete --exclude='repodata' \ rsync://mirrors.tuna.tsinghua.edu.cn/centos/$&#123;VER&#125;/$&#123;REPO&#125;/$&#123;ARCH&#125;/ /var/www/repos/centos/$&#123;VER&#125;/$&#123;REPO&#125;/$&#123;ARCH&#125;/ createrepo /var/www/repos/centos/$&#123;VER&#125;/$&#123;REPO&#125;/$&#123;ARCH&#125;/done 赋予权限1chmod 755 /etc/cron.daily/update-repo 配置httpd主机使其他客户端访问123456# vim /etc/httpd/conf.d/repos.confAlias /repos /var/www/repos&lt;directory /var/www/repos&gt; Options +Indexes Require all granted&lt;/directory&gt; 启动httpd服务12systemctl start httpdsystemctl enable httpd 客户端的配置文件，其中10.105.26.110是源服务器地址12345678910111213141516171819# vi /etc/yum.repos.d/CentOS-Base.repo[base]name=CentOS-$releasever - Base#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os&amp;infra=$infrabaseurl=http://10.105.26.110/repos/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[updates]name=CentOS-$releasever - Updates#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates&amp;infra=$infrabaseurl=http://10.105.26.110/repos/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[extras]name=CentOS-$releasever - Extras#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras&amp;infra=$infrabaseurl=http://10.105.26.110/repos/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos7修改网卡名称]]></title>
    <url>%2F2019%2F05%2F01%2FCentos7%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E5%90%8D%E7%A7%B0%2F</url>
    <content type="text"><![CDATA[修改设备名称1sed -i "s/ens33/eth0/g" /etc/sysconfig/network-scripts/ifcfg-ens33 重命名网卡配置文件1mv /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-eth0 修改grub文件1sed -i "s/root/root net.ifnames=0 biosdevname=0/g" /etc/default/grub 重新生成GRUB配置并更新内核参数，稍后重启12grub2-mkconfig -o /boot/grub2/grub.cfgreboot]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile CMD]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-CMD%2F</url>
    <content type="text"><![CDATA[CMD 与进程前后台和容器存活的关系设置镜像运行出来的容器的缺省命令 有两种写法，写多个和FROM一个已经有CMD的镜像的话，以最后一个为准12CMD ["executable", "param1", "param2"] CMD command param1 param2 前者是exec格式也是推荐格式，后者是/bin/sh格式，exec和CMD还有ENTRYPOINT这三者之间联系非常紧密，后面单独将相关的知识点。这里先用一个例子讲/bin/sh格式啥意思 我们发现pid为1的是一个/bin/sh的进程，而我们的进程在容器里在后面。容器是单独一个pid namespaces的。这里懒得去做个图了，借用下别人的图 默认下所有进程在一个顶级的pid namespaces里，pid namespaces像一个树一样。从根到最后可以多级串。容器的pid namespaces实际上是在宿主机上能看到的，也就是下面，我们可以看到容器在宿主机上的进程，由于子namespaces无法看到父级的namespaces，所以容器里第一个进程(也就是cmd)认为自己是pid为1，容器里其余进程都是它的子进程 在Linux中，只能给init已经安装信号处理函数的信号，其它信号都会被忽略，这可以防止init进程被误杀掉，即使是superuser。所以，kill -9 init不会kill掉init进程。但是容器的进程是在容器的ns里是init级别，我们可以在宿主机上杀掉它，之前线上的低版本docker 命令无法使用，同事无法停止错误容器，我便询问了进程名在宿主机找到后kill掉的。 接下来说说为啥推荐exec格式，exec格式的话第一个进程是我们的sleep进程，大家可以自己去构建镜像试试。推荐用exec格式是因为pid 为1的进程承担着pid namespaces的存活周期，听不懂的话我举个例子12345678[root@docker ~]# docker run -d alpine lsb2eedc510e718d2820ce79fcf630aa9521fc3525b9138a51f1f8bef496e2607a[root@docker ~]# docker run -d alpine sleep 10bee830e62508b52796f588d6defe5419e35acb6c944f0151e0cb4b40a260ef81[root@docker ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbee830e62508 alpine "sleep 10" 19 seconds ago Exited (0) 7 seconds ago sad_lamarrb2eedc510e71 alpine "ls" 28 seconds ago Exited (0) 26 seconds ago reverent_stallman 先看下docker run命令格式123# docker run --helpUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] docker run 后面镜像后面的command和arg会覆盖掉镜像的CMD。上面我那个例子覆盖掉centos镜像默认的CMD bash。我们可以看到ls的容器直接退出了，但是sleep 10的容器运行了10秒后就退出了。以上也说明了容器不是虚拟机，容器是个隔离的进程。 这说明了容器的存活是容器里pid为1的进程运行时长决定的。所以nginx的官方镜像里就是用的exec格式让nginx充当pid为1的角色。1CMD ["nginx", "-g", "daemon off;"] 这里nginx启动带了选项是什么意思呢，我举个初学者自己造轮子做nginx镜像来举例，也顺带按照初学者重复造轮子碰到错误的时候应该怎样去排查？上面我是按照初学者虚拟机的思维去做一个nginx镜像，结果构建错误，我们发现有个失败的容器就是RUN那层创建出来的，前面我说的实际上docker build就是运行容器执行步骤然后最后底层调用commit的原因。 现在我们来手动排下错，哪步报错可以把那步到后面的全部注释掉后构建个镜像，然后我们run起来的时候带上-ti选项分配一个能输入的伪终端，最后的command用sh或者bash，这样容器的主进程就是bash或者sh了，我们在里面执行报错的RUN(这里我例子简单，所以我直接run -ti centos bash)。实际上会发现nginx是在epel-release的源里，接下来改下Dockerfile再构建试试.123456# cat DockerfileFROM centosRUN yum install -y epel-release \ &amp;&amp; yum install -y nginxCMD ["nginx"]$ docker build -t test . 然后又是一个新手自己做镜像遇到的问题了，这个镜像运行了根本跑不起来，我们手动bash或者sh进去排查。12345$ docker run -d -p 80:80 testf13e98d4dc13b6fa13e375ca35cc58a23a340a07b677f0df245fc1ef3b7199c6$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf13e98d4dc13 test "nginx" 3 seconds ago Exited (0) 1 second ago determined_elgamal 似乎是卡主了？我们可以访问宿主机的ip:80看看会发现实际能访问到的，也就是说这样也是在运行，当然我们把CMD改成和官方一样直接docker run -d -p 80:80 test的话容器是不会退出的。 至于说为啥？答案就是前台的概念！ 我们有没有发现我们手动执行nginx带关闭daemon选项发现类似于hang住一样，实际上它就是前台跑。 单独的nginx回车，实际上是它拉起来了nginx，然后它退出了，但是！！！，别忘记了你这个nginx是pid为1的角色，你退出了你下面子进程全部完蛋，容器也会显示退出。所以既然你最终要跑nginx，你nginx得是前台跑。 但是这里肯定也有人说如果我主进程跑一个不退出的进程，然后进去启动nginx不也跑起来了吗？这样是可以的，但是存在信号转发机制和要考虑优雅退出，这块知识我在后面指令STOPSIGNAL讲。 判断一个命令(或者说带上选项)是不是前台跑的最简单一个验证就是(主进程sh或者bash进去后)执行它看它有没有回到终端。例如ls和yes命令，我们会发现yes命令一直刷y没有回到终端。 其实发展到现在，很多以前只有daemon后台跑的进程都慢慢的在docker火热下开始有前台运行的选项或者配置了，例如 redis的配置文件不写日志文件路径它就默认前台跑 uwsgi也是一样，命令行参数或者配置文件指定了日志文件路径就后台跑，否则前台跑 node本身是前台跑，但是一些信号可能不好处理，于是有了pm2 zabbix 的日志路径写console的话就是前台跑 其实我们用上前台选项的话也无法用docker logs看容器的log，是因为docker logs查看的是容器里的标准输出信息，我们可以看到官方nginx镜像Dockerfile是这样做的。123# forward request and error logs to docker log collector &amp;&amp; ln -sf /dev/stdout /var/log/nginx/access.log \ &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile EXPOSE]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-EXPOSE%2F</url>
    <content type="text"><![CDATA[EXPOSE用法1EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...] 例子123EXPOSE 80/tcpEXPOSE 80/udpEXPOSE 80 443 声明需要暴露的端口（缺省tcp），仅仅是声明并没有说写了它才能映射端口，对容器网络不熟悉的话后面会讲容器网络的。我们可以看到nginx官方镜像的Dockerfile里有写80。1EXPOSE 80 我们假设简单的run起来让外部访问的话可以这样1docker run -d -p 80:80 nginx:alpine 这条命令是使用nginx:alpine镜像运行一个容器，把宿主机的80映射到容器的80端口上，我们可以访问宿主机ip:80就可以看到默认nginx的index页面，如果说是云主机80可能需要备案，可以改成81:80。可以自己把nginx官方dockerfile的EXPOSE删掉发现还可以映射的。 EXPOSE作用是告诉使用者应该把容器的哪个端口暴漏出去。另一个作用给docker run -P用的。1docker run -P nginx:alpine 会映射宿主机上随机没被bind的端口到EXPOSE的端口，例如 random_port:80]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ONBUILD]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ONBUILD%2F</url>
    <content type="text"><![CDATA[ONBUILD用法1ONBUILD [INSTRUCTION] 构建的时候并不会执行，只有在构建出来的镜像被FROM的时候才执行，例如12FROM xxxxONBUILD RUN cd /root/ &amp;&amp; wget xxxx 然后构建出镜像B里root目录并没有下载东西，只有FROM B构建的镜像才会执行这个RUN，这个用得很少，记住即可]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile USER]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-USER%2F</url>
    <content type="text"><![CDATA[USER两种写法12USER &lt;user&gt;[:&lt;group&gt;] orUSER &lt;UID&gt;[:&lt;GID&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。可用可不用。 不用的情况建议给容器的最终进程指定用户去运行，例如nginx官方添加了一个不登陆的nginx用户，配置文件里指定使用这个用户运行nginx。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile WORKDIR]]></title>
    <url>%2F2019%2F04%2F30%2FDo%2F</url>
    <content type="text"><![CDATA[WORKDIR声明后续指令的工作目录，目录不存在则创建，可以理解为mkdir -p dir &amp;&amp; cd dir 1WORKDIR /path/to/workdir 可以在a中多次使用Dockerfile。如果提供了相对路径，则它将相对于前一条WORKDIR指令的路径 。例如：1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 最终pwd命令的输出Dockerfile将是 /a/b/c]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ADD]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ADD%2F</url>
    <content type="text"><![CDATA[ADD和COPY一样，但是源可以是一个url会自动下载，另外源是压缩包的话会自动解压，但是实际中不会使用它，因为前面讲RUN的时候说的层概念。例如下面是一个ADD用的多的举例1234ADD https://xxxxx/name.tar.gz /home/test/RUN cd /home/test &amp;&amp; \ 编译安装... \ rm -rf /home/test ADD下载源码包，然后RUN里编译安装完删除源码包。实际上后面的层起来的容器虽说读取不到源码包了，但是还是在镜像里，参照我之前的RUN里那个test.html的例子。 一般避免多余的层和容量都是RUN里去下载源码包，处理完后删掉源码包，参照nginx的dockerfile的第一个RUN。 https://github.com/nginxinc/docker-nginx/blob/7d7c67f2eaa6b2b32c718ba9d93f152870513c7c/mainline/alpine/Dockerfile#L7]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile COPY]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-COPY%2F</url>
    <content type="text"><![CDATA[COPY用法123COPY &lt;src&gt; &lt;dest&gt; COPY ["&lt;src&gt;",... "&lt;dest&gt;"] COPY home* /home 复制本地的文件到容器中的目录，目录不存在则会自动创建，源可以是多个。在低版本的docker里如果源是绝对路径例如/root/data/nginx的话会把整个系统的根上传到docker daemon，会发现上传的内容等同于根的已用容量，例如下面12345$ cat DockerfileFROM alpineCOPY /root/data/nginx.tar.gz /root/home$ docker build -t test .Sending build context to Docker daemon 7.8GB 主要是因为上下文的概念，认为上下文的根是client的/，所以会把客户端的/上传到docker daemon，现在新版本是强制相对路径了，如果是绝对路径会报错。相对路径相对于build最后的.这个上下文路径为相对路径。 另外COPY还能指定uid:gid，如果容器的rootfs里没有文件/etc/passwd和/etc/group文件只能使用数字不能使用组名。1234COPY --chown=55:mygroup files* /somedir/COPY --chown=bin files* /somedir/COPY --chown=1 files* /somedir/COPY --chown=10:11 files* /somedir/ COPY接受一个标志–from=&lt;name|index&gt;，该标志可用于将源位置设置为FROM .. AS 主要用于多阶段构建，后面会举个例子来讲解多阶段构建，多阶段构建是17.05之后才出现的功能。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile RUN]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-RUN%2F</url>
    <content type="text"><![CDATA[RUN有两种形式 RUN command ( 该命令在shell中运行，默认情况下在Linux上是/bin/sh -c或windows的cmd /S /C) RUN [“executable”, “param1”, “param2”] (exec 形式) exec形式不会调用shell先展开变量，也就是不会解析ENV或者ARG的变量，所以一般来讲用得比较多的就是第一种形式，多行的话可以利用\换行。12345RUN .....\ &amp;&amp; addgroup -S nginx \ &amp;&amp; adduser -D -S -h /var/cache/nginx -s /sbin/nologin -G nginx nginx \ &amp;&amp; apk add --no-cache --virtual .build-deps \ ..... 这里要注意的是一个RUN是一层，dockerfile的一些涉及到文件的指令和RUN都会是新的一层，主要是构建过程实际上还是容器去commit，目的相同的RUN尽量合并在同一个RUN里减少大小。下面我做个例子来说明原因123FROM alpineRUN apk add wget &amp;&amp; wget https://www.baidu.com -O test.htmlRUN echo 123 &gt; test.html 构建并运行123456789101112131415161718192021222324252627282930# docker build -t test .Sending build context to Docker daemon 2.048kBStep 1/3 : FROM alpine ---&gt; cdf98d1859c1Step 2/3 : RUN apk add wget &amp;&amp; wget https://www.baidu.com -O test.html ---&gt; Running in 07bd55d265b8fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gzfetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz(1/1) Installing wget (1.20.3-r0)Executing busybox-1.29.3-r10.triggerOK: 6 MiB in 15 packages--2019-04-30 05:38:10-- https://www.baidu.com/Resolving www.baidu.com... 58.217.200.39, 58.217.200.37Connecting to www.baidu.com|58.217.200.39|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 2443 (2.4K) [text/html]Saving to: 'test.html' 0K .. 100% 21.8M=0s2019-04-30 05:38:10 (21.8 MB/s) - 'test.html' saved [2443/2443]Removing intermediate container 07bd55d265b8 ---&gt; 9420c50ef6f7Step 3/3 : RUN echo 123 &gt; test.html ---&gt; Running in 8724c012ff49Removing intermediate container 8724c012ff49 ---&gt; b924abffdb62Successfully built b924abffdb62Successfully tagged test:latest 运行然后查看docker的存储目录查找123456789[root@docker ~]# docker run --rm test cat test.html123[root@docker ~]# find /var/lib/docker/overlay2/ -type f -name test.html/var/lib/docker/overlay2/3c4530c7cd077e1d6ec74135679fe7234eddc88fe72ada21f632cebfd26de4f5/diff/test.html/var/lib/docker/overlay2/802018b95e4f9b16e9946e2e827db5c3b0cd8631ac0759c31dffea212ff06d4f/diff/test.html[root@docker ~]# cat /var/lib/docker/overlay2/3c4530c7cd077e1d6ec74135679fe7234eddc88fe72ada21f632cebfd26de4f5/diff/test.html123[root@docker ~]# cat /var/lib/docker/overlay2/802018b95e4f9b16e9946e2e827db5c3b0cd8631ac0759c31dffea212ff06d4f/diff/test.html&lt;!DOCTYPE html&gt; 发现两个文件都存在，前面说到了容器在读取文件的时候从上层往下查找，查找到了就返回，但是我的这个Dockerfile里第一个RUN下载了index页面，第二个改了文件内容。 可以证明一个RUN是一层，也证明了之前容器读取文件的逻辑。同时假设我们的目的是最终的123，我们可以俩个RUN合并了，这样就不会有多余的第一个RUN产生的test.html文件。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ARG]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ARG%2F</url>
    <content type="text"><![CDATA[ARG格式有两种123456ARG key----------------ARK key=valueARG key=value \ key2=value2 \ key3=value3 一般来讲第二种用得多，表明build的时候不传入变量设置默认值，无值就是第一种下用户在docker build的时候必须传入值，否则就报错。例如我们可以把nginx官方dockerfile的第一个ENV改成ARG，我们想构建哪个版本直接build的时候传入变量就行了。 当然ARG是唯一一个可以用于FROM前面的指令，例如下面这样我们可以通过命令行传递参数来改变FROM的base镜像。12ARG jdk=1.8xxxxFROM openjdk:$jdk Docker其实也预定了一些ARG方便我们构建的时候使用代理 HTTP_PROXY HTTPS_PROXY FTP_PROXY NO_PROXY]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ENV]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ENV%2F</url>
    <content type="text"><![CDATA[ENV写法有两种，后者支持写多个，一般多个的话也是使用后者居多1234567ENV key value-------ENV key=value key2=value2ENV key=value \ key2=value2 \ key3=value3 \ key4=value4 设置一个环境变量，可以被dockerfile里后续的指令使用，也在容器运行过程中保持，支持的指令为:1ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR ONBUILD 可以在dockerhub上发现各种官方镜像的Dockerfile的步骤都是固定的，新版本发布直接改下ENV后构建下即可1234567ENV NGINX_VERSION 1.15.11RUN .... \ &amp;&amp; curl -fSL https://nginx.org/download/nginx-$NGINX_VERSION.tar.gz -o nginx.tar.gz \ ... &amp;&amp; cd /usr/src/nginx-$NGINX_VERSION \ &amp;&amp; ./configure $CONFIG --with-debug \ ... 很多应用镜像启动都是先启动一个脚本，拼接一堆参数最终传递给应用的主进程当作参数，最常见的就是tomcat，或者说很多的应用基于tomcat。下面是之前我修改一个镜像Dockerfile摸索出的启动脚本的运行过程1234567891011+ '[' -r /opt/atlassian/jira/bin/setenv.sh ']'+ . /opt/atlassian/jira/bin/setenv.sh$ cat /opt/atlassian/jira/bin/setenv.sh...JAVA_OPTS="-Xms$&#123;JVM_MINIMUM_MEMORY&#125; -Xmx$&#123;JVM_MAXIMUM_MEMORY&#125; $&#123;JVM_CODE_CACHE_ARGS&#125; $&#123;JAVA_OPTS&#125; $&#123;JVM_REQUIRED_ARGS&#125; $&#123;DISABLE_NOTIFICATIONS&#125; $&#123;JVM_SUPPORT_RECOMMENDED_ARGS&#125; $&#123;JVM_EXTRA_ARGS&#125; $&#123;JIRA_HOME_MINUSD&#125; $&#123;START_JIRA_JAVA_OPTS&#125;"...export JAVA_OPTS...exec java $JAVA_OPTS 其中有一行：1JAVA_OPTS="... $&#123;JAVA_OPTS&#125; ..." 他拼接了自己，如果想给java在最终参数后添加一些固定参数时，可以在构建镜像声明JAVA_OPTS，例如添加时区我们应该在Dockerfile里设置1ENV JAVA_OPTS='-Duser.timezone=GMT+08' docker run可以指定env，ENV指令不一样是给Dockerfile用的，有时候是给容器启动时候用的，我们可以在docker run的时候指定env或者覆盖env达到不需要修改镜像，例如常见的后端需要连接一个mysql，可以在后端代码os.getEnv(“mysql_address”)，我们启动的时候指定mysql_address变量为真实的mysql地址即可。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile MAINTAINER]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-MAINTAINER%2F</url>
    <content type="text"><![CDATA[MAINTAINER已经弃用，推荐使用LABEL，例如nginx dockerfile里的 LABEL maintainer=”NGINX Docker Maintainers &#x64;&#x6f;&#x63;&#x6b;&#101;&#114;&#45;&#109;&#x61;&#x69;&#110;&#x74;&#x40;&#110;&#103;&#105;&#x6e;&#x78;&#46;&#x63;&#111;&#x6d;“]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile FROM]]></title>
    <url>%2F2019%2F04%2F29%2FDockerfile-FROM-1%2F</url>
    <content type="text"><![CDATA[FROM用法1FROM &lt;baseimage&gt; 或者 FROM &lt;baseimage&gt;:&lt;tag&gt; 指定从哪个镜像为基础迭代，如果本地没有镜像则会从仓库拉取，通常是第一行，而scratch是空镜像，是所有rootfs和一些单独可执行文件做镜像的根源，关于scratch后续会说。 例如centos的Dockerfile是下面12345678910FROM scratchADD centos-7-x86_64-docker.tar.xz /LABEL org.label-schema.schema-version="1.0" \ org.label-schema.name="CentOS Base Image" \ org.label-schema.vendor="CentOS" \ org.label-schema.license="GPLv2" \ org.label-schema.build-date="20190305"CMD ["/bin/bash"] 而hello-world为123456FROM scratchCOPY hello /CMD ["/hello"]docker images | grep hellohello-world latest fce289e99eb9 3 months ago 1.84kB nginx:alpine镜像的dockerfile 链接为https://github.com/nginxinc/docker-nginx/blob/7d7c67f2eaa6b2b32c718ba9d93f152870513c7c/mainline/alpine/Dockerfile，大家可以仿照这个经典案例写出自己的Dockerfile。 nginx:alpine既满足运行的最小环境下大小又很小，主要归功于FROM alpine ，现在alpine这个系统和rootfs得益于docker发展非常快，也有很多应用镜像都有alpine版本。 12345678910111213141516FROM alpine:3.9LABEL maintainer="NGINX Docker Maintainers &lt;docker-maint@nginx.com&gt;"ENV NGINX_VERSION 1.15.11RUN ...省略步骤，步骤是下载源码，安装编译需要的依赖，编译安装完删掉源码包和编译的依赖保留运编译出来的nginx二进制和需要的所有so文件COPY nginx.conf /etc/nginx/nginx.confCOPY nginx.vh.default.conf /etc/nginx/conf.d/default.confEXPOSE 80STOPSIGNAL SIGTERMCMD ["nginx", "-g", "daemon off;"]]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像03]]></title>
    <url>%2F2019%2F04%2F29%2FDocker%E9%95%9C%E5%83%8F03%2F</url>
    <content type="text"><![CDATA[构建镜像构建镜像只有两种方式，docker build 和 docker commit。实际上docker build是调用的docker commit。不推荐手动去docker commmit运行的容器成镜像。所以主要讲docker build和dockerfile。 使用docker build 指定Dockerfile来完成一个新镜像的构建。命令格式一般为：1docker build [option] [-t &lt;image&gt;:&lt;tag&gt;] &lt;path&gt; 其中path指向的文件称为context（上下文），context包含docker build镜像过程中需要的Dockerfile以及其他的资源文件。执行build命令后执行流程如下： Docker client端 解析命令行参数，完成对相关信息的设置，Docker client向Docker server发送POST/build的HTTP请求，包含了所需的上下文文件。 Docker server端 创建一个临时目录，并将context指定的文件系统解压到该目录下 读取并解析Dockerfile 根据解析出的Dockerfile遍历其中的所有指令，并分发到不同的模块（parser）去执行 parser为Dockerfile的每一个指令创建一个对应的临时容器，在临时容器中执行当前指令，然后通过commit使用此镜像生成一个镜像层 Dockerfile中所有的指令对应的层的集合，就是此次build后的结果。如果指定了tag参数，便给镜像打上对应的tag。最后一次commit生成的镜像ID就会作为最终的镜像ID返回。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像02]]></title>
    <url>%2F2019%2F04%2F29%2FDocker%E9%95%9C%E5%83%8F02%2F</url>
    <content type="text"><![CDATA[容器是单独的一层读写层一个镜像可以运行无数个容器，容器需要读取文件的场景和对应原理是如下。 在无挂载卷情况下，通过docker cp或exec产生的数据，文件会在读写层里，删除容器则文件也一并删除。 读取文件，从上层往下找到镜像层，找到即可返回，复制到容器层读入内存。 修改文件，从上层往下找到镜像层，找到即可返回，复制到容器层后修改。 删除文件，找到后在容器层记录下删除操作(类似盖层布，后续读取的时候会认为文件不存在) 容器与镜像关系为下图 通过docker ps 的-s选项可以看出容器的size和容器层总大小，这里我用docker命令演示下容器是单独一层读写层和容器被删除后数据消失。 创建一个容器，在容器里写入1g数据，宿主机的可用容量减少1G，docker的overlay2存储目录记录了下这个文件，但是删除后文件也被删除了。在抽象逻辑上一个容器就是单独一个读写层，而删除容器后这层在宿主机上的文件也会被删除。 计算实际占用大小时镜像的大小不会被重复计算，只需要计算一个大小+它起的所有容器大小。目前启动了5个nginx1234567# docker ps -sCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZEac5c0d1f8d6b nginx:alpine "nginx -g 'daemon of…" 7 seconds ago Up 5 seconds 80/tcp web5 2B (virtual 16.1MB)0dd2c0c36084 nginx:alpine "nginx -g 'daemon of…" 11 seconds ago Up 9 seconds 80/tcp web4 2B (virtual 16.1MB)413d9270c702 nginx:alpine "nginx -g 'daemon of…" 15 seconds ago Up 13 seconds 80/tcp web3 2B (virtual 16.1MB)43f8e010f7bb nginx:alpine "nginx -g 'daemon of…" 19 seconds ago Up 17 seconds 80/tcp web2 2B (virtual 16.1MB)610abcfab29d nginx:alpine "nginx -g 'daemon of…" 25 seconds ago Up 23 seconds 80/tcp web1 2B (virtual 16.1MB) 使用exec往容器里写数据123456789101112docker exec web1 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=10'docker exec web2 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=20'docker exec web3 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=30'docker exec web4 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=40'docker exec web5 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=50']# docker ps -asCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZEac5c0d1f8d6b nginx:alpine "nginx -g 'daemon of…" 56 seconds ago Up 54 seconds 80/tcp web5 50MB (virtual 66.1MB)0dd2c0c36084 nginx:alpine "nginx -g 'daemon of…" About a minute ago Up 58 seconds 80/tcp web4 40MB (virtual 56.1MB)413d9270c702 nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 80/tcp web3 30MB (virtual 46.1MB)43f8e010f7bb nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 80/tcp web2 20MB (virtual 36.1MB)610abcfab29d nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 80/tcp web1 10MB (virtual 26.1MB) 实际占用量计算123456781 x 16.1MB 只读镜像层1 x 10MB1 x 20MB1 x 30MB1 x 40MB1 x 50MB===========================161.1MB 这样我们可以推导出docker镜像是分层和容器是单独一层只读镜像的。也有部分人不懂这些知识，每次是进容器里安装东西然后commit，导致最后容器越来越大，甚至看到过16g的镜像。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像01]]></title>
    <url>%2F2019%2F04%2F29%2FDocker%E9%95%9C%E5%83%8F01%2F</url>
    <content type="text"><![CDATA[Docker镜像层镜像为什么是有层的？镜像分层是为了解决什么？ 虽然镜像解决了打包，但是实际应用中我们的应用都是基于同一个rootfs来打包和迭代的，难道每个rootfs都会多份吗？ 为此docker利用了存储驱动AUFS，devicemapper，overlay，overlay2的存储技术实现了分层。初期是AUFS，到现在的overlay2驱动（不推荐devicemapper坑很多）。例如一个nginx:alpine和python:alpine镜像可以从分层角度这样去理解。 实际上只有不同的层才占据存储空间，相同的层则是引用关系。抽象地看镜像是一个实体，实际上是/var/lib/docker目录里的分层文件外加一些json和db文件把层联系起来组成了镜像。存储路径是/var/lib/docker/存储驱动类型/。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[proxmox中cloud-init使用方法]]></title>
    <url>%2F2019%2F04%2F18%2Fproxmox%E4%B8%ADcloud-init%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[模版制作标准分区ext4，不添加swap分区，原因下文说。 系统装完后，将网卡配置文件内的onboot打开，清除uuid。关闭selinux和firewalld以及碍事的NetworkManager。123systemctl disable --now firewalld NetworkManagersetenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config 为了让虚拟化层可以重启和关闭虚拟机，必须安装acpid服务；为了使根分区正确调整大小安装cloud-utils-growpart，cloud-init支持下发前设置信息写入。12yum install -y acpid cloud-init cloud-utils-growpartsystemctl enable acpid 禁用默认zeroconf路线1echo "NOZEROCONF=yes" &gt;&gt; /etc/sysconfig/network 防止ssh连接使用dns导致访问过慢12sed -ri '/UseDNS/&#123;s@#@@;s@\s+.+@ no@&#125;' /etc/ssh/sshd_configsystemctl restart sshd cloud-init配置文件:1. ssh_pwauth 为 0 是禁止使用password登陆。2. disable_root：1 是禁止root登陆。3. package-update-upgrade-install会在第一次开机启动时自动yum update -y。123sed -ri '/disable_root/&#123;s#\S$#0#&#125;' /etc/cloud/cloud.cfgsed -ri '/ssh_pwauth/&#123;s#\S$#1#&#125;' /etc/cloud/cloud.cfgsed -ri '/package-update/s@^@#@' /etc/cloud/cloud.cfg 默认cloud-init会创建一个系统类型的用户,可以注释掉。1234567# default_user:# name: centos# lock_passwd: true# gecos: Cloud User# groups: [wheel, adm, systemd-journal]# sudo: ["ALL=(ALL) NOPASSWD:ALL"]# shell: /bin/bash 安装些基础包和预设一些脚本的话就可以关机。12yum install vim git wget -ypoweroff 转换模版12345678root@pve:~# qm list VMID NAME STATUS MEM(MB) BOOTDISK(GB) PID 100 cloud-init stopped 2048 20.00 0 101 k8s-m1 running 2048 20.00 7438root@pve:~# qm set 100 --ide2 local-lvm:cloudinitupdate VM 100: -ide2 local-lvm:cloudinit Using default stripesize 64.00 KiB. Logical volume "vm-100-cloudinit" created. 在Dashboard上可以看到虚拟机的could-init部分已经可以更改属性了。 在Dashboard上把它转换成模板,部署时完整克隆,开机之前双击需要设置的信息即可,否则例如密码不设置默认是模板的密码。也可以通过命令行初始化虚拟机信息。1qm set &lt;vmid&gt; --ipconfig0 ip=10.105.26.x/23,gw=10.105.26.1 备份和恢复虚拟机123456789101112131415161718192021222324252627282930313233root@pve:~# vzdump 100INFO: starting new backup job: vzdump 100INFO: Starting Backup of VM 100 (qemu)INFO: status = stoppedINFO: update VM 100: -lock backupINFO: backup mode: stopINFO: ionice priority: 7INFO: VM Name: cloud-initINFO: include disk 'scsi0' 'local-lvm:vm-100-disk-0' 20GINFO: creating archive '/var/lib/vz/dump/vzdump-qemu-100-2019_04_18-12_48_38.vma'INFO: starting kvm to execute backup taskTotal translation table size: 0Total rockridge attributes bytes: 417Total directory bytes: 0Path table size(bytes): 10Max brk space used 0178 extents written (0 MB)INFO: started backup task 'd65a8f26-20fe-4232-abd3-ec0bcf4623cd'INFO: status: 3% (785645568/21474836480), sparse 1% (395206656), duration 3, read/write 261/130 MB/sINFO: status: 21% (4593876992/21474836480), sparse 19% (4184059904), duration 6, read/write 1269/6 MB/sINFO: status: 34% (7457996800/21474836480), sparse 32% (6929133568), duration 9, read/write 954/39 MB/sINFO: status: 50% (10746396672/21474836480), sparse 46% (10083291136), duration 12, read/write 1096/44 MB/sINFO: status: 61% (13169524736/21474836480), sparse 57% (12349382656), duration 15, read/write 807/52 MB/sINFO: status: 70% (15039004672/21474836480), sparse 64% (13956280320), duration 18, read/write 623/87 MB/sINFO: status: 80% (17196580864/21474836480), sparse 74% (15934279680), duration 21, read/write 719/59 MB/sINFO: status: 89% (19120455680/21474836480), sparse 82% (17667883008), duration 24, read/write 641/63 MB/sINFO: status: 95% (20594622464/21474836480), sparse 88% (18997477376), duration 27, read/write 491/48 MB/sINFO: status: 100% (21474836480/21474836480), sparse 92% (19877691392), duration 28, read/write 880/0 MB/sINFO: transferred 21474 MB in 28 seconds (766 MB/s)INFO: stopping kvm after backup taskINFO: archive file size: 1.49GBINFO: Finished Backup of VM 100 (00:00:32)INFO: Backup job finished 输出路径在：/var/lib/vz/dump/，导入的话使用如下命令1qmrestore vzdump-qemu-xx.vma &lt;vmid&gt;]]></content>
      <categories>
        <category>Proxmox</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s配置secret拉取私有仓库镜像]]></title>
    <url>%2F2019%2F04%2F10%2Fk8s%E9%85%8D%E7%BD%AEsecret%E6%8B%89%E5%8F%96%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[https://kubernetes.io/docs 样例1234567891011# cat ~/.docker/config.json &#123; "auths": &#123; "harbor.station.com": &#123; "auth": "YWRtaW46SGFyYm9yMTIzNDU=" &#125; &#125;, "HttpHeaders": &#123; "User-Agent": "Docker-Client/18.06.1-ce (linux)" &#125;&#125; 这个时候我们虽然可以通过交互式登录，使用docker pull拉取镜像，但无法通过k8s创建Pod时拉取镜像。 生成密钥secret1# kubectl create secret docker-registry harbor --docker-server=x.x.x.x --docker-username=admin --docker-password=Harbor12345 --docker-email=xx@qq.com 1) harbor: 指定密钥的键名称，可自行定义 2）–docker-server：指定docker仓库地址 3）–docker-username：指定docker仓库帐号 4) –docker-password：指定docker仓库密码 5) –docker-email：指定邮件地址（选填）` 查看密钥可以看到当前除了默认的密钥, 还有我们刚才生成的. 另外要注意的是, 该密钥只能在对应namespace使用, 也就是这里的default, 如果需要用到其他namespace, 比如说test, 就需要在生成的时候指定参数 -n test。1234# kubectl get secretsNAME TYPE DATA AGEdefault-token-mzmtj kubernetes.io/service-account-token 3 22mharbor kubernetes.io/dockerconfigjson 1 22m YAML例子其中imagePullSecrets是声明拉取镜像时需要指定密钥, harbor必须和上面生成密钥的键名一致, 另外检查一下pod和密钥是否在同一个namespace, 之后k8s便可以拉取镜像。1234567891011121314151617apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-deploy namespace: harborspec: replicas: 3 template: metadata: labels: app: web_server spec: containers: - name: nginx image: harbor.station.com/library/nginx:latest imagePullSecrets: - name: harbor]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Harbor</tag>
      </tags>
  </entry>
</search>
