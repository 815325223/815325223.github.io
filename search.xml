<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pod 的 lifecycle]]></title>
    <url>%2F2019%2F09%2F20%2FPod-%E7%9A%84-lifecycle%2F</url>
    <content type="text"><![CDATA[Pod phasePod 的 status 字段是一个 Pod Status 对象，Pod Status 对象中有一个 phase 字段。phase 拥有以下几种值： Pending：Kubernetes 已经接受 Pod 实例的创建，但其中包含一个或者多个容器并未创建成功。Pending 包含调度和通过网络下载镜像的时间。 Running：Pod 已经被调度到某个节点上，Pod 中所有的容器都已经创建。此时至少有一个容器正在运行，或正处于启动、重启状态。 Succeeded：Pod 中的所有容器正常终止，不会再起启动。 Faild：Pod 中所有容器都已经终止，至少有一个容器非正常结束，例如：退出状态码为 0，被系统 OOME。 Unknown：无法取得 Pod 状态，一般为与 Pod 所在节点的 kubelet 失去通信。 容器探针 liveness probe：kubelet 通过使用此类探针确定应用程序是否还运行，如果探测结果失败，则 kubelet 会杀死容器，并且容器将受到重启策略的影响；不提供探针，则默认状态为 success。 Pod 重启策略 PodSpec 中有一个 restartPolicy 字段，拥有三个值，Always、OnFailure、Never。默认为 Always。适用于 Pod 中的所有容器。restartPolicy 仅通过同一节点上的 kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数延迟（10s,20s,40s）重新启动 readiness probe：kubelet 使用此类探针确定应用程序是否已经就绪可以接受流量进来。当 Pod 中的容器处于就绪状态的时候，kubelet 才会认为 Pod 处于就绪状态（一个 Pod 下可能存在多个容器）。否则，kubelet 会把未就绪的 Pod 从 Service 中移除，此时流量就不会被路由到未就绪的 Pod 上。 探针是由 kubelet 对容器执行的定期检查，调用由容器实现的 Handler。有三种类型处理程序： ExecAction：在容器内执行自定义命令，如果命令退出返回码为 0 ，则认为成功。 TCPSocketAction：对指定端口上的 IP 地址进行 TCP 检查。 HTTPGetAction：对指定的端口和路径上的容器 IP 地址执行 HTTP Get 请求。响应状态码 大于等于200且小于400，则认为成功。 liveness exec 探针示例123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: liveness-exec labels: demo: livenessspec: containers: - name: liveness image: busybox:1.28 args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 liveness HTTP Get 探针示例123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: liveness-http labels: demo: livenessspec: containers: - name: liveness image: 815325223/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 timeoutSeconds：探测超时时间，默认1秒，最小1秒。successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是 1，但是如果是liveness 则必须是 1。最小值是 1。failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是 3，最小值是 1。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kubelet 状态更新机制]]></title>
    <url>%2F2019%2F08%2F07%2FKubelet-%E7%8A%B6%E6%80%81%E6%9B%B4%E6%96%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[kubernetes-reliability 当 Kubernetes 中 Node 节点出现状态异常时，节点上的 Pod 会被重新调度到其他节点上，由于和 Kubelet 的状态更新机制有关，当节点宕机后，Pod 并不会立即触发重新调度到其他节点。 kubelet 自身会定期更新状态到 kube-apiserver，通过参数 –node-status-update-frequency 指定上报频率，默认为10s 上报一次。 kube-controller-manager 通过参数 –node-monitor-period 检查 kubelet 的状态，默认为5s。 当 node 被 kubernetes 判定为 notready 状态，通过 –node-monitor-grace-period 参数配置，默认为40s。 当 node 被 kubernetes 判定为 unhealthy 状态，通过 –node-startup-grace-period 参数配置，默认为1m。 kubernetes 开始删除原 node 上的 pod，通过 –pod-eviction-timeout 参数配置，默认为5m。 kube-controller-manager 和 kubelet 都是异步工作的，在这过程中，可能会因为网络延迟、apiserver 的延迟、etcd 延迟、节点负载导致的延迟。所以，如果 –node-status-update-frequency 设置为5s，实际上，etcd 中的数据变化则会在 6～7s。 Kubelet 在更新状态失败时，会进行重试，默认为5次，通过配置参数 nodeStatusUpdateRetry。Kubelet 会在函数 tryUpdateNodeStatus 中尝试进行状态更新。Kubelet 使用了 golang 中的 http.Client()方法，但是没有至指定超时时间，所以，如果 apiserver 过载时，当建立 TCP 连接时可能会出现一些故障。因此，在 nodeStatusUpdateRetry * –node-status-update-frequency 时间后才会更新一次节点状态。同时，Kubernetes 的 controller manager 将尝试每–node-monitor-period 时间周期内检查 nodeStatusUpdateRetry 次。在 –node-monitor-grace-period 之后，会认为节点 unhealthy，然后会在 –pod-eviction-timeout 后删除 pod。kube-proxy 拥有一个 watch API，一旦 pod 被驱逐了，kube-proxy 将会通知更新节点的 iptables 规则，将 pod 从 service 的 endpoints 中移除，保证不会访问到来自故障节点的 pod。 对于这些参数的配置，需要根据不同的集群规模进行选择。 社区默认配置 参数 值 –node-status-update-frequency 10s –node-monitor-period 5s –node-monitor-grace-period 40s –pod-eviction-timeout 5m 快速更新和快速响应 参数 值 –node-status-update-frequency 4s –node-monitor-period 2s –node-monitor-grace-period 20s –pod-eviction-timeout 30s 在这种情况下，pod 将在50s 内逐出，因为该节点在20s 后将被视为 down，并且–pod-eviction-timeout 在30s 之后发生。 但是，这种情况会在 etcd 上产生开销，因为每个节点都会尝试每2秒更新一次状态。如果环境有1000个节点，那么每分钟将有15000个节点更新，这可能需要大型etcd容器甚至是etcd的专用节点。 如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。 中等更新和平均响应 参数 值 –node-status-update-frequency 20s –node-monitor-period 5s –node-monitor-grace-period 2m –pod-eviction-timeout 1m 在这种情况下，Kubelet 将尝试每20秒更新一次状态。因此，在 controller manager 在节点的不健康状态之前，它将是6 * 5 = 30次尝试。1m后，它将驱逐所 pod。驱逐前的总时间为3m。这种情况适用于中等环境，因为1000个节点每分钟需要3000个etcd更新。 实际上，将有4到6个节点更新尝试。 尝试总次数从20到30不等。 低更新和慢响应 参数 值 –node-status-update-frequency 1m –node-monitor-period 5s –node-monitor-grace-period 5m –pod-eviction-timeout 1m 在这种情况下，每个 kubelet 都会尝试每分钟更新一次状态。在不健康状态之前将有5 * 5 = 25次尝试。5m后，controller manager 将设置不健康状态。 这意味着 pod 在被标记为不健康后1m后将被驱逐。(总共花费6m）。 实际上，将有3到5次尝试。 尝试总次数从15到25不等。]]></content>
  </entry>
  <entry>
    <title><![CDATA[在 Kubernetes 上运行 Python 的最佳实践]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%9C%A8-Kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C-Python-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[创建 Dockerfile123456789FROM python:3.7-alpineRUN mkdir /appWORKDIR /appADD . /app/RUN pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txtEXPOSE 5000CMD ["python", "/app/main.py"] 从 Docker Hub 获取 3.7-alpine 官方 Python 镜像，在镜像中创建名为 app 的目录，将它设置为工作目录，将本地目录的文件复制到 app 文件夹中，运行 pip 安装程序依赖的包，监听5000端口，配置容器启动时的命令。 创建镜像1docker build -f Dockerfile -t hello-python:latest . 在 Docker 上直接运行1docker run --rm -p 5000:5000 hello-python:latest 通过浏览器访问 http://IP:5000 在 Kubernetes 上运行12345678910111213141516171819202122232425262728293031apiVersion: v1kind: Servicemetadata: name: hello-python-servicespec: selector: app: hello-python ports: - protocol: "TCP" port: 6000 targetPort: 5000 type: NodePort---apiVersion: apps/v1beta1kind: Deploymentmetadata: name: hello-pythonspec: replicas: 4 template: metadata: labels: app: hello-python spec: containers: - name: hello-python image: hello-python:latest imagePullPolicy: Never ports: - containerPort: 5000]]></content>
  </entry>
  <entry>
    <title><![CDATA[EKS 与 EC2 Spot 最佳实践]]></title>
    <url>%2F2019%2F07%2F17%2FEKS-%E4%B8%8E-EC2-Spot-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[前段时间去参加了 KubeCon 2019，听了胖虎老师的演讲，非常棒，在这里分享一下，希望给大家一些启发。 EC2 SpotEC2 Spot 实例可以利用 AWS 云中未使用的 EC2 资源。与按需实例的价格相比，使用 Spot 实例最高可以享受 90%的折扣。可以将 Spot 实例用于各种无状态、容错或者灵活的应用程序，例如大数据、容器化工作负载、CI/CD、Web服务器、高性能计算 (HPC) 以及其他测试和开发工作负载。Spot 实例与 Auto Scaling、EMR、ECS、CloudFormation、Data Pipeline 和 AWS Batch 等 AWS 服务紧密集成，因此可以选择如何启动和维护 Spot 实例上运行的应用程序。 EKS 结合 EC2 Spot由于 Kubernetes 维持副本期望状态，Spot 特别适合来创建一个高可用兼顾费用优化的 Kubernetes 集群。我们会利用 Auto Scaling Group多个实例类型与购买选项的新特性，将一个数量的 on-demand 实例与 Spot 实例混合在 一个 Auto Scaling Group 当中，除了保证具有一定的基线数量（baseline）由 on-demand 支撑之外，整个集群的横向扩展则由 Spot 与指定比例的 on-demand 来进行扩展。 架构图 使用 ASG 创建一个 node group，并且注册到 EKS 的 control plane。这个 ASG 由若干个数量的 on-demand 与 spot 组成。Spot 一旦面临关机事件，会触发一个关机事件处理机制（termination handing），提前对 Spot 实例进行 node draining，确保上面运行的 Pod 可以提前被重新调度（reschedule）到其他节点。 创建 EKS 集群1234567891011121314151617---apiVersion: eksctl.io/v1alpha5kind: ClusterConfigmetadata: name: eksdemo region: ap-northeast-1nodeGroups: - name: ng-1 minSize: 4 maxSize: 8 instancesDistribution: instanceTypes: ["t3.small", "t3.medium", "t3.large"] # At least two instance types should be specified onDemandBaseCapacity: 2 onDemandPercentageAboveBaseCapacity: 0 spotInstancePools: 2 通过以上配置清单利用 eksctl 就可以在 ap-northeast-1 region 创建一个名为 eksdemo 的 EKS 集群，同时建立起一个名为 ng-1的nodegroup ，当中由多种 instance types 组成，其中 OnDemand 基线(baseline)为 2，意味着保证会有两台 on-demand 实例，而其他的实例则由 ondemand+spot 组成，但由于我们定义了 onDemandPercentageAboveBaseCapacity 为 0, 意味着额外需要的两台实例(minSize-onDemandBaseCapacity)全部由 spot 来满足。关于 ASG 更多参数可以参考该 链接，更多的 cluster YAML 配置示例可以参考 weaveworks 的 github 使用 eks-refarch 模板创建除了使用 eksctl 之外，也可以透过 aws-samples/amazon-eks-refarch-cloudformation 来创建这样的混合集群，amazon-eks-refarch-cloudformation 使用100%的 cloudformation 模版，并且提供丰富的特性。 node labels, taints and tolerations通过 eks-templates 模版创建的混合集群，会自动将 ondemand 与 spot 实例都打上不同的 node labels，同时也会对 Spot 打上相应的 taints。 验证集群通过 kubectl get nodes –show-labels 列出所有节点，可以看到 ondemand 与 Spot 各自带上了不同的 lifecycle 标签。在 EC2 Console，查看 Auto Scaling Group，会看到相应的配置。 部署 eks-lambda-drainerSpot 实例当面临资源回收，强迫被关机的时候，系统会提前两分钟收到通知，我们可以借由 CloudWatch Event 捕捉到这个通知信息出发一个外部调度的 Lambda function 对这个节点进行 node draining。eks-lambda-drainer 可以帮助我们部署一个完全 serverless 独立于 Kubernetes 集群之外的无服务器时间响应 handler，并且监听整个 VPC 内的 Spot实例关机信号，一旦提前两分钟获得关机信号就会对这个节点进行 kubectl drain 操作，确保节点上的Pod 能被及时重新调度到其他节点上运行。 关于 eks-lambda-drainer 的部署方式，可以参考这个 Github 测试 Spot实例 关机事件处理eks-lambda-drainer 部署完成，当 Spot 实例准备关机的时候，eks-lambda-drainer 会开始进行相应处理，从 Lambda Log 可以看到如下的信息，表示确实对 node 节点进行了 kubectl drain 的操作。 总结EKS 结合 EC2 Spot 是一个非常好的组合，常见的部署模式是把重要的controller 或 agent 指定部署到 on demand 节点，而其他应用则部署到 Spot 实例，甚至也可以全部都部署到 Spot 实例。只要在 ASG 里面选择多种节点类型，并且同时选择至少三个 AZ，如此 ASG 就会尽可能维护所需要的节点数量保证一定的高可用性。 公开案例知名在线旅游服务商 Skyscanner 在 This is My Architecture系列影片当中揭露了他们在 AWS上面构建 Kubernetes 集群，横跨全球多个区域，多个集群，尖峰时间支撑每秒 60-75K QPS，以及每个月8千万个月独立用户，全部100% 使用 EC2 Spot 实例构建 Kubernetes 集群。 SkyScanner: Building Highly-Available, Multi-Region Kubernetes Clusters on 100% Amazon EC2 Spot]]></content>
  </entry>
  <entry>
    <title><![CDATA[IngressController 高可用方案]]></title>
    <url>%2F2019%2F07%2F17%2FIngressController-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[流量从入口到 Ingress Controller Pod 存在如下几种方式： Type 为 LoadBalancer 的时候，通过 externalIPs。 Type 为 LoadBalancer 的时候，通过云厂商的 LB 来负载均衡，并且支持自动分配公网 IP。通过 LoadBalancer 暴露的每个服务都会获得一个公网 IP，缺点是需要额外收费，且自己在云上搭建的 Kubernetes 无法使用。 不创建 Service，Pod 直接用 HOSTPORT，与 hostNetwork 功能类似，如果代理的四层服务，如 mysql，需要修改 Pod 的 template 来滚动更新，让 Nginx Bind 的四层端口能映射到宿主机上。 Type 为 NodePort，流量通过它负载进来，但是不在 Ingress Controller Pod 所在的 Node 上，会从这个 Node 上的 kube-proxy 转发到 Ingress Controller 的 Pod 上。 综上所述，不创建 Service 的效率最高，负载四层的时候也不用修改 Pod 的 template。在 hostNetwork 模式下，Pod 会继承宿主机的网络协议，继承宿主机的 DNS，导致 Service 的请求直接走宿主机上的公网 DNS 服务器，而不是集群内部的 DNS 服务器，需要将 Pod 的 dnsPolicy 设置为 ClusterFirstWithHostNet 即可解决。 高可用的部署方式一般有两种： DS + nodeSelector Deploy（设置 repicas）+ nodeSelector + Pod 反亲和性 线下使用 VIP 飘在存活的 Ingress Controller Pod 的宿主机上，云上使用 SLB 代替 VIP，有条件直接上硬件负载均衡，例如 F5 等。Nginx ingress 不创建 Service 的时候，会一直报 warning 告警日志，对机器是一直没必要的额外负载，可以随便创建一个同名 Service，不带 标签选择器和 ClusterIP 为 null 即可。关于域名请求，如果部署在公司局域网，内部有 DNS Server 的话，域名解析到 Ingress Controller 的宿主机 IP，或者 VIP、LB 的 IP，否则需要修改每个人电脑的 /etc/hosts 文件。如果没有 DNS Server 的可以运行一个 external-dns，它的上游 DNS 是公网的 DNS 服务器，局域网内电脑的 DNS Server 指向它即可，云上的话把域名请求解析到对应的 IP 即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[在Linux内快速查找文件中包含的关键字]]></title>
    <url>%2F2019%2F06%2F12%2F%E5%9C%A8Linux%E5%86%85%2F</url>
    <content type="text"><![CDATA[查找根目录下包含主机名的所有文件1$ find / -type f | xargs grep -i "hostname" 2&gt;/dev/null]]></content>
  </entry>
  <entry>
    <title><![CDATA[浅谈Traefik]]></title>
    <url>%2F2019%2F06%2F12%2F%E6%B5%85%E8%B0%88Traefik%2F</url>
    <content type="text"><![CDATA[IngressIngress 是授权入站连接到达集群服务的规则集合。 从外部流量调度到 NodePort 上的 Service 从 Service 调度到 Ingress Controller Ingress Controller 根据 Ingress 中定义的虚拟主机或后端URL 根据虚拟主机名调度到后端的一组 Pod Ingress ControllerIngress Controller是一个统称，是一类工具的集合。 Ingress Nginx：Kubernetes 官方维护方案，魔改版的 Nginx Traefik：开源 HTTP 方向代理与负载均衡，支持 Ingress Ingress Kong：开源 API gateway 解决方案所开发的 Kubernetes Ingress Controller F5 BIG-IP Controller：F5 所开发的 Ingress Controller Ingress的引入目前Kubernetes暴露服务的方式只有三种：LoadBalancer Service、ExternalName、NodePort Service，而 Cluster IP 更不能被集群外的主机所访问，Service 虽然解决了服务发现和负载均衡，但只支持4层负载均衡，并不支持7层。NodePort 不易于管理庞大的端口、Rolling update 效率低下，需要额外搭建额外的负载均衡，只建议用于测试；LoadBalancer 必须运行在支持Cloud Provider 之上，而且需要花费额外的费用购买弹性 IP。我们运行的大多数服务都是应用层 HTTP(S)，从长远角度考虑，想把集群内部的 Service 暴露到集群外，Ingress Controller 都是比较好的方案，它反向代理集群内的七层服务，从而通过 vhost 子域名那样路由到后端的服务。 Traefik先贴一张 Traefik 官方图，api.domain.com 将流量路由到集群内的 API pod，backoffice.domain.com 流量路由到一组名为 backoffice 的 pod 上。试想一想，在传统的 Nginx 上要想额外再增加 admin 服务该如何处理？Nginx 可以通过虚拟主机域名区分不同的服务，而每个服务通过 upstream 进行定义不同的负载均衡池，再加上 location 进行负载均衡的反向代理，只需要修改 nginx.conf 即可实现，但是在 Kubernetes 中又该如何实现这种方式调度呢？Ingress 就是把以上动作抽象出来，变为一个 Ingress 对象，创建和更新 YAML 即可，然后 Ingress Controller 通过与 API Server 交互，动态的去感知集群中 Ingress 规则的变化来生成 Nginx 配置文件，最后 reload 生效更改后的配置。 Traefik部署详解RBACKubernetes 在1.6+中引入了基于角色的访问控制（RBAC），以允许对 Kubernetes 资源和 API 进行细粒度控制。 如果群集配置了 RBAC，则需要授权 Traefik 使用 Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的 RoleBindings 或单个全局 ClusterRoleBinding。 每个命名空间的 RoleBinding 可以限制授予权限，只有 Traefik 正在监视的名称空间才能使用，从而遵循最小权限原则。如果 Traefik 不应该监视所有名称空间，并且名称空间集不会动态更改，那么这是首选方法。否则，必须使用单个 ClusterRoleBinding。 traefik-rbac.yaml12345678910111213141516171819202122232425262728293031323334353637---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerrules: - apiGroups: - "" resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controllersubjects:- kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system 通过Deployment或DaemonSet部署 当使用 Deployment 部署时，可伸缩性更好。因为当使用 DaemonSet 时，每个节点会有一个 traefik-ingress-controller，当然你可以通过标签去控制它在哪些节点上启用。 DaemonSet 可以使用 NET_BIND_SERVICE 功能运行，这将允许它绑定到每个主机上的端口80/443。这将允许绕过 kube-proxy，并减少流量跳跃。请注意，这违反了 Kubernetes 最佳实践指南，并提出了调度/扩展问题的可能性。尽管存在潜在问题，但这仍然是大多数入口控制器的选择，关于单点的问题，如何做 Ingress Controller 将再往后讨论。 traefik-deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556---apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: replicas: 1 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 containers: - image: traefik name: traefik-ingress-lb ports: - name: http containerPort: 80 - name: admin containerPort: 8080 args: - --api - --kubernetes - --logLevel=INFO---kind: ServiceapiVersion: v1metadata: name: traefik-ingress-service namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web - protocol: TCP port: 8080 name: admin type: NodePort 将公开两个允许访问入口和 Web 界面的 NodePort。 traefik-ds.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960---apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 containers: - image: traefik name: traefik-ingress-lb ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO nodeSelector: edgenode: "true"---kind: ServiceapiVersion: v1metadata: name: traefik-ingress-service namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web - protocol: TCP port: 8080 name: admin 这将创建一个在主机上使用特权端口80/8080的守护进程。这可能不适用于所有提供程序，但说明了静态（非NodePort）hostPort绑定。仍然可以在群集内部使用 traefik-ingress-service 来访问 DaemonSet Pod。 如果有多个边缘节点，使用 nodeSelector 选择边缘节点来调度 traefik-ingress-controller 运行在多个节点，在边缘节点前放置一个负载均衡器，如：nginx、keepalived，将所有边缘节点作为负载均衡器的后端。12kubectl label nodes k8s-n1 edgenode=truekubectl label nodes k8s-n2 edgenode=true 验证效果123kubectl get ds -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEtraefik-ingress-controller 2 2 2 2 2 edgenode=true 13m Deployment 部署与 DaemonSet 部署之间的差异：前者具有更容易的向上和向下扩展性。它可以实现完整的 Pod 生命周期，并支持 Kubernetes 1.2 的滚动更新。运行部署至少需要一个 Pod。后者会自动扩展到满足特定选择器的所有节点，并保证一次填充一个节点。 Kubernetes 1.7 也完全支持滚动更新，适用于 DaemonSets。 Ingress 对象上面通过 NodePort 来访问 Traefik 的 Dashboard，如何通过 ingress 访问？首先需要创建一个 ingress 对象。123456789101112131415apiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-web-ui namespace: kube-system annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: traefik.demo.com http: paths: - backend: serviceName: traefik-ingress-service servicePort: admin 最关键的是 rules 部分，这里为 Traefik 的 Dashboard 定义一个 ingress 对象，serviceName 对应的是上面创建的 traefik-ingress-service，为了避免端口更改，对应的 servicePort 则是 admin。 验证： 如果无 DNS Server，可以修改本地计算机的 /etc/hosts 文件，添加 traefik.demo.com 对应的 Ingress Controller 所在的 Node IP，或者是 Ingress Controller 的 VIP。 在浏览器访问 http://traefik.demo.com，hostPort 保证了直接用域名访问，而不用增加端口号访问的问题。 TLS 认证目前大部分场景都会使用 https 来访问我们的服务，可以使用自签名或者从正规机构购买的 CA 证书，保证任何人访问的时候，浏览器会显示受信任的证书，也就是 Chrome 的小绿盾。1$ openssl req -newkey rsa:2048 -nodes -keyout tls.key -x509 -days 365 -out tls.crt 创建 secret 对象1kubectl create secret generic traefik-cert --from-file=tls.crt --from-file=tls.key -n kube-system 配置 Traefik12345678910111213defaultEntryPoints = ["http", "https"][entryPoints] [entryPoints.http] address = ":80" [entryPoints.http.redirect] entryPoint = "https" [entryPoints.https] address = ":443" [entryPoints.https.tls] [[entryPoints.https.tls.certificates]] CertFile = "/ssl/tls.crt" KeyFile = "/ssl/tls.key" 上面的配置文件中，配置了 http 和 https 两个入口，并且配置了 http 强制跳转到 https 服务，访问 https 服务，证书是必不可少的，CertFile 和 KeyFile 指定了两个文件，但是在 Traefik Pod 中并不存在这两个证书，可以通过 ConfigMap 将 traefik.toml 配置文件挂载到 Traefik Pod 中。1$ kubectl create configmap traefik-conf --from-file=traefik.toml -n kube-system 改造 Traefik Pod 配置清单1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 volumes: - name: ssl secret: secretName: traefik-cert - name: config configMap: name: traefik-conf containers: - image: traefik name: traefik-ingress-lb volumeMounts: - mountPath: "/ssl" name: "ssl" - mountPath: "/config" name: "config" ports: - name: http containerPort: 80 hostPort: 80 - name: https containerPort: 443 hostPort: 443 - name: admin containerPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --configfile=/config/traefik.toml - --api - --kubernetes - --logLevel=INFO nodeSelector: edgenode: "true" 与之前相比，增加了 443 端口，启动参数中指定了 traefik 的配置文件，并通过 volume 把相关的 secret 和 configmap 挂载进来。更新完 Traefik Pod 后，再次访问 traefik 的 dashboard 会跳转到 https 的地址，并会提示相关证书相关的报警信息，这是因为证书是自签发的，并不受浏览器新人，如果从正规机构购买的证书则不会出现报警信息，而是小绿盾。点击下面的高级，就可以强制跳转，这样就能正常访问 traefik 的 dashboard 了。 配置 ingress创建三个 websites 示例 (cheese-deployments.yaml)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: stilton labels: app: cheese cheese: stiltonspec: replicas: 2 selector: matchLabels: app: cheese task: stilton template: metadata: labels: app: cheese task: stilton version: v0.0.1 spec: containers: - name: cheese image: errm/cheese:stilton ports: - containerPort: 80---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: cheddar labels: app: cheese cheese: cheddarspec: replicas: 2 selector: matchLabels: app: cheese task: cheddar template: metadata: labels: app: cheese task: cheddar version: v0.0.1 spec: containers: - name: cheese image: errm/cheese:cheddar ports: - containerPort: 80---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: wensleydale labels: app: cheese cheese: wensleydalespec: replicas: 2 selector: matchLabels: app: cheese task: wensleydale template: metadata: labels: app: cheese task: wensleydale version: v0.0.1 spec: containers: - name: cheese image: errm/cheese:wensleydale ports: - containerPort: 80 为每个 cheese pods 创建相关的 Service (cheese-services.yaml)1234567891011121314151617181920212223242526272829303132333435363738394041---apiVersion: v1kind: Servicemetadata: name: stiltonspec: ports: - name: http targetPort: 80 port: 80 selector: app: cheese task: stilton---apiVersion: v1kind: Servicemetadata: name: cheddarspec: ports: - name: http targetPort: 80 port: 80 selector: app: cheese task: cheddar---apiVersion: v1kind: Servicemetadata: name: wensleydale annotations: traefik.backend.circuitbreaker: "NetworkErrorRatio() &gt; 0.5"spec: ports: - name: http targetPort: 80 port: 80 selector: app: cheese task: wensleydale]]></content>
  </entry>
  <entry>
    <title><![CDATA[Nginx编译安装]]></title>
    <url>%2F2019%2F06%2F11%2FNginx%2F</url>
    <content type="text"><![CDATA[安装编译环境这里以Centos为例 1yum -y install gcc gcc-c++ automake autoconf libtool make 安装相关依赖库pcre库为了rewrite，zlib库为了gzip压缩，将下面包全部解压缩 12345cd /optwget http://nginx.org/download/nginx-1.10.2.tar.gzwget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.39.tar.gz wget http://zlib.net/zlib-1.2.11.tar.gzwget https://www.openssl.org/source/openssl-1.0.2l.tar.gz 编译安装1234./configure --prefix=/opt/nginx --user=www --group=www --with-openssl=../openssl-1.0.2l \--with-pcre=../pcre-8.39 --with-zlib=../zlib-1.2.11 --with-http_realip_module \--with-http_ssl_module --with-http_stub_status_modulemake &amp;&amp; make install 此处编译将openssl、pcre、zlib编译至nginx启动内核，从而不依赖系统openssl、pcre、zlib，加快nginx启动速度和方便移植到其他系统使用。 查看安装版本12345678# /opt/nginx/sbin/nginx -Vnginx version: nginx/1.10.2built by gcc 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC)built with OpenSSL 1.0.2l 25 May 2017TLS SNI support enabledconfigure arguments: --prefix=/opt/nginx --user=www --group=www --with-openssl=../openssl-1.0.2l --with-pcre=../pcre-8.39 --with-zlib=../zlib-1.2.11 --with-http_realip_module --with-http_ssl_module --with-http_stub_status_module NGINX systemed service file路径：/lib/systemd/system/nginx.service 123456789101112131415[Unit]Description=The NGINX HTTP and reverse proxy serverAfter=syslog.target network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/opt/nginx/logs/nginx.pidExecStartPre=/opt/nginx/sbin/nginx -tExecStart=/opt/nginx/sbin/nginxExecReload=/opt/nginx/sbin/nginx -s reloadExecStop=/bin/kill -s QUIT $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target 启动NGINX1systemctl daemon-reload &amp;&amp; systemctl enable --now nginx.service nginx.conf配置内的pid指定位置必须和nginx.service文件内的路径保持一致。]]></content>
  </entry>
  <entry>
    <title><![CDATA[二进制安装Kubernetes]]></title>
    <url>%2F2019%2F06%2F10%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85Kubernetes%2F</url>
    <content type="text"><![CDATA[系统初始化设置永久主机名1hostnamectl set-hostname k8s-m1 设置主机解析如果不存在DNS解析，则每台主机需设置/etc/hosts文件，添加主机与IP的对应关系。123456cat &gt;&gt; /etc/hosts &lt;&lt;EOF10.105.26.201 k8s-m110.105.26.202 k8s-m210.105.26.203 k8s-m310.105.26.210 k8s-n1EOF 免密码登陆其他节点12345ssh-keygen -t rsassh-copy-id root@k8s-m1ssh-copy-id root@k8s-m2ssh-copy-id root@k8s-m3ssh-copy-id root@k8s-n1 关闭防火墙与selinux123systemctl disable --now firewalld NetworkManagersetenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config 关闭swap分区如果开启了swap分区，会导致kubelet启动失败（可通过–fail-swap-on参数忽略swap开启）。12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab 关闭dnsmasqlinux系统开启了dnsmasq（GUI环境），将DNS server设置为127.0.0.1，会导致docker容器无法解析域名。12systemctl stop dnsmasqsystemctl disable dnsmasq 优化内核参数123456789101112131415161718192021222324252627282930313233343536cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf# https://github.com/moby/moby/issues/31208 # ipvsadm -l --timout# 修复ipvs模式下长连接timeout问题 小于900即可net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1net.ipv4.neigh.default.gc_stale_time = 120net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.default.arp_announce = 2net.ipv4.conf.lo.arp_announce = 2net.ipv4.conf.all.arp_announce = 2net.ipv4.ip_forward = 1net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 1024net.ipv4.tcp_synack_retries = 2# 要求iptables不对bridge的数据进行处理net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-arptables = 1net.netfilter.nf_conntrack_max = 2310720fs.inotify.max_user_watches=89100fs.may_detach_mounts = 1fs.file-max = 52706963fs.nr_open = 52706963vm.swappiness = 0vm.overcommit_memory=1vm.panic_on_oom=0EOFsysctl --system 设置系统时区调整系统TimeZone，将当前的UTC时间写入硬件时钟1234timedatectl set-timezone Asia/Shanghaitimedatectl set-local-rtc 0systemctl restart rsyslogsystemctl restart crond 关闭无关的服务1systemctl stop postfix &amp;&amp; systemctl disable postfix 设置rsyslogd和systemd journaldsystemd的journald是Centos 7缺省的日志记录工具，它记录了所有系统、内核、Service Unit的日志。相比systemd，journald记录的日志有如下优势：1. 可以记录到内存或文件系统；(默认记录到内存，对应的位置为 /run/log/jounal)；2. 可以限制占用的磁盘空间、保证磁盘剩余空间；3. 可以限制日志文件大小、保存的时间；journald默认将日志转发给rsyslog，这会导致日志写了多份，/var/log/messages中包含了太多无关日志，不方便后续查看，同时也影响系统性能。123456789101112131415161718192021222324252627mkdir /var/log/journal # 持久化保存日志的目录mkdir /etc/systemd/journald.conf.dcat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF[Journal]# 持久化保存到磁盘Storage=persistent# 压缩历史日志Compress=yesSyncIntervalSec=5mRateLimitInterval=30sRateLimitBurst=1000# 最大占用空间 10GSystemMaxUse=10G# 单日志文件最大 200MSystemMaxFileSize=200M# 日志保存时间 2 周MaxRetentionSec=2week# 不将日志转发到 syslogForwardToSyslog=noEOFsystemctl restart systemd-journald 升级内核Centos 7.x系统自带的3.10.x内核存在一些Bugs，导致Docker和Kubernetes不稳定，例如：1. 高版本的 docker(1.13 以后) 启用了3.10版本kernel实验支持的kernel memory account功能，当节点压力大如频繁启动和停止容器时会导致cgroup memory leak；2. 网络设备引用计数泄漏，会导致类似于报错：”kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1”; 自选内核安装123export Kernel_Version=4.18.9-1wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml&#123;,-devel&#125;-$&#123;Kernel_Version&#125;.el7.elrepo.x86_64.rpmyum localinstall -y kernel-ml* 最新内核安装1234rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpmyum --disablerepo="*" --enablerepo="elrepo-kernel" list available --showduplicates | grep -Po '^kernel-ml.x86_64\s+\K\S+(?=.el7)'yum --disablerepo="*" --enablerepo=elrepo-kernel install -y kernel-ml&#123;,-devel&#125; 修改内核启动顺序12grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfggrubby --default-kernel 开启user_namespace.enable=11grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)" 重新加载内核1reboot 安装ipvs1yum install ipvsadm ipset sysstat conntrack libseccomp vim wget curl jq -y 设置开机自动加载ipvs内核模块1234567891011121314:&gt; /etc/modules-load.d/ipvs.confmodule=(ip_vsip_vs_rrip_vs_wrrip_vs_shnf_conntrackbr_netfilter )for kernel_module in $&#123;module[@]&#125;;do /sbin/modinfo -F filename $kernel_module |&amp; grep -qv ERROR &amp;&amp; echo $kernel_module &gt;&gt; /etc/modules-load.d/ipvs.conf || :donesystemctl enable --now systemd-modules-load.service docker条件检查12curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh &gt; check-config.shbash check-config.sh 利用官方脚本安装docker12export VERSION=18.06curl -fsSL "https://get.docker.com/" | bash -s -- --mirror Aliyun 配置加速源和docker启动参数使用systemd12345678910111213141516mkdir -p /etc/docker/cat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; "exec-opts": ["native.cgroupdriver=systemd"], "registry-mirrors": ["https://ib9xyhrv.mirror.aliyuncs.com"], "storage-driver": "overlay2", "storage-opts": [ "overlay2.override_kernel_check=true" ], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m", "max-file": "3" &#125;&#125;EOF 设置docker开机启动，并设置docker命令补全12yum install -y epel-release bash-completion &amp;&amp; cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/systemctl enable --now docker 创建相关目录并分发集群配置参数脚本1234567source environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp environment.sh root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; "chmod +x /opt/k8s/bin/*" done 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/bash# 生成 EncryptionConfig 所需的加密 keyexport ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)# 集群各机器 IP 数组export NODE_IPS=(10.105.26.201 10.105.26.202 10.105.26.203)export WORKER_IPS=(10.105.26.210)# 集群各 IP 对应的主机名数组export NODE_NAMES=(k8s-m1 k8s-m2 k8s-m3)export WORKER_NAMES=(k8s-n1)# etcd 集群服务地址列表export ETCD_ENDPOINTS="https://10.105.26.201:2379,https://10.105.26.202:2379,https://10.105.26.203:2379"# etcd 集群间通信的 IP 和端口export ETCD_NODES="k8s-m1=https://10.105.26.201:2380,k8s-m2=https://10.105.26.202:2380,k8s-m3=https://10.105.26.203:2380"# kube-apiserver 的反向代理(kube-nginx)地址端口export KUBE_APISERVER="https://127.0.0.1:8443"# 节点间互联网络接口名称export IFACE="eth0"# etcd 数据目录export ETCD_DATA_DIR="/data/k8s/etcd/data"# etcd WAL 目录，建议是 SSD 磁盘分区，或者和 ETCD_DATA_DIR 不同的磁盘分区export ETCD_WAL_DIR="/data/k8s/etcd/wal"# k8s 各组件数据目录export K8S_DIR="/data/k8s/k8s"# docker 数据目录# export DOCKER_DIR="/data/k8s/docker"## 以下参数一般不需要修改# TLS Bootstrapping 使用的 Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 生成BOOTSTRAP_TOKEN="41f7e4ba8b7be874fcff18bf5cf41a7c"# 最好使用 当前未用的网段 来定义服务网段和 Pod 网段# 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 保证)SERVICE_CIDR="10.254.0.0/16"# Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证)CLUSTER_CIDR="172.30.0.0/16"# 服务端口范围 (NodePort Range)export NODE_PORT_RANGE="30000-32767"# flanneld 网络配置前缀export FLANNEL_ETCD_PREFIX="/kubernetes/network"# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)export CLUSTER_KUBERNETES_SVC_IP="10.254.0.1"# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)export CLUSTER_DNS_SVC_IP="10.254.0.2"# 集群 DNS 域名（末尾不带点号）export CLUSTER_DNS_DOMAIN="cluster.local"# 将二进制目录 /opt/k8s/bin 加到 PATH 中export PATH=/opt/k8s/bin:$PATH 创建CA证书和秘钥kubernetes集群各组件需要使用x509证书对通信进行加密和认证。CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。 安装cfssl工具1234567wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfochmod +x /usr/local/bin/cfssl* 创建根证书CA证书是集群所有节点共享的，只需要创建一个CA证书，后续创建的所有证书都由它签名。 创建配置文件1234567891011121314151617181920cat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOF signing：表示该证书可用于签名其它证书，生成的ca.pem证书中CA=TRUE； server auth：表示client可以用该该证书对server提供的证书进行验证； client auth：表示server可以用该该证书对client提供的证书进行验证； 创建证书签名请求文件123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "k8s", "OU": "IT" &#125; ]&#125;EOF CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法； O：Organization，kube-apiserver从证书中提取该字段作为请求用户所属的组 (Group)； kube-apiserver将提取的User、Group作为RBAC授权的用户标识； 生成CA证书和私钥1cfssl gencert -initca ca-csr.json | cfssljson -bare ca 分发证书到其他节点123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p /etc/kubernetes/pki" scp ca*.pem ca-config.json root@$&#123;node_ip&#125;:/etc/kubernetes/pki done 部署kubectl工具下载kubectl二进制文件并分发到其他节点12345678wget https://dl.k8s.io/v1.14.2/kubernetes-client-linux-amd64.tar.gztar -xzvf kubernetes-client-linux-amd64.tar.gzfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kubernetes/client/bin/kubectl root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; "chmod +x /opt/k8s/bin/*" done 创建admin证书和私钥kubectl与apiserver https安全端口通信，apiserver对提供的证书进行认证和授权。kubectl作为集群的管理工具，需要被授予最高权限，这里创建具有最高权限的admin证书。12345678910111213141516171819cat &gt; admin-csr.json &lt;&lt;EOF&#123; "CN": "admin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "system:masters", "OU": "IT" &#125; ]&#125;EOF O为system:masters，kube-apiserver收到该证书后将请求的Group设置为 system:masters； 预定义的ClusterRoleBinding cluster-admin将Group system:masters与Role cluster-admin绑定，该Role授予所有 PI的权限； 该证书只会被kubectl当做client证书使用，所以hosts字段为空； 生成证书和私钥1234cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kubeconfig文件12345678910111213141516171819202122# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kubectl.kubeconfig # 设置客户端认证参数kubectl config set-credentials admin \ --client-certificate=/opt/k8s/work/admin.pem \ --client-key=/opt/k8s/work/admin-key.pem \ --embed-certs=true \ --kubeconfig=kubectl.kubeconfig # 设置上下文参数kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=admin \ --kubeconfig=kubectl.kubeconfig # 设置默认上下文kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig –certificate-authority：验证kube-apiserver证书的根证书； –client-certificate、–client-key：刚生成的admin证书和私钥，连接 kube-apiserver时使用； –embed-certs=true：将ca.pem和admin.pem证书内容嵌入到生成的kubectl.kubeconfig文件中(不加时，写入的是证书文件路径，后续拷贝 kubeconfig到其它机器时，还需要单独拷贝证书文件，不方便。)； 分发kubeconfig文件到其他节点123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p ~/.kube" scp kubectl.kubeconfig root@$&#123;node_ip&#125;:~/.kube/config done 部署etcd集群etcd是基于Raft的分布式key-value存储系统，由CoreOS开发，常用于服务发现、共享配置以及并发控制（如leader选举、分布式锁等）。kubernetes使用 etcd存储所有运行数据。 下载etcd二进制文件并分发12345678wget https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gztar -xvf etcd-v3.3.13-linux-amd64.tar.gzfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp etcd-v3.3.13-linux-amd64/etcd* root@$&#123;node_ip&#125;:/opt/k8s/bin ssh root@$&#123;node_ip&#125; "chmod +x /opt/k8s/bin/*" done 创建etcd证书和私钥123456789101112131415161718192021222324cat &gt; etcd-csr.json &lt;&lt;EOF&#123; "CN": "etcd", "hosts": [ "127.0.0.1", "10.105.26.201", "10.105.26.202", "10.105.26.203" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "k8s", "OU": "IT" &#125; ]&#125;EOF 在生产环境在证书内预留几个IP，已防止意外故障迁移时还需要重新生成证书 生成证书和私钥1234cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd 分发证书和私钥到其他节点123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p /etc/kubernetes/pki/etcd" scp etcd*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/pki/etcd/ done 创建etcd的systemd unit模版文件12345678910111213141516171819202122232425262728293031323334353637383940414243cat &gt; etcd.service.template &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=$&#123;ETCD_DATA_DIR&#125;ExecStart=/opt/k8s/bin/etcd \\ --data-dir=$&#123;ETCD_DATA_DIR&#125; \\ --wal-dir=$&#123;ETCD_WAL_DIR&#125; \\ --name=##NODE_NAME## \\ --cert-file=/etc/kubernetes/pki/etcd/etcd.pem \\ --key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/pki/ca.pem \\ --peer-cert-file=/etc/kubernetes/pki/etcd/etcd.pem \\ --peer-key-file=/etc/kubernetes/pki/etcd/etcd-key.pem \\ --peer-trusted-ca-file=/etc/kubernetes/pki/ca.pem \\ --peer-client-cert-auth \\ --client-cert-auth \\ --listen-peer-urls=https://##NODE_IP##:2380 \\ --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\ --listen-client-urls=https://##NODE_IP##:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://##NODE_IP##:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster=$&#123;ETCD_NODES&#125; \\ --initial-cluster-state=new \\ --auto-compaction-mode=periodic \\ --auto-compaction-retention=1 \\ --max-request-bytes=33554432 \\ --quota-backend-bytes=6442450944 \\ --heartbeat-interval=250 \\ --election-timeout=2000Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF WorkingDirectory、–data-dir：指定工作目录和数据目录为 ${ETCD_DATA_DIR}，需在启动服务前创建这个目录； –wal-dir：指定wal目录，为了提高性能，一般使用SSD或者和–data-dir不同的磁盘； –name：指定节点名称，当–initial-cluster-state值为new时，–name的参数值必须位于–initial-cluster列表中； –cert-file、–key-file：etcd server与client通信时使用的证书和私钥； –trusted-ca-file：签名client证书的CA证书，用于验证client证书； –peer-cert-file、–peer-key-file：etcd与peer通信使用的证书和私钥； –peer-trusted-ca-file：签名peer证书的CA证书，用于验证peer证书； 分发etcd system unit文件到其他节点替换模板文件中的变量，为各节点创建systemd unit文件1234for (( i=0; i &lt; 3; i++ )) do sed -e "s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/" -e "s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/" etcd.service.template &gt; etcd-$&#123;NODE_IPS[i]&#125;.service done 分发生成的systemd unit文件12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp etcd-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/etcd.service done 启动etcd服务123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p $&#123;ETCD_DATA_DIR&#125; $&#123;ETCD_WAL_DIR&#125;" ssh root@$&#123;node_ip&#125; "systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd " &amp; done 必须先创建etcd数据目录和工作目录 etcd进程首次启动时会等待其它节点的etcd加入集群，命令systemctl start etcd会卡住一段时间，为正常现象 验证服务状态123456789for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ETCDCTL_API=3 /opt/k8s/bin/etcdctl \ --endpoints=https://$&#123;node_ip&#125;:2379 \ --cacert=/etc/kubernetes/pki/ca.pem \ --cert=/etc/kubernetes/pki/etcd/etcd.pem \ --key=/etc/kubernetes/pki/etcd/etcd-key.pem endpoint health done 预期输出 123456&gt;&gt;&gt; 10.105.26.201https://10.105.26.201:2379 is healthy: successfully committed proposal: took = 1.706544ms&gt;&gt;&gt; 10.105.26.202https://10.105.26.202:2379 is healthy: successfully committed proposal: took = 2.495669ms&gt;&gt;&gt; 10.105.26.203https://10.105.26.203:2379 is healthy: successfully committed proposal: took = 2.228788ms 查看当前的 leader12345ETCDCTL_API=3 /opt/k8s/bin/etcdctl \ -w table --cacert=/etc/kubernetes/pki/ca.pem \ --cert=/etc/kubernetes/pki/etcd/etcd.pem \ --key=/etc/kubernetes/pki/etcd/etcd-key.pem \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status 预期输出 1234567+----------------------------+------------------+---------+---------+-----------+-----------+------------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+----------------------------+------------------+---------+---------+-----------+-----------+------------+| https://10.105.26.201:2379 | 41d3e233e1ce5cff | 3.3.13 | 20 kB | true | 2 | 8 || https://10.105.26.202:2379 | 54033052a4cf5146 | 3.3.13 | 20 kB | false | 2 | 8 || https://10.105.26.203:2379 | 5b1caf2378628ff0 | 3.3.13 | 20 kB | false | 2 | 8 |+----------------------------+------------------+---------+---------+-----------+-----------+------------+ 部署flannel网络kubernetes要求集群内各节点(包括master节点)能通过Pod网段互联互通。flannel使用 vxlan技术为各节点创建一个可以互通的Pod网络，使用的端口为 UDP8472。flanneld第一次启动时，从etcd获取配置的Pod网段信息，为本节点分配一个未使用的地址段，然后创建flannedl.1网络接口。flannel将分配给自己的Pod网段信息写入/run/flannel/docker文件，docker后续使用这个文件中的环境变量设置docker0网桥，从而从这个地址段为本节点的所有Pod容器分配 IP。 下载flanneld二进制文件123456789mkdir /opt/k8s/work/flannelwget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gztar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannelfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp flannel/&#123;flanneld,mk-docker-opts.sh&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; "chmod +x /opt/k8s/bin/*" done 创建flannel证书和私钥12345678910111213141516171819cat &gt; flanneld-csr.json &lt;&lt;EOF&#123; "CN": "flanneld", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "k8s", "OU": "IT" &#125; ]&#125;EOF 该证书只会被kubectl当做client证书使用，所以hosts字段为空 生成证书和私钥1234cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld 将生成的证书和私钥分发到master和worker123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p /etc/flanneld/cert" scp flanneld*.pem root@$&#123;node_ip&#125;:/etc/flanneld/cert done 向etcd写入集群Pod网段信息123456etcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/opt/k8s/work/ca.pem \ --cert-file=/opt/k8s/work/flanneld.pem \ --key-file=/opt/k8s/work/flanneld-key.pem \ mk $&#123;FLANNEL_ETCD_PREFIX&#125;/config '&#123;"Network":"'$&#123;CLUSTER_CIDR&#125;'", "SubnetLen": 21, "Backend": &#123;"Type": "vxlan"&#125;&#125;' flanneld当前版本(v0.11.0)不支持etcd v3，故使用etcd v2 API写入配置key和网段数据 写入的Pod网段${CLUSTER_CIDR}地址段（如 /16）必须小于SubnetLen，必须与kube-controller-manager的–cluster-cidr参数值一致 创建flanneld的systemd unit文件12345678910111213141516171819202122232425262728cat &gt; flanneld.service &lt;&lt; EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyExecStart=/opt/k8s/bin/flanneld \\ -etcd-cafile=/etc/kubernetes/pki/ca.pem \\ -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\ -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\ -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; \\ -iface=$&#123;IFACE&#125; \\ -ip-masqExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=alwaysRestartSec=5StartLimitInterval=0[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOF mk-docker-opts.sh脚本将分配给flanneld的Pod子网段信息写入/run/flannel/docker文件，后续docker启动时使用这个文件中的环境变量配置docker0 网桥 flanneld使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用-iface参数指定通信接口 flanneld运行时需要root权限 -ip-masq: flanneld为访问Pod网络外的流量设置SNAT规则，同时将传递给 Docker的变量–ip-masq（/run/flannel/docker文件中）设置为false，这样 Docker将不再创建SNAT规则；Docker的–ip-masq为true时，创建的SNAT规则比较“暴力”：将所有本节点Pod发起的、访问非docker0接口的请求做SNAT，这样访问其他节点Pod的请求来源IP会被设置为flannel.1接口的IP，导致目的 Pod看不到真实的来源Pod IP。flanneld创建的SNAT规则比较温和，只对访问非 Pod网段的请求做SNAT 分发flanneld systemd unit文件到master和worker12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp flanneld.service root@$&#123;node_ip&#125;:/etc/systemd/system/ done 启动flanneld服务12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld" done 检查分配给各flanneld的Pod网段信息查看集群 Pod 网段(/16)123456etcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/etc/kubernetes/pki/ca.pem \ --cert-file=/etc/flanneld/cert/flanneld.pem \ --key-file=/etc/flanneld/cert/flanneld-key.pem \ get $&#123;FLANNEL_ETCD_PREFIX&#125;/config 预期输出1&#123;"Network":"172.30.0.0/16", "SubnetLen": 21, "Backend": &#123;"Type": "vxlan"&#125;&#125; 查看已分配的 Pod 子网段列表(/24)123456etcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/etc/kubernetes/pki/ca.pem \ --cert-file=/etc/flanneld/cert/flanneld.pem \ --key-file=/etc/flanneld/cert/flanneld-key.pem \ ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets 可能的输出123/kubernetes/network/subnets/172.30.224.0-21/kubernetes/network/subnets/172.30.128.0-21/kubernetes/network/subnets/172.30.232.0-21 查看某一Pod网段对应的节点IP和flannel接口地址123456etcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/etc/kubernetes/pki/ca.pem \ --cert-file=/etc/flanneld/cert/flanneld.pem \ --key-file=/etc/flanneld/cert/flanneld-key.pem \ get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.232.0-21 可能的输出1&#123;"PublicIP":"10.105.26.203","BackendType":"vxlan","BackendData":&#123;"VtepMAC":"26:b4:0b:f2:56:ce"&#125;&#125; 172.30.232.0/21被分配给节点k8s-m3（10.105.26.203） VtepMAC为k8s-m3节点的flannel.1网卡MAC 地址 验证各节点能通过Pod网段互通验证是否创建了flannel接口12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh $&#123;node_ip&#125; "/usr/sbin/ip addr show flannel.1|grep -w inet" done 预期输出123456&gt;&gt;&gt; 10.105.26.201 inet 172.30.224.0/32 scope global flannel.1&gt;&gt;&gt; 10.105.26.202 inet 172.30.128.0/32 scope global flannel.1&gt;&gt;&gt; 10.105.26.203 inet 172.30.232.0/32 scope global flannel.1 在各节点上ping所有flannel接口IP1234567for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh $&#123;node_ip&#125; "ping -c 1 172.30.128.0" ssh $&#123;node_ip&#125; "ping -c 1 172.30.224.0" ssh $&#123;node_ip&#125; "ping -c 1 172.30.232.0" done kube-apiserver高可用之nginx代理 控制节点的kube-controller-manager、kube-scheduler是多实例部署，所以只要有一个实例正常，就可以保证高可用 集群内的Pod使用K8S服务域名kubernetes访问kube-apiserver，kube-dns会自动解析出多个kube-apiserver节点的IP，所以也是高可用的 在每个节点起一个nginx进程，后端对接多个apiserver实例，nginx对它们做健康检查和负载均衡 kubelet、kube-proxy、controller-manager、scheduler通过本地的nginx（监听 127.0.0.1）访问kube-apiserver，从而实现kube-apiserver的高可用 下载和编译nginx12wget http://nginx.org/download/nginx-1.15.3.tar.gztar -xzvf nginx-1.15.3.tar.gz 编译参数 123cd nginx-1.15.3mkdir nginx-prefix./configure --with-stream --without-http --prefix=$(pwd)/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module –with-stream：开启4层透明转发(TCP Proxy)功能 –without-xxx：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小 输出 123456789101112131415Configuration summary + PCRE library is not used + OpenSSL library is not used + zlib library is not used nginx path prefix: "/opt/k8s/work/nginx-1.15.3/nginx-prefix" nginx binary file: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx" nginx modules path: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/modules" nginx configuration prefix: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/conf" nginx configuration file: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/conf/nginx.conf" nginx pid file: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/logs/nginx.pid" nginx error log file: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/logs/error.log" nginx http access log file: "/opt/k8s/work/nginx-1.15.3/nginx-prefix/logs/access.log" nginx http client request body temporary files: "client_body_temp" nginx http proxy temporary files: "proxy_temp" 编译和安装 12cd /opt/k8s/work/nginx-1.15.3make &amp;&amp; make install 验证编译 1./nginx-prefix/sbin/nginx -v 输出 1nginx version: nginx/1.15.3 查看 nginx 动态链接的库 1ldd ./nginx-prefix/sbin/nginx 输出 12345linux-vdso.so.1 =&gt; (0x00007ffd5bdd8000)libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fe523035000)libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007fe522e19000)libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fe522a4c000)/lib64/ld-linux-x86-64.so.2 (0x00007fe523239000) 安装和部署nginx创建目录结构 123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125; done 分发二进制程序 1234567for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp /opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/sbin/kube-nginx ssh root@$&#123;node_ip&#125; "chmod a+x /opt/k8s/kube-nginx/sbin/*" ssh root@$&#123;node_ip&#125; "mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125;" done 配置nginx，开启4层透明转发功能 12345678910111213141516171819202122cat &gt; kube-nginx.conf &lt;&lt;EOFworker_processes 1;events &#123; worker_connections 1024;&#125;stream &#123; upstream backend &#123; hash $remote_addr consistent; server 10.105.26.201:6443 max_fails=3 fail_timeout=30s; server 10.105.26.202:6443 max_fails=3 fail_timeout=30s; server 10.105.26.203:6443 max_fails=3 fail_timeout=30s; &#125; server &#123; listen 127.0.0.1:8443; proxy_connect_timeout 1s; proxy_pass backend; &#125;&#125;EOF 分发配置文件 123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-nginx.conf root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/conf/kube-nginx.conf done 配置systemd unit文件，启动服务123456789101112131415161718192021cat &gt; kube-nginx.service &lt;&lt;EOF[Unit]Description=kube-apiserver nginx proxyAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=forkingExecStartPre=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -tExecStart=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginxExecReload=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -s reloadPrivateTmp=trueRestart=alwaysRestartSec=5StartLimitInterval=0LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 分发systemd unit文件 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-nginx.service root@$&#123;node_ip&#125;:/etc/systemd/system/ done 启动kube-nginx服务 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "systemctl daemon-reload &amp;&amp; systemctl enable kube-nginx &amp;&amp; systemctl restart kube-nginx" done 检查kube-nginx服务运行状态 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "systemctl status kube-nginx |grep 'Active:'" done 部署master节点kubernetes master节点运行如下组件 kube-apiserver kube-scheduler kube-controller-manager kube-nginx kube-scheduler和kube-controller-manager会自动选举产生一个leader实例，其它实例处于阻塞模式，当leader挂了后，重新选举产生新的leader，从而保证服务可用性 kube-apiserver是无状态的，需要通过kube-nginx进行代理访问，从而保证服务可用性 下载nginx二进制文件并分发12345678910wget https://dl.k8s.io/v1.14.2/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetestar -xzvf kubernetes-src.tar.gzfor node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kubernetes/server/bin/&#123;apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; "chmod +x /opt/k8s/bin/*" done 部署高可用kube-apiserver集群123456789101112131415161718192021222324252627282930cat &gt; kubernetes-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "hosts": [ "127.0.0.1", "10.105.26.201", "10.105.26.202", "10.105.26.203", "10.254.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local." ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "k8s", "OU": "System" &#125; ]&#125;EOF hosts字段指定授权使用该证书的IP和域名列表，这里列出了master节点IP、kubernetes服务的IP和域名 kubernetes服务IP是apiserver自动创建的，一般是–service-cluster-ip-range参数指定的网段的第一个IP 123kubectl get svc kubernetesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 15m 生成证书和私钥1234cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 将生成的证书和私钥文件拷贝到所有master节点 123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p /etc/kubernetes/pki" scp kubernetes*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/pki/ done 创建加密配置文件12345678910111213cat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: $&#123;ENCRYPTION_KEY&#125; - identity: &#123;&#125;EOF 将加密配置文件拷贝到master节点的/etc/kubernetes目录下 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp encryption-config.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/ done 创建审计策略文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189cat &gt; audit-policy.yaml &lt;&lt;EOFapiVersion: audit.k8s.io/v1beta1kind: Policyrules: # The following requests were manually identified as high-volume and low-risk, so drop them. - level: None resources: - group: "" resources: - endpoints - services - services/status users: - 'system:kube-proxy' verbs: - watch - level: None resources: - group: "" resources: - nodes - nodes/status userGroups: - 'system:nodes' verbs: - get - level: None namespaces: - kube-system resources: - group: "" resources: - endpoints users: - 'system:kube-controller-manager' - 'system:kube-scheduler' - 'system:serviceaccount:kube-system:endpoint-controller' verbs: - get - update - level: None resources: - group: "" resources: - namespaces - namespaces/status - namespaces/finalize users: - 'system:apiserver' verbs: - get # Don't log HPA fetching metrics. - level: None resources: - group: metrics.k8s.io users: - 'system:kube-controller-manager' verbs: - get - list # Don't log these read-only URLs. - level: None nonResourceURLs: - '/healthz*' - /version - '/swagger*' # Don't log events requests. - level: None resources: - group: "" resources: - events # node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes - level: Request omitStages: - RequestReceived resources: - group: "" resources: - nodes/status - pods/status users: - kubelet - 'system:node-problem-detector' - 'system:serviceaccount:kube-system:node-problem-detector' verbs: - update - patch - level: Request omitStages: - RequestReceived resources: - group: "" resources: - nodes/status - pods/status userGroups: - 'system:nodes' verbs: - update - patch # deletecollection calls can be large, don't log responses for expected namespace deletions - level: Request omitStages: - RequestReceived users: - 'system:serviceaccount:kube-system:namespace-controller' verbs: - deletecollection # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data, # so only log at the Metadata level. - level: Metadata omitStages: - RequestReceived resources: - group: "" resources: - secrets - configmaps - group: authentication.k8s.io resources: - tokenreviews # Get repsonses can be large; skip them. - level: Request omitStages: - RequestReceived resources: - group: "" - group: admissionregistration.k8s.io - group: apiextensions.k8s.io - group: apiregistration.k8s.io - group: apps - group: authentication.k8s.io - group: authorization.k8s.io - group: autoscaling - group: batch - group: certificates.k8s.io - group: extensions - group: metrics.k8s.io - group: networking.k8s.io - group: policy - group: rbac.authorization.k8s.io - group: scheduling.k8s.io - group: settings.k8s.io - group: storage.k8s.io verbs: - get - list - watch # Default level for known APIs - level: RequestResponse omitStages: - RequestReceived resources: - group: "" - group: admissionregistration.k8s.io - group: apiextensions.k8s.io - group: apiregistration.k8s.io - group: apps - group: authentication.k8s.io - group: authorization.k8s.io - group: autoscaling - group: batch - group: certificates.k8s.io - group: extensions - group: metrics.k8s.io - group: networking.k8s.io - group: policy - group: rbac.authorization.k8s.io - group: scheduling.k8s.io - group: settings.k8s.io - group: storage.k8s.io # Default level for all other requests. - level: Metadata omitStages: - RequestReceivedEOF 分发审计策略文件 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp audit-policy.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/audit-policy.yaml done 创建metrics-server使用的证书12345678910111213141516171819cat &gt; proxy-client-csr.json &lt;&lt;EOF&#123; "CN": "aggregator", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "k8s", "OU": "System" &#125; ]&#125;EOF CN名称为aggregator，需要与metrics-server的–requestheader-allowed-names参数配置一致，否则访问会被metrics-server拒绝 生成证书和私钥1234cfssl gencert -ca=/etc/kubernetes/pki/ca.pem \ -ca-key=/etc/kubernetes/pki/ca-key.pem \ -config=/etc/kubernetes/pki/ca-config.json \ -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client 将生成的证书和私钥文件拷贝到所有master节点 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp proxy-client*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/pki/ done 创建kube-apiserver systemd unit模板文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172cat &gt; kube-apiserver.service.template &lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-apiserverExecStart=/opt/k8s/bin/kube-apiserver \\ --advertise-address=##NODE_IP## \\ --default-not-ready-toleration-seconds=360 \\ --default-unreachable-toleration-seconds=360 \\ --feature-gates=DynamicAuditing=true \\ --max-mutating-requests-inflight=2000 \\ --max-requests-inflight=4000 \\ --default-watch-cache-size=200 \\ --delete-collection-workers=2 \\ --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --etcd-cafile=/etc/kubernetes/pki/ca.pem \\ --etcd-certfile=/etc/kubernetes/pki/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/pki/kubernetes-key.pem \\ --etcd-servers=$&#123;ETCD_ENDPOINTS&#125; \\ --bind-address=##NODE_IP## \\ --secure-port=6443 \\ --tls-cert-file=/etc/kubernetes/pki/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/pki/kubernetes-key.pem \\ --insecure-port=0 \\ --audit-dynamic-configuration \\ --audit-log-maxage=15 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-mode=batch \\ --audit-log-truncate-enabled \\ --audit-log-batch-buffer-size=20000 \\ --audit-log-batch-max-size=2 \\ --audit-log-path=$&#123;K8S_DIR&#125;/kube-apiserver/audit.log \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --profiling \\ --anonymous-auth=false \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --enable-bootstrap-token-auth \\ --requestheader-allowed-names="" \\ --requestheader-client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-extra-headers-prefix="X-Remote-Extra-" \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --service-account-key-file=/etc/kubernetes/pki/ca.pem \\ --authorization-mode=Node,RBAC \\ --runtime-config=api/all=true \\ --enable-admission-plugins=NodeRestriction \\ --allow-privileged=true \\ --apiserver-count=3 \\ --event-ttl=168h \\ --kubelet-certificate-authority=/etc/kubernetes/pki/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/pki/kubernetes-key.pem \\ --kubelet-https=true \\ --kubelet-timeout=10s \\ --proxy-client-cert-file=/etc/kubernetes/pki/proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/proxy-client-key.pem \\ --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\ --service-node-port-range=$&#123;NODE_PORT_RANGE&#125; \\ --logtostderr=true \\ --v=2Restart=on-failureRestartSec=10Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF –advertise-address：apiserver对外通告的IP（kubernetes服务后端节点 IP）； –default-*-toleration-seconds：设置节点异常相关的阈值； –max-*-requests-inflight：请求相关的最大阈值； –etcd-*：访问etcd的证书和etcd服务器地址； –experimental-encryption-provider-config：指定用于加密etcd中secret的配置； –bind-address：https监听的IP，不能为127.0.0.1，否则外界不能访问它的安全端口6443； –secret-port：https 监听端口； –insecure-port=0：关闭监听 http 非安全端口(8080)； –tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件； –audit-*：配置审计策略和审计日志文件相关的参数； –client-ca-file：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书； –enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证； –requestheader-*：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用； –requestheader-client-ca-file：用于签名 –proxy-client-cert-file 和 –proxy-client-key-file 指定的证书；在启用了 metric aggregator 时使用； 如果 –requestheader-allowed-names 不为空，则–proxy-client-cert-file 证书的 CN 必须位于 allowed-names 中，默认为 aggregator; –service-account-key-file：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 –service-account-private-key-file 指定私钥文件，两者配对使用； –runtime-config=api/all=true： 启用所有版本的 APIs，如 autoscaling/v2alpha1； –authorization-mode=Node,RBAC、–anonymous-auth=false： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求； –enable-admission-plugins：启用一些默认关闭的 plugins； –allow-privileged：运行执行 privileged 权限的容器； –apiserver-count=3：指定 apiserver 实例的数量； –event-ttl：指定 events 的保存时间； –kubelet-：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权； –proxy-client-*：apiserver 访问 metrics-server 使用的证书； –service-cluster-ip-range： 指定 Service Cluster IP 地址段； –service-node-port-range： 指定 NodePort 的端口范围； 如果kube-apiserver机器没有运行kube-proxy，则还需要添加–enable-aggregator-routing=true参数；requestheader-client-ca-file指定的CA证书，必须具有client auth and server auth 为各节点创建和分发kube-apiserver systemd unit文件1234for (( i=0; i &lt; 3; i++ )) do sed -e "s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/" -e "s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/" kube-apiserver.service.template &gt; kube-apiserver-$&#123;NODE_IPS[i]&#125;.service done 分发生成的systemd unit文件 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-apiserver-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-apiserver.service done 启动kube-apiserver服务123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p $&#123;K8S_DIR&#125;/kube-apiserver" ssh root@$&#123;node_ip&#125; "systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver" done 检查kube-apiserver运行状态12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "systemctl status kube-apiserver |grep 'Active:'" done 打印kube-apiserver写入etcd的数据123456ETCDCTL_API=3 etcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --cacert=/opt/k8s/work/ca.pem \ --cert=/opt/k8s/work/etcd.pem \ --key=/opt/k8s/work/etcd-key.pem \ get /registry/ --prefix --keys-only 检查集群信息12345678910111213141516$ kubectl cluster-infoKubernetes master is running at https://127.0.0.1:8443To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.$ kubectl get all --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault service/kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 12m$ kubectl get componentstatusesNAME STATUS MESSAGE ERRORcontroller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refusedscheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refusedetcd-0 Healthy &#123;"health":"true"&#125;etcd-2 Healthy &#123;"health":"true"&#125;etcd-1 Healthy &#123;"health":"true"&#125; 检查kube-apiserver监听的端口12netstat -lnpt|grep kubetcp 0 0 10.105.26.201:6443 0.0.0.0:* LISTEN 26178/kube-apiserve 6443: 接收https请求的安全端口，对所有请求做认证和授权 由于关闭了非安全端口，故没有监听 8080 授予kube-apiserver访问kubelet API的权限在执行kubectl exec、run、logs等命令时，apiserver会将请求转发到 kubelet的https端口。这里定义RBAC规则，授权apiserver使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问kubelet API的权限12kubectl create clusterrolebinding kube-apiserver:kubelet-apis \--clusterrole=system:kubelet-api-admin --user kubernetes 部署高可用kube-controller-manager集群该集群包含3个节点，启动后将通过竞争选举机制产生一个leader节点，其它节点为阻塞状态。当leader节点不可用时，阻塞的节点将再次进行选举产生新的 leader节点，从而保证服务的可用性。 为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager在如下两种情况下使用该证书： 与kube-apiserver的安全端口通信; 在安全端口(https，10252) 输出prometheus格式的metrics； 创建kube-controller-manager证书和私钥123456789101112131415161718192021222324cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF&#123; "CN": "system:kube-controller-manager", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "hosts": [ "127.0.0.1", "10.105.26.201", "10.105.26.202", "10.105.26.203" ], "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "system:kube-controller-manager", "OU": "System" &#125; ]&#125;EOF hosts列表包含所有kube-controller-manager节点IP； CN和O均为system:kube-controller-manager，kubernetes内置的 ClusterRoleBindings system:kube-controller-manager赋予kube-controller-manager工作所需的权限。 生成证书和私钥1234cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager 将生成的证书和私钥分发到所有master节点12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-controller-manager*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/pki/ done 创建和分发kubeconfig文件kube-controller-manager使用kubeconfig文件访问apiserver，该文件提供了 apiserver地址、嵌入的CA证书和kube-controller-manager证书123456789101112131415161718kubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-credentials system:kube-controller-manager \ --client-certificate=kube-controller-manager.pem \ --client-key=kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-context system:kube-controller-manager \ --cluster=kubernetes \ --user=system:kube-controller-manager \ --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig 分发kubeconfig到所有master节点12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-controller-manager.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/ done 创建kube-controller-manager systemd unit模版文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950cat &gt; kube-controller-manager.service.template &lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-controller-managerExecStart=/opt/k8s/bin/kube-controller-manager \\ --profiling \\ --cluster-name=kubernetes \\ --controllers=*,bootstrapsigner,tokencleaner \\ --kube-api-qps=1000 \\ --kube-api-burst=2000 \\ --leader-elect \\ --use-service-account-credentials\\ --concurrent-service-syncs=2 \\ --bind-address=##NODE_IP## \\ --secure-port=10252 \\ --tls-cert-file=/etc/kubernetes/pki/kube-controller-manager.pem \\ --tls-private-key-file=/etc/kubernetes/pki/kube-controller-manager-key.pem \\ --port=0 \\ --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-allowed-names=&quot;&quot; \\ --requestheader-client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \\ --experimental-cluster-signing-duration=8760h \\ --horizontal-pod-autoscaler-sync-period=10s \\ --concurrent-deployment-syncs=10 \\ --concurrent-gc-syncs=30 \\ --node-cidr-mask-size=24 \\ --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\ --pod-eviction-timeout=6m \\ --terminated-pod-gc-threshold=10000 \\ --root-ca-file=/etc/kubernetes/pki/ca.pem \\ --service-account-private-key-file=/etc/kubernetes/pki/ca-key.pem \\ --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --logtostderr=true \\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF –port=0：关闭监听非安全端口（http），同时 –address 参数无效，–bind-address 参数有效； –secure-port=10252、–bind-address=0.0.0.0: 在所有网络接口监听 10252 端口的 https /metrics 请求； –kubeconfig：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver； –authentication-kubeconfig 和 –authorization-kubeconfig：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。kube-controller-manager 不再使用 –tls-ca-file 对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。 –cluster-signing-*-file：签名 TLS Bootstrap 创建的证书； –experimental-cluster-signing-duration：指定 TLS Bootstrap 证书的有效期； –root-ca-file：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验； –service-account-private-key-file：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 –service-account-key-file 指定的公钥文件配对使用； –service-cluster-ip-range ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致； –leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态； –controllers=*,bootstrapsigner,tokencleaner：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token； –horizontal-pod-autoscaler-*：custom metrics 相关参数，支持 autoscaling/v2alpha1； –tls-cert-file、–tls-private-key-file：使用 https 输出 metrics 时使用的 Server 证书和秘钥； –use-service-account-credentials=true: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver； 为各master节点创建和分发kube-controller-mananger systemd unit文件替换模板文件中的变量，为各节点创建systemd unit文件1234for (( i=0; i &lt; 3; i++ )) do sed -e "s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/" -e "s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/" kube-controller-manager.service.template &gt; kube-controller-manager-$&#123;NODE_IPS[i]&#125;.service done 分发到所有 master 节点12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-controller-manager-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-controller-manager.service done 启动kube-controller-manager服务123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p $&#123;K8S_DIR&#125;/kube-controller-manager" ssh root@$&#123;node_ip&#125; "systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager" done 检查服务运行状态12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "systemctl status kube-controller-manager|grep Active" done kube-controller-manager的权限ClusteRole system:kube-controller-manager的权限很小，只能创建 secret、serviceaccount等资源对象，各controller的权限分散到 ClusterRole system:controller:XXX 中：12345678910111213141516# kubectl describe clusterrole system:kube-controller-managerName: system:kube-controller-managerLabels: kubernetes.io/bootstrapping=rbac-defaultsAnnotations: rbac.authorization.kubernetes.io/autoupdate: truePolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- secrets [] [] [create delete get update] endpoints [] [] [create get update] serviceaccounts [] [] [create get update] events [] [] [create patch update] tokenreviews.authentication.k8s.io [] [] [create] subjectaccessreviews.authorization.k8s.io [] [] [create] configmaps [] [] [get] namespaces [] [] [get] *.* [] [] [list watch] 需要在kube-controller-manager的启动参数中添加–use-service-account-credentials=true参数，这样main controller会为各controller创建对应的 ServiceAccount XXX-controller。内置的ClusterRoleBinding system:controller:XXX将赋予各XXX-controller ServiceAccount对应的 ClusterRole system:controller:XXX 权限。12345678910111213141516171819202122232425262728# kubectl get clusterrole|grep controllersystem:controller:attachdetach-controller 74msystem:controller:certificate-controller 74msystem:controller:clusterrole-aggregation-controller 74msystem:controller:cronjob-controller 74msystem:controller:daemon-set-controller 74msystem:controller:deployment-controller 74msystem:controller:disruption-controller 74msystem:controller:endpoint-controller 74msystem:controller:expand-controller 74msystem:controller:generic-garbage-collector 74msystem:controller:horizontal-pod-autoscaler 74msystem:controller:job-controller 74msystem:controller:namespace-controller 74msystem:controller:node-controller 74msystem:controller:persistent-volume-binder 74msystem:controller:pod-garbage-collector 74msystem:controller:pv-protection-controller 74msystem:controller:pvc-protection-controller 74msystem:controller:replicaset-controller 74msystem:controller:replication-controller 74msystem:controller:resourcequota-controller 74msystem:controller:route-controller 74msystem:controller:service-account-controller 74msystem:controller:service-controller 74msystem:controller:statefulset-controller 74msystem:controller:ttl-controller 74msystem:kube-controller-manager 74m 以deployment controller为例1234567891011121314151617# kubectl describe clusterrole system:controller:deployment-controllerName: system:controller:deployment-controllerLabels: kubernetes.io/bootstrapping=rbac-defaultsAnnotations: rbac.authorization.kubernetes.io/autoupdate: truePolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- replicasets.apps [] [] [create delete get list patch update watch] replicasets.extensions [] [] [create delete get list patch update watch] events [] [] [create patch update] pods [] [] [get list update watch] deployments.apps [] [] [get list update watch] deployments.extensions [] [] [get list update watch] deployments.apps/finalizers [] [] [update] deployments.apps/status [] [] [update] deployments.extensions/finalizers [] [] [update] deployments.extensions/status [] [] [update] 查看当前的 leader123456789101112# kubectl get endpoints kube-controller-manager --namespace=kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"kube-m1_b95d689d-8b43-11e9-8cf3-3e6b5fecb6ef","leaseDurationSeconds":15,"acquireTime":"2019-06-10T05:51:00Z","renewTime":"2019-06-10T06:12:28Z","leaderTransitions":0&#125;' creationTimestamp: "2019-06-10T05:51:00Z" name: kube-controller-manager namespace: kube-system resourceVersion: "2262" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: b95fd6a1-8b43-11e9-b331-3e6b5fecb6ef 部署高可用kube-scheduler集群该集群包含3个节点，启动后将通过竞争选举机制产生一个leader节点，其它节点为阻塞状态。当leader节点不可用后，剩余节点将再次进行选举产生新的 leader节点，从而保证服务的可用性。 为保证通信安全，本文档先生成x509证书和私钥，kube-scheduler在如下两种情况下使用该证书： 与kube-apiserver的安全端口通信; 在安全端口(https，10251) 输出prometheus格式的metrics； 创建kube-scheduler证书和私钥123456789101112131415161718192021222324cat &gt; kube-scheduler-csr.json &lt;&lt;EOF&#123; "CN": "system:kube-scheduler", "hosts": [ "127.0.0.1", "10.105.26.201", "10.105.26.202", "10.105.26.203" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "system:kube-scheduler", "OU": "System" &#125; ]&#125;EOF hosts列表包含所有kube-scheduler节点IP； CN和O均为system:kube-scheduler，kubernetes内置的ClusterRoleBindings system:kube-scheduler将赋予kube-scheduler工作所需的权限； 生成证书和私钥 1234cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler 将生成的证书和私钥分发到所有master节点 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-scheduler*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/pki/ done 创建和分发kubeconfig文件123456789101112131415161718kubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-credentials system:kube-scheduler \ --client-certificate=kube-scheduler.pem \ --client-key=kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-context system:kube-scheduler \ --cluster=kubernetes \ --user=system:kube-scheduler \ --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig 分发kubeconfig到所有master节点 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-scheduler.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/ done 创建kube-scheduler配置文件12345678910111213141516171819202122232425262728293031cat &gt; kube-scheduler.service.template &lt;&lt;EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-schedulerExecStart=/opt/k8s/bin/kube-scheduler \\ --config=/etc/kubernetes/kube-scheduler.yaml \\ --bind-address=##NODE_IP## \\ --secure-port=10259 \\ --port=0 \\ --tls-cert-file=/etc/kubernetes/pki/kube-scheduler.pem \\ --tls-private-key-file=/etc/kubernetes/pki/kube-scheduler-key.pem \\ --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-allowed-names="" \\ --requestheader-client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-extra-headers-prefix="X-Remote-Extra-" \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --logtostderr=true \\ --v=2Restart=alwaysRestartSec=5StartLimitInterval=0[Install]WantedBy=multi-user.targetEOF –kubeconfig：指定kubeconfig文件路径，kube-scheduler使用它连接和验证kube-apiserver； –leader-elect=true：集群运行模式，启用选举功能；被选为leader的节点负责处理工作，其它节点为阻塞状态； 替换模版文件中的变量 1234for (( i=0; i &lt; 3; i++ )) do sed -e "s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/" -e "s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/" kube-scheduler.yaml.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.yaml done 分发kube-scheduler配置文件到所有master节点 12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-scheduler-$&#123;node_ip&#125;.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/kube-scheduler.yaml done 创建kube-scheduler systemd unit模板文件12345678910111213141516171819202122232425262728293031cat &gt; kube-scheduler.service.template &lt;&lt;EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-schedulerExecStart=/opt/k8s/bin/kube-scheduler \\ --config=/etc/kubernetes/kube-scheduler.yaml \\ --bind-address=##NODE_IP## \\ --secure-port=10259 \\ --port=0 \\ --tls-cert-file=/etc/kubernetes/pki/kube-scheduler.pem \\ --tls-private-key-file=/etc/kubernetes/pki/kube-scheduler-key.pem \\ --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-allowed-names="" \\ --requestheader-client-ca-file=/etc/kubernetes/pki/ca.pem \\ --requestheader-extra-headers-prefix="X-Remote-Extra-" \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --logtostderr=true \\ --v=2Restart=alwaysRestartSec=5StartLimitInterval=0[Install]WantedBy=multi-user.targetEOF 为各节点创建和分发kube-scheduler systemd unit文件替换模板文件中的变量，为各节点创建 systemd unit 文件1234for (( i=0; i &lt; 3; i++ )) do sed -e "s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/" -e "s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/" kube-scheduler.service.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.service done 分发systemd unit文件到所有master节点12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" scp kube-scheduler-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-scheduler.service done 启动kube-scheduler服务123456for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "mkdir -p $&#123;K8S_DIR&#125;/kube-scheduler" ssh root@$&#123;node_ip&#125; "systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler" done 检查服务运行状态12345for node_ip in $&#123;NODE_IPS[@]&#125; do echo "&gt;&gt;&gt; $&#123;node_ip&#125;" ssh root@$&#123;node_ip&#125; "systemctl status kube-scheduler|grep Active" done 查看当前的 leader123456789101112# kubectl get endpoints kube-scheduler --namespace=kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"kube-m1_d5bb5bf8-8b4a-11e9-acfe-3e6b5fecb6ef","leaseDurationSeconds":15,"acquireTime":"2019-06-10T06:41:55Z","renewTime":"2019-06-10T07:41:36Z","leaderTransitions":0&#125;' creationTimestamp: "2019-06-10T06:41:55Z" name: kube-scheduler namespace: kube-system resourceVersion: "8290" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler uid: d6560c75-8b4a-11e9-b331-3e6b5fecb6ef]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes证书说明]]></title>
    <url>%2F2019%2F06%2F08%2FKubernetes%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[CFSSL是CloudFlare开源的一款PKI/TLS工具。CFSSL包含一个命令行工具和一个用于签名，验证并且捆绑TLS证书的HTTP API服务。使用Go语言编写。 安装CFSSL1234wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfochmod +x /usr/local/bin/cfssl* 集群相关证书类型client certificate：用于服务端认证客户端，例如etcdctl、etcd proxy、fleetctl、docker客户端。 server certificate：服务端使用，客户端以此验证服务端身份，例如docker服务端、kube-apiserver。 peer certificate：双向证书，用于etcd集群成员间通信。 根据认证对象可以将证书分成三类：服务器证书server cert，客户端证书client cert，对等证书peer cert（表示既是server cert又是client cert），在kubernetes集群中需要的证书种类如下： etcd节点需要标识自己服务的server cert，也需要client cert与etcd集群其他节点交互，当然可以分别指定2个证书，也可以使用一个对等证书。 master节点需要标识apiserver服务的server cert，也需要client cert连接etcd集群，这里也使用一个对等证书。 kubectl，calico，kube-proxy只需要client cert，因此证书请求中hosts字段可以为空。 kubelet证书比较特殊，不是手动生成，它由node节点TLS BootStrap向apiserver请求，由master节点的controller-manager 自动签发，包含一个client cert和一个server cert。 创建CA配置文件配置证书生成策略，规定CA可以颁发哪种类型的证书12345678910111213141516171819ca-config.json&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125; signing：表示该证书可用于签名其它证书，生成的ca.pem证书中CA=TRUE server auth：表示client可以用该该证书对server提供的证书进行验证 client auth：表示server可以用该该证书对client提供的证书进行验证 创建CA证书签名请求1234567891011121314151617181920ca-csr.json&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 4096 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "kubernetes", "OU": "System" &#125; ], "ca": &#123; "expiry": "87600h" &#125;&#125; CN：Common Name，kube-apiserver从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法 O：Organization，kube-apiserver从证书中提取该字段作为请求用户所属的组 (Group) kube-apiserver将提取的User、Group作为RBAC授权的用户标识 请不要修改证书配置的CN、O字段，这两个字段名称比较特殊，大多数为system:开头，实际上是为了匹配RBAC规则 生成CA证书和私钥生成CA所必需的文件ca-key.pem（私钥）和ca.pem（证书），还会生成ca.csr（证书签名请求），用于交叉签名或重新签名。123456789cfssl gencert -initca ca-csr.json | cfssljson -bare caCA├── ca-config.json├── ca.csr├── ca-csr.json├── ca-key.pem└── ca.pem0 directories, 5 files apiserver TLS 认证端口需要的证书123456789101112131415161718192021222324252627282930kube-apiserver-csr.json&#123; "CN": "kubernetes", "hosts": [ "127.0.0.1", "localhost", "10.105.26.201", "10.105.26.202", "10.105.26.203", "10.254.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "kubernetes", "OU": "System" &#125; ]&#125; hosts字段指定授权使用该证书的IP和域名列表，这里列出了master节点IP、kubernetes服务的IP和域名。 kubernetes服务IP是apiserver自动创建的，一般是–service-cluster-ip-range参数指定的网段的第一个IP。 controller manager 连接 apiserver 需要使用的证书123456789101112131415161718192021222324kube-controller-manager-csr.json&#123; "CN": "system:kube-controller-manager", "hosts": [ "127.0.0.1", "localhost", "10.105.26.201", "10.105.26.202", "10.105.26.203" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "system:kube-controller-manager", "OU": "System" &#125; ]&#125; hosts列表包含所有kube-controller-manager节点IP。 CN和O均为system:kube-controller-manager，kubernetes内置的ClusterRoleBindings system:kube-controller-manager赋予kube-controller-manager工作所需的权限。 scheduler连接apiserver需要使用的证书123456789101112131415161718192021222324kube-scheduler-csr.json&#123; "CN": "system:kube-scheduler", "hosts": [ "127.0.0.1", "localhost", "10.105.26.201", "10.105.26.202", "10.105.26.203" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "system:kube-scheduler", "OU": "System" &#125; ]&#125; hosts列表包含所有kube-scheduler节点IP。 CN和O均为system:kube-scheduler，kubernetes内置的ClusterRoleBindings system:kube-scheduler将赋予kube-scheduler工作所需的权限。 proxy组件连接apiserver需要使用的证书123456789101112131415161718kube-proxy-csr.json&#123; "CN": "system:kube-proxy", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "JiangSu", "L": "SuZhou", "O": "system:kube-proxy", "OU": "System" &#125; ]&#125; CN：指定该证书的User为system:kube-proxy。 预定义的RoleBinding system:node-proxier将User system:kube-proxy与Role system:node-proxier绑定，该Role授予了调用kube-apiserver Proxy相关API的权限。 该证书只会被kube-proxy当做client证书使用，所以hosts字段为空。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Harbor彻底删除镜像]]></title>
    <url>%2F2019%2F05%2F16%2FHarbor%E5%BD%BB%E5%BA%95%E5%88%A0%E9%99%A4%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[Harbor私有仓库运行一段时间后，仓库中存有大量镜像，会占用太多的存储空间。直接通过Harbor界面删除相关镜像，并不会自动删除存储中的文件和镜像。需要停止Harbor服务，执行垃圾回收命令，进行存储空间清理和回收。 先在Harbor UI中删除不需要的镜像 停止Harbor服务，执行垃圾回收命令1234docker-compose stopdocker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect --dry-run /etc/registry/config.yml \\使用--dry-run参数运行容器，预览运行效果，但不删除任何数据docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect /etc/registry/config.yml \\不使用--dry-run参数，将删除相关的文件和镜像docker-compose start 验证效果 统计目录/data/registry/docker/registry/v2/blobs&amp;repositories，对比之前大小。 1du -sh /data/registry/docker/registry/v2/blobs&amp;repositories 重新上传之前删除的镜像，如没成功删除会报镜像已存在。 后续搭建镜像仓库是容器云最核心的步骤之一。基本上最近几年大家都在自己的容器集群中搭建了镜像仓库。作为开源项目，Docker发布的Registry(V1版本)和最后发布了一份Distribution(V2版本)都在解决这个事情。但是不管是当初快速迭代的时候没有仔细考虑，还是迭代速度太快欠下来的技术债，反正是把我们这些实际应用者郁闷了良久。因为真正运营起一套容器云系统之后，会产生大量的镜像，这些镜像的管理就会摆上日程。一般用了没多久，就发现在单机上可以轻易删除镜像的操作在镜像仓库上尽然没有提供。当然在社区发现此问题后在第二个版本中，已经提供了相关的删除方案。当时因为其中细节有趣，我拿出来讲讲，让各位看官能顺着文字知道个前后因果。 在Distribution的代码库中查看ROADMAP.md中有明确的说明删除操作的利弊。首先，镜像内容是存储在一层虚拟文件系统(VFS)之上，由多个文件块(Blobs)、描述清单文件(Manifests)和 标签文件（tags）组成，因为镜像设计是多层的。所以这些文件会互相依赖，在没有确认文件块(Blob)是否被其他镜像使用的情况下，直接删除会让仓库不完整。所以一般的做法是只做删除标记，而不是真删除，通过垃圾回收机制来遍历当前仓库的有向关系图(DAG)，然后在删除没有被引用的文件块。 目前Docker Distribution属于维护阶段，已经再把实现标准往OCI社区迁移。所以未来大家应该以OCI镜像作为基准。通过代码补丁来实现支持OCI版本的镜像仓库。 如何有效的删除镜像的方法也是有的，大致分为4种方法，请参考： Reference Counting- 引用计数。 Lock the World GC- 全局垃圾回收。 Generational GC- 两代垃圾回收。 Centralized Oracle- 中央数据库。 每种方案都有利弊。所以在实现过程中一定要多考虑和实践。]]></content>
      <tags>
        <tag>Harbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群常用设置]]></title>
    <url>%2F2019%2F05%2F15%2F%E9%9B%86%E7%BE%A4%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[服务器CentOS系统安装完毕后的基本配置使用root用户在管理节点和计算节点上进行以下操作： 修改/etc/profile.d/perl-homedir.sh配置文件，以免每次登录用户，自动在家目录下生成perl5文件夹 12perl -p -i -e 's/PERL_HOMEDIR=1/PERL_HOMEDIR=0/' /etc/profile.d/perl-homedir.shecho 'eval "$(perl -Mlocal::lib=$HOME/.perl5)"' &gt;&gt; ~/.bashrc 修改/etc/sudoers配置文件，将自己的用户（例如 train）变成超级管理员用户 1perl -i.bak -e 'while (&lt;&gt;) &#123; if (/^root/) &#123; print; print "train ALL=(ALL) NOPASSWD:ALL\n"; last; &#125; else &#123; print &#125; &#125;' /etc/sudoers 修改/etc/selinux/config配置文件，永久关闭linux的一个安全机制，开启该安全机制会对很多操作造成阻碍。 12perl -p -i -e 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/configsetenforce 0 修改/etc/ssh/sshd_config配置文件，使openssh远程登录更安全，更快速 12345678910perl -p -i -e 's/#RSAAuthentication/RSAAuthentication/' /etc/ssh/sshd_configperl -p -i -e 's/#PubkeyAuthentication/PubkeyAuthentication/' /etc/ssh/sshd_configperl -p -i -e 's/#AuthorizedKeysFile/AuthorizedKeysFile/' /etc/ssh/sshd_configperl -p -i -e 's/.*PermitRootLogin.*/PermitRootLogin no/' /etc/ssh/sshd_configperl -p -i -e 's/.*Protocol\s+2.*/Protocol 2/' /etc/ssh/sshd_configperl -p -i -e 's/.*ClientAliveInterval.*/ClientAliveInterval 60/' /etc/ssh/sshd_configperl -p -i -e 's/.*ClientAliveCountMax.*/ClientAliveCountMax 10/' /etc/ssh/sshd_configperl -p -i -e 's/.*UseDNS.*/UseDNS no/' /etc/ssh/sshd_configperl -p -i -e 's/GSSAPIAuthentication yes/GSSAPIAuthentication no/' /etc/ssh/sshd_configsystemctl restart sshd.service 增加系统资源权限 1234567891011perl -p -i -e 's/^\*.*\n$//' /etc/security/limits.confcat &lt;&lt; EOF &gt;&gt; /etc/security/limits.conf* soft nofile 10240* hard nofile 102400* soft stack 10240* hard stack 102400* soft core unlimited* hard core unlimited* soft nproc 10240* hard nproc 102400EOF 配置集群中各服务器的主机名和IP地址使用root用户在管理节点 和计算节点服务器上对infiniband网口进行配置，修改 /etc/sysconfig/network-scripts/ifcfg-ib0 配置文件内容：12345BOOTPROTO=noneONBOOT=yesIPADDR=192.168.1.12PREFIX=24GATEWAY=192.168.1.1 修改好ifcfg文件后，重启网络服务，使生效：1systemctl restart network 各节点服务器在infiniband网络之间的联通需要在控制节点node1上安装一些相关的系统软件，并启用相应服务：1234yum install opensm* opensm-devel* infiniband-diags perftest* gperf* opensm*systemctl restart opensm.servicesystemctl enable rdma.servicesystemctl enable opensm.service 然后将所有节点服务器的 /etc/hosts 文件内容修改成同样的内容：123456cat &lt;&lt; EOF &gt; /etc/hosts192.168.1.12 master192.168.1.13 node01192.168.1.14 node02192.168.1.15 node03EOF 将控制节点的以太网共享给计算节点控制节点通过电信100M宽带连接外网，通过网线将master控制节点连接到电信网关（光猫和路由器合一的电信盒子）上。设置网口自动使用DHCP方法分配IP地址即可。在外网可以正常连接的情况，可以将该网络通过infiniband网卡共享给其它计算节点。 在master控制节点上使用root用户进行操作： 开启NAT转发 开放DNS使用的53端口并重启防火墙，否则可能导致内网服务器虽然设置正确的DNS，但是依然无法进行域名解析。 控制节点上是在eth0网口连接外网，对其网络进行共享。 123456789 firewall-cmd --permanent --zone=public --add-masquerade firewall-cmd --zone=public --add-port=53/tcp --permanentsystemctl restart firewalld.service echo 'net.ipv4.ip_forward=1' &gt;&gt; /etc/sysctl.conf sysctl -p firewall-cmd --permanent --direct --passthrough ipv4 -t nat -I POSTROUTING -o eth0 -j MASQUERADE -s 12.12.12.0/24 systemctl restart firewalld.service 在计算节点上对infiniband网卡进行IP设置时，将网关设置成提供网络的主机IP即可，即将网关设置成master管理节点的IP地址。 将控制节点的存储共享给计算节点在控制节点master服务器上，修改NFS配置文件/etc/sysconfig/nfs配置文件，打开所有带有PORT的注释行，表示使用相应的防火墙端口，并修改防火墙配置，开放对应端口：12345678910111213141516perl -p -i -e 's/^#(.*PORT)/$1/' /etc/sysconfig/nfsfirewall-cmd --add-port=32803/udp --permanentfirewall-cmd --add-port=32803/tcp --permanentfirewall-cmd --add-port=32769/udp --permanentfirewall-cmd --add-port=32769/tcp --permanentfirewall-cmd --add-port=892/udp --permanentfirewall-cmd --add-port=892/tcp --permanentfirewall-cmd --add-port=662/udp --permanentfirewall-cmd --add-port=662/tcp --permanentfirewall-cmd --add-port=2020/udp --permanentfirewall-cmd --add-port=2020/tcp --permanentfirewall-cmd --add-port=875/udp --permanentfirewall-cmd --add-port=875/tcp --permanentsystemctl restart firewalld.service 然后，在控制节点master服务器上，启动NFS服务，并设置成开机启动：12345systemctl restart rpcbind.servicesystemctl restart nfs.servicesystemctl enable rpcbind.servicesystemctl enable nfs.service 继续，在控制节点master服务器上， 修改/etc/exports文件内容，添加被共享的文件夹信息，并使配置生效：123456cat &lt;&lt; EOF &gt;&gt; /etc/exports/disk 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)/opt 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)EOFexportfs -rv 在各计算节点服务器上，使用root用户修改配置文件/etc/fstab，对master的共享文件夹进行挂载：12345678mkdir /diskcat &lt;&lt; EOF &gt;&gt; /etc/fstab192.168.1.12:/disk /disk nfs defaults 0 0192.168.1.12:/opt /opt nfs defaults 0 0EOFmount -a 在集群计算机上创建新用户首先，生成文件/disk/users.txt。该文件每行一个待生成的用户名。 然后，在所有节点服务器中进行操作，生成用户并使create_random_passwd.pl命令赋予随机密码：123456cd /diskfor i in `cat users.txt`do useradd $i 2&gt; /dev/null ./create_random_passwd.pl $idone 在控制节点master服务器中进行操作：在大容量存储对应的共享文件夹中建立新用户的专属文件夹；使用root用户生成新用户的ssh密钥对数据和授权文件信息并放入到各新用户的家目录下。1234567891011121314151617/bin/rm /disk/ssh_info/ -rfmkdir -p /disk/ssh_info/for i in `cat users.txt`do mkdir /disk/ssh_info//$i /disk/$i chown -R $i:$i /disk/$i chmod 700 /disk/$i ssh-keygen -t dsa -P '' -f /disk/ssh_info/$i/id_dsa chown -R $i:$i /disk/ssh_info/$i mkdir /home/$i/.ssh /bin/cp -a /disk/ssh_info/$i/* /home/$i/.ssh chown -R $i:$i /home/$i/.ssh chmod 700 /home/$i/.ssh cat /disk/ssh_info/$i/id_dsa.pub &gt;&gt; /home/$i/.ssh/authorized_keys chown -R $i:$i /home/$i/.ssh/authorized_keys chmod 600 /home/$i/.ssh/authorized_keysdone 在各个计算节点服务器中使用root用户将上一步生成的ssh密钥对数据和授权文件信息放入到计算节点服务器中各新用户的家目录下：123456789101112131415161718cd /diskfor i in `cat users.txt`do useradd $i 2&gt; /dev/null ./create_random_passwd.pl $idonefor i in `cat users.txt`do mkdir /home/$i/.ssh /bin/cp -a /disk/ssh_info/$i/* /home/$i/.ssh chown -R $i:$i /home/$i/.ssh chmod 700 /home/$i/.ssh cat /disk/ssh_info/$i/id_dsa.pub &gt;&gt; /home/$i/.ssh/authorized_keys chown -R $i:$i /home/$i/.ssh/authorized_keys chmod 600 /home/$i/.ssh/authorized_keysdone create_random_passwd.pl程序代码：12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/perl#use strict;use Getopt::Long;my $usage = &lt;&lt;USAGE;Usage: $0 [options] username 使用root用户执行该程序，输入用户名，则能调用passwd命令给该用户创建一个随机密码。并将用户名及其密码输出到标准输出。 --length &lt;int&gt; default:10 设置生成密码的字符长度。USAGEif (@ARGV==0) &#123; die $usage &#125;my $length;GetOptions( "length:i" =&gt; \$length,);$length ||= 10;my @cha = ('!', '@', '#', '$', '%', '^', '&amp;', '*', '.', '_');foreach (0..9) &#123; push @cha, $_;&#125;foreach (a..z) &#123; push @cha, $_;&#125;foreach (A..Z) &#123; push @cha, $_;&#125;my $passwd;for (1..$length) &#123; my $cha_num = rand(@cha); $passwd .= $cha[$cha_num];&#125;print "$ARGV[0]\t$passwd\n";my $cmdString = "echo \'$passwd\' | passwd --stdin $ARGV[0] &amp;&gt; /dev/null";(system $cmdString) == 0 or die "Faield to excute: $cmdString, $!\n"; 远程桌面软件vncserver安装和使用由于控制节点master是连接到了电信网关上，没有固定IP地址，推荐使用vnc来对内网服务器使用图形化桌面方法进行控制。 首先，使用root用户在master服务器上进行操作，安装vncserver软件并开放相应的防火墙5901，5902，5903端口：123456yum install vcn vnc-serverfirewall-cmd --zone=pulic --add-port=5901/tcp --permanentfirewall-cmd --zone=pulic --add-port=5902/tcp --permanentfirewall-cmd --zone=pulic --add-port=5903/tcp --permanentsystemctl restart firewalld.service 然后，使用普通用户（例如，train）开启vncserver服务：12vncserver# 第一次启动需要输入密码 进行其它vnc操作并修改桌面分辨率，提供更好的vnc体验：12345678910111213141516查看当前开启的vncserver桌面列表vncserver -list查看第一个vncserver桌面的端口号cat ~/.vnc/node1\:1.log关闭第一个vncserver桌面 vncserver -kill :1修改vncserver桌面的分辨率cat &lt;&lt; EOF &gt;&gt; .vnc/configgeometry=2000x1052EOF关闭后再次启动vncserver桌面，则分辨率变得更好了vncserve 为了让vnc能在外网对master进行控制。需要将master控制节点服务器和公网服务器使用ssh进行连接，开启反向隧道，并进行端口转发，在master服务器上进行操作。以下命令将master服务器VNC服务对应的5901端口映射到公网服务器xxx.xx.xxx.xx的4497端口上：1ssh -N -f -R 4497:localhost:5901 train@xxx.xx.xxx.xx 注意，以上命令需要在公网服务器xxx.xx.xxx.xx上拥有train用户的密码，才能ssh连接成功；并且，还需要使用该公网服务器的root用户开启4497防火墙端口，同时在ssh配置文件设置允许端口转发，才能使vnc访问生效。 最后，在windows系统下下载vncviewer软件，然后安装并打开软件，输入xxx.xx.xxx.xx:4497，再输入之前设置的密码，即可访问远程桌面。 在控制节点上控制计算节点的开机和关机在控制节点上，对计算节点可以使用ssh连接并导入shutdown指令的方法进关机。基于此原理，编写名为poweroff的Perl程序来对指定的节点进行关机。该程序代码：123456#!/usr/bin/perluse strict;my $usage = &lt;&lt;USAGE;Usage: $0 node10 node11 node12 ... 使用此命令关闭目标节点。该命令后可以输入1个或多个主机名，关闭相应的计算节点。若命令后输入的主机名中有一个是all，则会关闭所有的计算节点（从node11到node20）。此外，支持node11-node15这样中间带有中划线的输入方法，表示多个连续的节点。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647For example: $0 node11 node13-node16 node20USAGEif (@ARGV==0)&#123;die $usage&#125;my @node = qw/node10 node11 node12 node13 node14 node15 node16 node17 node18 node19 node20/;my %node;foreach (@node) &#123; $node&#123;$_&#125; = 1; &#125;my %target;foreach (@ARGV) &#123; if ($_ eq "all") &#123; foreach (@node) &#123; $target&#123;$_&#125; = 1; &#125; last; &#125; elsif (m/(\d+)-node(\d+)/) &#123; foreach ($1 .. $2) &#123; $target&#123;"node$_"&#125; = 1; &#125; &#125; else &#123; if (exists $node&#123;$_&#125;) &#123; $target&#123;$_&#125; = 1; &#125; else &#123; print STDERR "Warning: $_不是能控制的目标节点。\n"; &#125; &#125;&#125;foreach (sort keys %target) &#123; &amp;guanji($_);&#125;sub guanji &#123; print STDERR "正在检测到 $_ 的连接\n"; my $ping = `ping $_ -c 1`; if ($ping =~ m/Unreachable/) &#123; print STDERR "Warning: $_连接失败，可能已经处于关机状态。\n"; &#125; else &#123; my $cmdString = "ssh $_ 'sudo shutdown -h now' &amp;&gt; /dev/null"; system $cmdString; print "对主机 $_ 已经发送关机指令\n"; &#125;&#125; 在控制节点node1上，可以使用wol软件基于网络唤醒的方法对计算节点进行开机。基于此原理，编写名为 kaiji 的Perl程序对指定节点进行开机。该程序代码：123456#!/usr/bin/perluse strict;my $usage = &lt;&lt;USAGE;Usage: $0 node10 node11 node12 ... 使用此命令开启目标节点。该命令后可以输入1个或多个主机名，开启相应的计算节点。若命令后输入的主机名中有一个是all，则会开启所有的计算节点（从node11到node20）。此外，支持node11-node15这样中间带有中划线的输入方法，表示多个连续的节点。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849For example: $0 node11 node13-node16 node20USAGEif (@ARGV==0)&#123;die $usage&#125;my @node = qw/node10 node11 node12 node13 node14 node15 node16 node17 node18 node19 node20/;my %node = ("node10" =&gt; "00:e0:ec:27:e9:f0","node11" =&gt; "e8:61:1f:11:e9:4b","node12" =&gt; "e8:61:1f:11:e8:3f","node13" =&gt; "e8:61:1f:1b:ec:80","node14" =&gt; "e8:61:1f:1b:ed:84","node15" =&gt; "e8:61:1f:1b:ec:9e","node16" =&gt; "e8:61:1f:1b:ed:0e","node17" =&gt; "e8:61:1f:1b:ed:b4","node18" =&gt; "e8:61:1f:1b:ec:94","node19" =&gt; "e8:61:1f:1b:ec:5a","node20" =&gt; "e8:61:1f:1b:eb:d0");my %target;foreach (@ARGV) &#123; if ($_ eq "all") &#123; foreach (@node) &#123; $target&#123;$_&#125; = 1; &#125; last; &#125; elsif (m/(\d+)-node(\d+)/) &#123; foreach ($1 .. $2) &#123; $target&#123;"node$_"&#125; = 1; &#125; &#125; else &#123; if (exists $node&#123;$_&#125;) &#123; $target&#123;$_&#125; = 1; &#125; else &#123; print STDERR "Warning: $_不是能控制的目标节点。\n"; &#125; &#125;&#125;foreach (sort keys %target) &#123; &amp;kaiji($_);&#125;sub kaiji &#123; print "对主机 $_ 已经发送开机指令\n"; my $cmdString = "/opt/sysoft/wol-0.7.1/bin/wol --host=10.10.10.255 $node&#123;$_&#125;"; system $cmdString;&#125;]]></content>
      <categories>
        <category>HPC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Centos7部署SGE]]></title>
    <url>%2F2019%2F05%2F15%2F%E5%9C%A8Centos7%E9%83%A8%E7%BD%B2SGE%2F</url>
    <content type="text"><![CDATA[设置防火墙，放行SGE所需端口1234# firewall-cmd --add-port=992/udp --permanent# firewall-cmd --add-port=6444/tcp --permanent# firewall-cmd --add-port=6445/tcp --permanent# systemctl restart firewalld.service 从SGE官网下载最新版本的SGE源码包并进行编译和安装安装依赖的系统软件12# yum install csh java-1.8.0-openjdk java-1.8.0-openjdk-devel gcc ant automake hwloc-devel openssl-devel libdb-devel pam-devel libXt-devel motif-devel ncurses-libs ncurses-devel# yum install ant-junit junit javacc 下载SGE软件并进行编译12345$ wget https://arc.liv.ac.uk/downloads/SGE/releases/8.1.9/sge-8.1.9.tar.gz -P ~/software/$ tar zxf ~/software/sge-8.1.9.tar.gz$ cd sge-8.1.9/source$ ./scripts/bootstrap.sh$ ./aimk -no-herd -no-java 将编译好的SGE安装到指定的文件夹1234567# mkdir /opt/sysoft/sge# export SGE_ROOT=/opt/sysoft/sge# ./scripts/distinst -local -allall -noexit# cd ../../ &amp;&amp; rm sge-8.1.9/ -rf# echo 'export SGE_ROOT=/opt/sysoft/sge' &gt;&gt; ~/.bashrc# echo 'PATH=$PATH:/opt/sysoft/sge/bin/:/opt/sysoft/sge/bin/lx-amd64/' &gt;&gt; ~/.bashrc# source ~/.bashrc 部署SGE前设置主机名部署SGE前，需要设置好各个节点的主机名，需要修改3个文件。修改配置文件 /etc/sysconfig/network 内容：12NETWORKING=yesHOSTNAME=master 修改配置文件 /proc/sys/kernel/hostname 内容：1master 修改配置文件 /etc/hosts 内容（注意删除掉127.0.0.1和localhost的行）：1234192.168.30.1 master192.168.30.2 node1192.168.30.3 node2192.168.30.4 node3 在所有节点上部署SGE12cd $SGE_ROOT./install_qmaster 运行部署命令后，会进入交互式界面。基本上全部都按Enter键使用默认设置即可。需要注意的事项是： 有一步骤是启动Grid Engine qmasster服务，可能会启动不了导致失败。原因是多次运行该命令进行部署，第一次会成功运行qmaster daemon，以后重新运行该程序进行部署则会失败。需要删除相应的sge_qmaster进程再进行部署。 启动Grid Engine qmasster服务，要提供部署SGE的节点主机名信息，按y和Enter键使用一个文件来提供主机信息，输入文件路径/etc/hosts提供主机信息。 只有先进行一个控制节点部署后，才能对各个计算节点进行部署。计算节点的部署比较简单，交互过程全部按Enter即可。1./install_execd 启动SGE软件部署完毕后，若需要使用SGE软件，则执行如下命令载入SGE的环境变量信息：1$ source /opt/sysoft/sge/default/common/settings.sh 或将该信息添加到~/.bashrc从而永久生效：12$ echo 'source /opt/sysoft/sge/default/common/settings.sh' &gt;&gt; ~/.bashrc$ source ~/.bashrc 启动SGE软件方法：12$ /opt/sysoft/sge/default/common/sgemaster # 控制节点启动$ /opt/sysoft/sge/default/common/sgeexecd # 计算节点启动 查看SGE软件运行日志文件：12Qmaster: /opt/sysoft/sge/default/spool/qmaster/messagesExec daemon: /opt/sysoft/sge/default/spool/&lt;hostname&gt;/messages 使用SGE软件部署完毕SGE后，会生成一个默认主机用户组@allhosts，它包含所有的执行节点；生成一个默认的all.q队列名，它包含所有节点所有计算资源。默认的队列包含的计算资源是最大的。 通过使用命令qconf -mq queuename来对队列进行配置。修改hostlist来配置该队列可以使用执行主机；修改slots来配置各台执行主机可使用的线程数。从而对队列的计算资源进行设置。 使用qconf命令对SGE进行配置：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758qconf -ae hostname 添加执行主机qconf -de hostname 删除执行主机qconf -sel 显示执行主机列表qconf -ah hostname 添加管理主机qconf -dh hostname 删除管理主机qconf -sh 显示管理主机列表qconf -as hostname 添加提交主机qconf -ds hostname 删除提交主机qconf -ss 显示提交主机列表qconf -ahgrp groupname 添加主机用户组qconf -mhgrp groupname 修改主机用户组qconf -shgrp groupname 显示主机用户组成员qconf -shgrpl 显示主机用户组列表qconf -aq queuename 添加集群队列qconf -dq queuename 删除集群队列qconf -mq queuename 修改集群队列配置qconf -sq queuename 显示集群队列配置qconf -sql 显示集群队列列表qconf -ap PE_name 添加并行化环境qconf -mp PE_name 修改并行化环境qconf -dp PE_name 删除并行化环境qconf -sp PE_name 显示并行化环境qconf -spl 显示并行化环境名称列表qstat -f 显示执行主机状态qstat -u user 查看用户的作业qhost 显示执行主机资源信息 使用qsub提交作业12345678910111213141516171819202122232425262728293031qsub简单示例：$ qsub -V -cwd -o stdout.txt -e stderr.txt run.sh其中run.sh中包含需要运行的程序，其内容示例为如下三行：#!/bin/bash#$ -S /bin/bashperl -e 'print "abc\n";print STDERR "123\n";'qsub的常用参数：-V 将当前shell中的环境变量输出到本次提交的任务中。-cwd 在当前工作目录下运行程序。默认设置下，程序的运行目录是当前用户在其计算节点的家目录。-o 将标准输出添加到指定文件尾部。默认输出文件名是$job_name.o$job_id。-e 将标准错误输出添加到指定文件尾部。默认输出文件名是$job_name.e$job_id。-q 指定投递的队列，若不指定，则会尝试寻找最小负荷且有权限的队列开始任务。-S 指定运行run.sh中命令行的软件，默认是tcsh。推荐使用bash，设置该参数的值为 /bin/bash 即可，或者在run.sh文件首部添加一行#$ -S /bin/bash。若不设置为bash，则会在标准输出中给出警告信息：Warning: no access to tty (Bad file descriptor)。-hold_jid 后接多个使用逗号分隔的job_id，表示只有在这些job运行完毕后，才开始运行此任务。-N 设置任务名称。默认的job name为qsub的输入文件名。-p 设置任务优先级。其参数值范围为 -1023 ~ 1024 ，该值越高，越优先运行。但是该参数设置为正数需要较高的权限，系统普通用户不能设置为正数。-j y|n 设置是否将标准输出和标准错误输出流合并到 -o 参数结果中。-pe 设置并行化环境。 任务提交后的管理：1234567891011$ qstat -f 查看当前用户在当前节点提交的所有任务，任务的状态有4中情况：qw，等待状态，刚提交任务的时候是该状态，一旦有计算资源了会马上运行；hqw，该任务依赖于其它正在运行的job，待前面的job执行完毕后再开始运行，qsub提交任务的时候使用-hold_jid参数则会是该状态；Eqw，投递任务出错；r，任务正在运行；s，被暂时挂起，往往是由于优先级更高的任务抢占了资源；dr，节点挂掉后，删除任务就会出现这个状态，只有节点重启后，任务才会消失。$ qstat -j jobID 按照任务id查看$ qstat -u user 按照用户查看$ qdel -j jobID 删除任务]]></content>
      <categories>
        <category>HPC</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[cobbler批量部署系统]]></title>
    <url>%2F2019%2F05%2F12%2Fcobbler%E6%89%B9%E9%87%8F%E9%83%A8%E7%BD%B2%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[准备工作关闭防火墙和selinux123[root@cobbler ~]# sed -i s#SELINUX=enforcing#SELINUX=disabled# /etc/selinux/config[root@cobbler ~]# systemctl stop firewalld[root@cobbler ~]# systemctl disable firewalld 修改网卡信息1[root@cobbler ~]# nmcli connection modify eth0 ipv4.addresses 192.168.221.10/24 ipv4.gateway 192.168.221.2 ipv4.dns 192.168.221.2 ipv4.method manual connection.autoconnect yes connection.interface-name eth0 安装cobbler相关程序包1[root@cobbler ~]# yum install cobbler cobbler-web pykickstart httpd dhcp tftp-server -y 启动cobbler和httpd服务12[root@cobbler ~]# systemctl start httpd cobblerd[root@cobbler ~]# systemctl enable httpd cobblerd 检查cobbler配置1234567891011121314[root@cobbler ~]# cobbler checkThe following are potential configuration items that you may want to fix: 1 : The 'server' field in /etc/cobbler/settings must be set to something other than localhost, or kickstarting features will not work. This should be a resolvable hostname or IP for the boot server as reachable by all machines that will use it.2 : For PXE to be functional, the 'next_server' field in /etc/cobbler/settings must be set to something other than 127.0.0.1, and should match the IP of the boot server on the PXE network.3 : change 'disable' to 'no' in /etc/xinetd.d/tftp4 : Some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run 'cobbler get-loaders' to download them, or, if you only want to handle x86/x86_64 netbooting, you may ensure that you have installed a *recent* version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The 'cobbler get-loaders' command is the easiest way to resolve these requirements.5 : enable and start rsyncd.service with systemctl6 : debmirror package is not installed, it will be required to manage debian deployments and repositories7 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to 'cobbler' and should be changed, try: "openssl passwd -1 -salt 'random-phrase-here' 'your-password-here'" to generate new one8 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use them Restart cobblerd and then run 'cobbler sync' to apply changes. Cobbler相关问题问题1修改/etc/cobbler/settings文件中的server参数的值为提供cobbler服务的主机相应的IP地址或主机名12[root@cobbler cobbler]# cp /etc/cobbler/settings&#123;,.ori&#125;[root@cobbler cobbler]# sed -i 's/server: 127.0.0.1/server: 192.168.221.10/' /etc/cobbler/settings 问题2修改/etc/cobbler/settings文件中的next_server参数的值为提供PXE服务的主机相应的IP地址1234[root@cobbler cobbler]# sed -i 's/next_server: 127.0.0.1/next_server: 192.168.221.10/' /etc/cobbler/settings[root@cobbler cobbler]# grep "server: 192.168.221.10" /etc/cobbler/settingsnext_server: 192.168.221.10server: 192.168.221.10 问题3修改/etc/xinetd.d/tftp文件中的disable参数修改为 disable = no12[root@cobbler cobbler]# cp /etc/xinetd.d/tftp&#123;,.bak&#125;[root@cobbler cobbler]# sed -i 's/disable.*= yes/disable = no/g' /etc/xinetd.d/tftp 问题4执行 cobbler get-loaders 命令即可；否则，需要安装syslinux程序包，而后复制/usr/share/syslinux/{pxelinux.0,memu.c32}等文件至/var/lib/cobbler/loaders/目录中1234567891011121314[root@cobbler cobbler]# cobbler get-loaderstask started: 2018-08-30_170803_get_loaderstask started (id=Download Bootloader Content, time=Thu Aug 30 17:08:03 2018)downloading https://cobbler.github.io/loaders/README to /var/lib/cobbler/loaders/READMEdownloading https://cobbler.github.io/loaders/COPYING.elilo to /var/lib/cobbler/loaders/COPYING.elilodownloading https://cobbler.github.io/loaders/COPYING.yaboot to /var/lib/cobbler/loaders/COPYING.yabootdownloading https://cobbler.github.io/loaders/COPYING.syslinux to /var/lib/cobbler/loaders/COPYING.syslinuxdownloading https://cobbler.github.io/loaders/elilo-3.8-ia64.efi to /var/lib/cobbler/loaders/elilo-ia64.efidownloading https://cobbler.github.io/loaders/yaboot-1.3.17 to /var/lib/cobbler/loaders/yabootdownloading https://cobbler.github.io/loaders/pxelinux.0-3.86 to /var/lib/cobbler/loaders/pxelinux.0downloading https://cobbler.github.io/loaders/menu.c32-3.86 to /var/lib/cobbler/loaders/menu.c32downloading https://cobbler.github.io/loaders/grub-0.97-x86.efi to /var/lib/cobbler/loaders/grub-x86.efidownloading https://cobbler.github.io/loaders/grub-0.97-x86_64.efi to /var/lib/cobbler/loaders/grub-x86_64.efi*** TASK COMPLETE *** 问题5123[root@cobbler cobbler]# systemctl start rsyncd[root@cobbler cobbler]# systemctl enable rsyncd问题6 问题61234[root@cobbler cobbler]# yum install debmirror -y[root@cobbler cobbler]# vi /etc/debmirror.conf#@dists="sid";#@arches="i386"; 问题7生成密码来取代默认的密码，前者为干扰码，后者为真正的密码12345[root@cobbler cobbler]# openssl passwd -1 -salt 'jay.cheng' '123456'$1$jay.chen$1Ktf4J.R.RsFfY3mz63Ro/[root@cobbler cobbler]# sed -i s/'default_password_crypted:.*'/'default_password_crypted: "$1$jay.chen$1Ktf4J.R.RsFfY3mz63Ro\/"'/g /etc/cobbler/settings[root@cobbler cobbler]# grep -n default_pass /etc/cobbler/settings101:default_password_crypted: "$1$jay.chen$1Ktf4J.R.RsFfY3mz63Ro/" 问题81[root@cobbler cobbler]# yum install -y fence-agents 由cobbler管理DHCP和防止循环装系统12345678[root@cobbler cobbler]# sed -i 's/manage_dhcp: 0/manage_dhcp: 1/g' /etc/cobbler/settings[root@cobbler cobbler]# grep -n manage_dhcp /etc/cobbler/settings242:manage_dhcp: 1269:# if using cobbler with manage_dhcp, put the IP address355:# Note that if manage_dhcp and manage_dns are disabled, the respective[root@cobbler cobbler]# sed -i 's/pxe_just_once: 0/pxe_just_once: 1/' /etc/cobbler/settings[root@cobbler cobbler]# grep -n pxe_just_once /etc/cobbler/settings292:pxe_just_once: 1 重启cobbler服务后，再次运行检查配置命令123[root@cobbler cobbler]# systemctl restart cobblerd.service[root@cobbler cobbler]# cobbler checkNo configuration problems found. All systems go.]]></content>
  </entry>
  <entry>
    <title><![CDATA[构建标准且人性化镜像]]></title>
    <url>%2F2019%2F05%2F02%2F%E6%9E%84%E5%BB%BA%E6%A0%87%E5%87%86%E4%B8%94%E4%BA%BA%E6%80%A7%E5%8C%96%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[正确的FROM合适的镜像很多新手一上来就是FROM centos然后RUN 一堆yum install的，这样还停留在虚拟机的角度。可以FROM alpine或者干脆拿官方的改，alpine初期的时候问题蛮多的，很多人建议使用alpine做基础镜像最好是测试好再上线，现在alpine的快速发展，这种现象很少了。 不要用imageID或者latest标签id的话不便于长期发展，而latest标签无法回滚。 不要重复造轮子现在dockerhub上有很多的镜像了，很多人还是喜欢造轮子，造出来的镜像层又多，无用的文件又停留在层理，主进程还不是业务进程，还不支持传入环境变量来让用户选择场景和传入配置信息启动。 如果你的是一个java应用，那么你应该使用java作为基础应用，如果你是tomcat应用，你应该使用tomcat作为基础应用，而不是按照虚拟机的思维，把Java装好，然后装应用；tomcat也一样，装java，装tomcat，装应用。 镜像大小之前我举例的ADD添加源码包和RUN rm -f删掉ADD的源码包，虽说最终起来的容器看不到源码包。实际上文件还停留在镜像的层里，所以尽量合并和减少层防止层保持住文件。 最后一些零散的建议和常见错误 编写entrypoint脚本让启动更人性化 同时如果是初期上docker到生产，考虑到排错啥的，可以在官方dockerfile里添加一些常见的排错命 尽量使用ENV和ARG让人不改或者少改Dockerfile即可做构建对应版本的镜像 容器时间不对的话可以安装包tzdate，声明变量TZ即可声明时区，或者构建的时候带上/etc/localtime或者运行的时候挂载宿主机的/etc/localtime。 如果是编译型语言，妥善利用多阶段构建（后面容器无法运行排错的时候会讲解多阶构建） 代码里应该要注意优雅退出。收到信号的时候释放东西啥的。 代码，war，jar，go编译的二进制到底应不应该放在镜像里？其实现在的java和php，还有go啥的依赖的运行环境基本不会变，变更发布新版本也就只有代码，war，jar和go编译的二进制，为此可以两种做法: 全部打包到镜像里 不变的层做个镜像，启动利用entrypoint脚本接受传入的git分支或者war包啥的内网下载直链下载到容器里或者启动直接挂载nfs里的war包或者代码啥的启动 很多人都是传统的第一种思维，看到第二种的时候直接张口说这样不行。如果后续接触到了k8s会发现k8s有个initContainers，谷歌也说了可以利用initContainers去初始化或者克隆git代码。 其实两种均可，例如第一种，在没有gc原生docker下，每一次发布都会老版本镜像存在，虽说层共享，但是最后的代码层的容量还是占据了宿主机容量的。 第二种每次启动都需要下载，需要网速，如果是内网可以尝试，代码或者war包啥的都是在容器层，不会吃宿主机多大容量。实在接受不了可以运维给研发做个这种通用镜像给他们用。 最后是推荐一个漠然大佬的示例，漠然大佬的github上很多镜像下载量很多，可以去他github看，这里我放下他的java的应用示例 https://github.com/Gozap/dockerfile]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[exec与entrypoint使用脚本]]></title>
    <url>%2F2019%2F05%2F02%2Fexec%E4%B8%8Eentrypoint%E4%BD%BF%E7%94%A8%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[现在很多有状态的官方镜像的ENTRYPOINT都是使用了一个脚本。例如redis12345COPY docker-entrypoint.sh /usr/local/bin/ENTRYPOINT ["docker-entrypoint.sh"]EXPOSE 6379CMD ["redis-server"] 12345678910111213141516#!/bin/shset -e# first arg is `-f` or `--some-option`# or first arg is `something.conf`if [ "$&#123;1#-&#125;" != "$1" ] || [ "$&#123;1%.conf&#125;" != "$1" ]; then set -- redis-server "$@"fi# allow the container to be started with `--user`if [ "$1" = 'redis-server' -a "$(id -u)" = '0' ]; then find . \! -user redis -exec chown redis '&#123;&#125;' + exec gosu redis "$0" "$@"fiexec "$@" 最终运行的是docker-entrypoint.sh redis-server 第一个if的逻辑是如果docker run 选项 redis -x 或者–xx或者xxx.conf，就把脚本收到的$@改编成redis-server $@,例如我们可同docker run -d redis –port 7379修改启动的容器里的redis端口。如果我们传入的command不是-开头的也不是.conf结尾的字符，例如是date，则会跑到最后的逻辑执行我们的date命令不会启动redis-server 第二个if这里，如果满足第一个if或者直接默认的cmd下而且容器里用户uid是0，则把属主不是redis的文件改成redis用户，然后切成redis用户去启动redis-server。 我们可以看到entrypoint能在业务进程启动前做很多事情。而且优秀的镜像都离不开entrypoint脚本，能够根据用户传入的变量和command来切换启动的场景和配置。 前面说了，主进程一定要是业务进程，这里怎么是个脚本呢，那业务进程不就不是pid为1了吗？ 这里用了exec来退位让贤，最终redis-server还是pid为1的。可以简单几个命令讲解下exec的作用。 写个test.sh脚本，在脚本里用pstree -p，运行脚本bash test.sh查看进程层次 发现pstree是在我们脚本bash(1998)的子进程 然后在脚本最后面加一行exec pstree -p看看输出 我们发现bash进程运行的时候pid是2022，然后第二个pstree上升到了2022这一层次了，假设pid为a的命令或者二进制exec执行了命令b，那b就接替了a的pid。如果说我们entrypoint或者cmd使用脚本，那么我们一定要在脚本最后启动业务进程的时候前面加个exec让脚本退位让贤。 最后环境变量写配置文件涉及到修改，还有一些判断是否初次启动的有下面一些工具或者套路。 xmlstarlet 处理xml pip安装shyaml 处理yaml jq读取json nodejs的npm安装json可以修改json文件 处理excel或者csv使用in2csv，csvkit 提供了 in2csv，csvcut，csvjoin，csvgrep touch -d “@0”写在构建的最后一个RUN里把时间戳设置为1970-1-1，然后用stat命令判断 if [ “$(stat -c “%Y” “${CONF_INSTALL}/conf/server.xml”)” -eq “0” ]; then 另外entrypoint脚本COPY进去的时候注意可执行权限，如果Windows上传到Linux构建会因为entrpoint脚本没带权限无法运行]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile STOPSIGNAL]]></title>
    <url>%2F2019%2F05%2F02%2FDockerfile-STOPSIGNAL%2F</url>
    <content type="text"><![CDATA[格式，缺省信号为SIGTERM1234STOPSIGNAL signal------STOPSIGNAL SIGTERMSTOPSIGNAL 9 可以是kill -l的信号名字也可以信号数字:1234567891011121314kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX docker run的选项可以覆盖镜像定义的STOPSIGNAL信号1--stop-signal string Signal to stop a container (default "SIGTERM") 在docker stop停止运行容器的时候指定发送给容器里pid为1角色的信号。默认超时10秒，超时则发送kill强杀进程。一般业务进程都是pid为1，所有官方的进程都会处理收到的SIGTERM信号进行优雅收尾退出。 前面说过了如果CMD是/bin/sh格式的话，主进程是一个sh -c的进程，shell不用trap处理的话是无法转发信号的。下面我举个例子 例子是是网上找的，两种CMD方式启动的redis123456789FROM ubuntu:14.04RUN apt-get update &amp;&amp; apt-get -y install redis-server &amp;&amp; rm -rf /var/lib/apt/lists/*EXPOSE 6379CMD /usr/bin/redis-server----------------------------FROM ubuntu:14.04RUN apt-get update &amp;&amp; apt-get -y install redis-server &amp;&amp; rm -rf /var/lib/apt/lists/*EXPOSE 6379CMD ["/usr/bin/redis-server"] 构建两种镜像，然后docker run -d img_name，然后docker stop这俩镜像启动的容器会发现exec的redis能在docker stop的时候收到信号优雅退出Received SIGTERM, scheduling shutdown123456[1] 11 Feb 08:13:01.633 * The server is now ready to accept connections on port 6379[1 | signal handler] (1455179074) Received SIGTERM, scheduling shutdown...[1] 11 Feb 08:24:34.259 # User requested shutdown...[1] 11 Feb 08:24:34.259 * Saving the final RDB snapshot before exiting.[1] 11 Feb 08:24:34.262 * DB saved on disk[1] 11 Feb 08:24:34.262 # Redis is now ready to exit, bye bye... 而/bin/sh的形式的redis在docker stop后去docker logs看日志会发现根本没有优雅退出，类似于强制杀掉一样。1[5] 11 Feb 08:12:40.109 * The server is now ready to accept connections on port 6379 这是因为/bin/sh形式启动的redis主进程是一个sh，shell不会转发信号，所以最后sh被超时的docker stop发送了kill信号杀掉，整个容器生存周期结束，redis没有触发signal handler。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile VOLUME]]></title>
    <url>%2F2019%2F05%2F02%2FDockerfile-VOLUME%2F</url>
    <content type="text"><![CDATA[VOLUME两种写法，无区别12VOLUME ["/data","/mysql"]VOLUME /var/log /var/db 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中。 为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ENTRYPOINT]]></title>
    <url>%2F2019%2F05%2F02%2FDockerfile-ENTRYPOINT%2F</url>
    <content type="text"><![CDATA[ENTRYPOINT和CMD用法也一样两种格式，唯一要注意的就是区别，CMD和ENTRYPOINT只有一个或者两者都有都可以，容器最终运行的命令为：1&lt;ENTRYPOINT&gt; &lt;CMD&gt; alpine的root目录是没有文件的，所以ls /root没有输出，我们用选项去覆盖住entrypoint可以看到输出了date。注意一点是覆盖entrypoint的时候镜像的CMD会被忽略，我们真要调试的时候需要加command的话，可以在docker run的镜像后面加command和arg。 上面例子可以很形象的证明了是这个关系，最终运行的是 ，同时不光在docker run的时候覆盖掉CMD，也可以覆盖掉默认的entrypoint。很多时候我们可以主进程bash或者sh进去手动启动看看。老版本接触不多，不确定老版本有没有–entrypoint的选项。 最后如果是/bin/sh的entrypoint会忽略掉CMD和docker run的command参数]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建私有YUM源]]></title>
    <url>%2F2019%2F05%2F01%2F%E5%88%9B%E5%BB%BA%E7%A7%81%E6%9C%89YUM%E6%BA%90%2F</url>
    <content type="text"><![CDATA[安装httpd1yum -y install httpd 安装所需软件1yum -y install rsync createrepo 创建相关目录1mkdir -p /var/www/repos/centos/7/&#123;os,updates,extras&#125;/x86_64 赋予读写权限1chmod -R 755 /var/www/repos 从清华源同步1234567891011rsync -avz --delete --exclude='repodata' \rsync://mirrors.tuna.tsinghua.edu.cn/centos/7/os/x86_64/ \/var/www/repos/centos/7/os/x86_64/ rsync -avz --delete --exclude='repodata' \rsync://mirrors.tuna.tsinghua.edu.cn/centos/7/updates/x86_64/ \/var/www/repos/centos/7/updates/x86_64/ rsync -avz --delete --exclude='repodata' \rsync://mirrors.tuna.tsinghua.edu.cn/centos/7/extras/x86_64/ \/var/www/repos/centos/7/extras/x86_64/ 创建 metadata repositories123createrepo /var/www/repos/centos/7/os/x86_64/ createrepo /var/www/repos/centos/7/updates/x86_64/ createrepo /var/www/repos/centos/7/extras/x86_64/ 设置定时任务，每天同步1234567891011# vi /etc/cron.daily/update-repo#!/bin/bashVER='7'ARCH='x86_64'REPOS=(os updates extras)for REPO in $&#123;REPOS[@]&#125;do rsync -avz --delete --exclude='repodata' \ rsync://mirrors.tuna.tsinghua.edu.cn/centos/$&#123;VER&#125;/$&#123;REPO&#125;/$&#123;ARCH&#125;/ /var/www/repos/centos/$&#123;VER&#125;/$&#123;REPO&#125;/$&#123;ARCH&#125;/ createrepo /var/www/repos/centos/$&#123;VER&#125;/$&#123;REPO&#125;/$&#123;ARCH&#125;/done 赋予权限1chmod 755 /etc/cron.daily/update-repo 配置httpd主机使其他客户端访问123456# vim /etc/httpd/conf.d/repos.confAlias /repos /var/www/repos&lt;directory /var/www/repos&gt; Options +Indexes Require all granted&lt;/directory&gt; 启动httpd服务12systemctl start httpdsystemctl enable httpd 客户端的配置文件，其中10.105.26.110是源服务器地址12345678910111213141516171819# vi /etc/yum.repos.d/CentOS-Base.repo[base]name=CentOS-$releasever - Base#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os&amp;infra=$infrabaseurl=http://10.105.26.110/repos/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[updates]name=CentOS-$releasever - Updates#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates&amp;infra=$infrabaseurl=http://10.105.26.110/repos/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[extras]name=CentOS-$releasever - Extras#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras&amp;infra=$infrabaseurl=http://10.105.26.110/repos/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos7修改网卡名称]]></title>
    <url>%2F2019%2F05%2F01%2FCentos7%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E5%90%8D%E7%A7%B0%2F</url>
    <content type="text"><![CDATA[修改设备名称1sed -i "s/ens33/eth0/g" /etc/sysconfig/network-scripts/ifcfg-ens33 重命名网卡配置文件1mv /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-eth0 修改grub文件1sed -i "s/root/root net.ifnames=0 biosdevname=0/g" /etc/default/grub 重新生成GRUB配置并更新内核参数，稍后重启12grub2-mkconfig -o /boot/grub2/grub.cfgreboot]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile CMD]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-CMD%2F</url>
    <content type="text"><![CDATA[CMD 与进程前后台和容器存活的关系设置镜像运行出来的容器的缺省命令 有两种写法，写多个和FROM一个已经有CMD的镜像的话，以最后一个为准12CMD ["executable", "param1", "param2"] CMD command param1 param2 前者是exec格式也是推荐格式，后者是/bin/sh格式，exec和CMD还有ENTRYPOINT这三者之间联系非常紧密，后面单独将相关的知识点。这里先用一个例子讲/bin/sh格式啥意思 我们发现pid为1的是一个/bin/sh的进程，而我们的进程在容器里在后面。容器是单独一个pid namespaces的。这里懒得去做个图了，借用下别人的图 默认下所有进程在一个顶级的pid namespaces里，pid namespaces像一个树一样。从根到最后可以多级串。容器的pid namespaces实际上是在宿主机上能看到的，也就是下面，我们可以看到容器在宿主机上的进程，由于子namespaces无法看到父级的namespaces，所以容器里第一个进程(也就是cmd)认为自己是pid为1，容器里其余进程都是它的子进程 在Linux中，只能给init已经安装信号处理函数的信号，其它信号都会被忽略，这可以防止init进程被误杀掉，即使是superuser。所以，kill -9 init不会kill掉init进程。但是容器的进程是在容器的ns里是init级别，我们可以在宿主机上杀掉它，之前线上的低版本docker 命令无法使用，同事无法停止错误容器，我便询问了进程名在宿主机找到后kill掉的。 接下来说说为啥推荐exec格式，exec格式的话第一个进程是我们的sleep进程，大家可以自己去构建镜像试试。推荐用exec格式是因为pid 为1的进程承担着pid namespaces的存活周期，听不懂的话我举个例子12345678[root@docker ~]# docker run -d alpine lsb2eedc510e718d2820ce79fcf630aa9521fc3525b9138a51f1f8bef496e2607a[root@docker ~]# docker run -d alpine sleep 10bee830e62508b52796f588d6defe5419e35acb6c944f0151e0cb4b40a260ef81[root@docker ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbee830e62508 alpine "sleep 10" 19 seconds ago Exited (0) 7 seconds ago sad_lamarrb2eedc510e71 alpine "ls" 28 seconds ago Exited (0) 26 seconds ago reverent_stallman 先看下docker run命令格式123# docker run --helpUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] docker run 后面镜像后面的command和arg会覆盖掉镜像的CMD。上面我那个例子覆盖掉centos镜像默认的CMD bash。我们可以看到ls的容器直接退出了，但是sleep 10的容器运行了10秒后就退出了。以上也说明了容器不是虚拟机，容器是个隔离的进程。 这说明了容器的存活是容器里pid为1的进程运行时长决定的。所以nginx的官方镜像里就是用的exec格式让nginx充当pid为1的角色。1CMD ["nginx", "-g", "daemon off;"] 这里nginx启动带了选项是什么意思呢，我举个初学者自己造轮子做nginx镜像来举例，也顺带按照初学者重复造轮子碰到错误的时候应该怎样去排查？上面我是按照初学者虚拟机的思维去做一个nginx镜像，结果构建错误，我们发现有个失败的容器就是RUN那层创建出来的，前面我说的实际上docker build就是运行容器执行步骤然后最后底层调用commit的原因。 现在我们来手动排下错，哪步报错可以把那步到后面的全部注释掉后构建个镜像，然后我们run起来的时候带上-ti选项分配一个能输入的伪终端，最后的command用sh或者bash，这样容器的主进程就是bash或者sh了，我们在里面执行报错的RUN(这里我例子简单，所以我直接run -ti centos bash)。实际上会发现nginx是在epel-release的源里，接下来改下Dockerfile再构建试试.123456# cat DockerfileFROM centosRUN yum install -y epel-release \ &amp;&amp; yum install -y nginxCMD ["nginx"]$ docker build -t test . 然后又是一个新手自己做镜像遇到的问题了，这个镜像运行了根本跑不起来，我们手动bash或者sh进去排查。12345$ docker run -d -p 80:80 testf13e98d4dc13b6fa13e375ca35cc58a23a340a07b677f0df245fc1ef3b7199c6$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf13e98d4dc13 test "nginx" 3 seconds ago Exited (0) 1 second ago determined_elgamal 似乎是卡主了？我们可以访问宿主机的ip:80看看会发现实际能访问到的，也就是说这样也是在运行，当然我们把CMD改成和官方一样直接docker run -d -p 80:80 test的话容器是不会退出的。 至于说为啥？答案就是前台的概念！ 我们有没有发现我们手动执行nginx带关闭daemon选项发现类似于hang住一样，实际上它就是前台跑。 单独的nginx回车，实际上是它拉起来了nginx，然后它退出了，但是！！！，别忘记了你这个nginx是pid为1的角色，你退出了你下面子进程全部完蛋，容器也会显示退出。所以既然你最终要跑nginx，你nginx得是前台跑。 但是这里肯定也有人说如果我主进程跑一个不退出的进程，然后进去启动nginx不也跑起来了吗？这样是可以的，但是存在信号转发机制和要考虑优雅退出，这块知识我在后面指令STOPSIGNAL讲。 判断一个命令(或者说带上选项)是不是前台跑的最简单一个验证就是(主进程sh或者bash进去后)执行它看它有没有回到终端。例如ls和yes命令，我们会发现yes命令一直刷y没有回到终端。 其实发展到现在，很多以前只有daemon后台跑的进程都慢慢的在docker火热下开始有前台运行的选项或者配置了，例如 redis的配置文件不写日志文件路径它就默认前台跑 uwsgi也是一样，命令行参数或者配置文件指定了日志文件路径就后台跑，否则前台跑 node本身是前台跑，但是一些信号可能不好处理，于是有了pm2 zabbix 的日志路径写console的话就是前台跑 其实我们用上前台选项的话也无法用docker logs看容器的log，是因为docker logs查看的是容器里的标准输出信息，我们可以看到官方nginx镜像Dockerfile是这样做的。123# forward request and error logs to docker log collector &amp;&amp; ln -sf /dev/stdout /var/log/nginx/access.log \ &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile EXPOSE]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-EXPOSE%2F</url>
    <content type="text"><![CDATA[EXPOSE用法1EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...] 例子123EXPOSE 80/tcpEXPOSE 80/udpEXPOSE 80 443 声明需要暴露的端口（缺省tcp），仅仅是声明并没有说写了它才能映射端口，对容器网络不熟悉的话后面会讲容器网络的。我们可以看到nginx官方镜像的Dockerfile里有写80。1EXPOSE 80 我们假设简单的run起来让外部访问的话可以这样1docker run -d -p 80:80 nginx:alpine 这条命令是使用nginx:alpine镜像运行一个容器，把宿主机的80映射到容器的80端口上，我们可以访问宿主机ip:80就可以看到默认nginx的index页面，如果说是云主机80可能需要备案，可以改成81:80。可以自己把nginx官方dockerfile的EXPOSE删掉发现还可以映射的。 EXPOSE作用是告诉使用者应该把容器的哪个端口暴漏出去。另一个作用给docker run -P用的。1docker run -P nginx:alpine 会映射宿主机上随机没被bind的端口到EXPOSE的端口，例如 random_port:80]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ONBUILD]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ONBUILD%2F</url>
    <content type="text"><![CDATA[ONBUILD用法1ONBUILD [INSTRUCTION] 构建的时候并不会执行，只有在构建出来的镜像被FROM的时候才执行，例如12FROM xxxxONBUILD RUN cd /root/ &amp;&amp; wget xxxx 然后构建出镜像B里root目录并没有下载东西，只有FROM B构建的镜像才会执行这个RUN，这个用得很少，记住即可]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile USER]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-USER%2F</url>
    <content type="text"><![CDATA[USER两种写法12USER &lt;user&gt;[:&lt;group&gt;] orUSER &lt;UID&gt;[:&lt;GID&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。可用可不用。 不用的情况建议给容器的最终进程指定用户去运行，例如nginx官方添加了一个不登陆的nginx用户，配置文件里指定使用这个用户运行nginx。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile WORKDIR]]></title>
    <url>%2F2019%2F04%2F30%2FDo%2F</url>
    <content type="text"><![CDATA[WORKDIR声明后续指令的工作目录，目录不存在则创建，可以理解为mkdir -p dir &amp;&amp; cd dir 1WORKDIR /path/to/workdir 可以在a中多次使用Dockerfile。如果提供了相对路径，则它将相对于前一条WORKDIR指令的路径 。例如：1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 最终pwd命令的输出Dockerfile将是 /a/b/c]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ADD]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ADD%2F</url>
    <content type="text"><![CDATA[ADD和COPY一样，但是源可以是一个url会自动下载，另外源是压缩包的话会自动解压，但是实际中不会使用它，因为前面讲RUN的时候说的层概念。例如下面是一个ADD用的多的举例1234ADD https://xxxxx/name.tar.gz /home/test/RUN cd /home/test &amp;&amp; \ 编译安装... \ rm -rf /home/test ADD下载源码包，然后RUN里编译安装完删除源码包。实际上后面的层起来的容器虽说读取不到源码包了，但是还是在镜像里，参照我之前的RUN里那个test.html的例子。 一般避免多余的层和容量都是RUN里去下载源码包，处理完后删掉源码包，参照nginx的dockerfile的第一个RUN。 https://github.com/nginxinc/docker-nginx/blob/7d7c67f2eaa6b2b32c718ba9d93f152870513c7c/mainline/alpine/Dockerfile#L7]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile COPY]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-COPY%2F</url>
    <content type="text"><![CDATA[COPY用法123COPY &lt;src&gt; &lt;dest&gt; COPY ["&lt;src&gt;",... "&lt;dest&gt;"] COPY home* /home 复制本地的文件到容器中的目录，目录不存在则会自动创建，源可以是多个。在低版本的docker里如果源是绝对路径例如/root/data/nginx的话会把整个系统的根上传到docker daemon，会发现上传的内容等同于根的已用容量，例如下面12345$ cat DockerfileFROM alpineCOPY /root/data/nginx.tar.gz /root/home$ docker build -t test .Sending build context to Docker daemon 7.8GB 主要是因为上下文的概念，认为上下文的根是client的/，所以会把客户端的/上传到docker daemon，现在新版本是强制相对路径了，如果是绝对路径会报错。相对路径相对于build最后的.这个上下文路径为相对路径。 另外COPY还能指定uid:gid，如果容器的rootfs里没有文件/etc/passwd和/etc/group文件只能使用数字不能使用组名。1234COPY --chown=55:mygroup files* /somedir/COPY --chown=bin files* /somedir/COPY --chown=1 files* /somedir/COPY --chown=10:11 files* /somedir/ COPY接受一个标志–from=&lt;name|index&gt;，该标志可用于将源位置设置为FROM .. AS 主要用于多阶段构建，后面会举个例子来讲解多阶段构建，多阶段构建是17.05之后才出现的功能。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile RUN]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-RUN%2F</url>
    <content type="text"><![CDATA[RUN有两种形式 RUN command ( 该命令在shell中运行，默认情况下在Linux上是/bin/sh -c或windows的cmd /S /C) RUN [“executable”, “param1”, “param2”] (exec 形式) exec形式不会调用shell先展开变量，也就是不会解析ENV或者ARG的变量，所以一般来讲用得比较多的就是第一种形式，多行的话可以利用\换行。12345RUN .....\ &amp;&amp; addgroup -S nginx \ &amp;&amp; adduser -D -S -h /var/cache/nginx -s /sbin/nologin -G nginx nginx \ &amp;&amp; apk add --no-cache --virtual .build-deps \ ..... 这里要注意的是一个RUN是一层，dockerfile的一些涉及到文件的指令和RUN都会是新的一层，主要是构建过程实际上还是容器去commit，目的相同的RUN尽量合并在同一个RUN里减少大小。下面我做个例子来说明原因123FROM alpineRUN apk add wget &amp;&amp; wget https://www.baidu.com -O test.htmlRUN echo 123 &gt; test.html 构建并运行123456789101112131415161718192021222324252627282930# docker build -t test .Sending build context to Docker daemon 2.048kBStep 1/3 : FROM alpine ---&gt; cdf98d1859c1Step 2/3 : RUN apk add wget &amp;&amp; wget https://www.baidu.com -O test.html ---&gt; Running in 07bd55d265b8fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gzfetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz(1/1) Installing wget (1.20.3-r0)Executing busybox-1.29.3-r10.triggerOK: 6 MiB in 15 packages--2019-04-30 05:38:10-- https://www.baidu.com/Resolving www.baidu.com... 58.217.200.39, 58.217.200.37Connecting to www.baidu.com|58.217.200.39|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 2443 (2.4K) [text/html]Saving to: 'test.html' 0K .. 100% 21.8M=0s2019-04-30 05:38:10 (21.8 MB/s) - 'test.html' saved [2443/2443]Removing intermediate container 07bd55d265b8 ---&gt; 9420c50ef6f7Step 3/3 : RUN echo 123 &gt; test.html ---&gt; Running in 8724c012ff49Removing intermediate container 8724c012ff49 ---&gt; b924abffdb62Successfully built b924abffdb62Successfully tagged test:latest 运行然后查看docker的存储目录查找123456789[root@docker ~]# docker run --rm test cat test.html123[root@docker ~]# find /var/lib/docker/overlay2/ -type f -name test.html/var/lib/docker/overlay2/3c4530c7cd077e1d6ec74135679fe7234eddc88fe72ada21f632cebfd26de4f5/diff/test.html/var/lib/docker/overlay2/802018b95e4f9b16e9946e2e827db5c3b0cd8631ac0759c31dffea212ff06d4f/diff/test.html[root@docker ~]# cat /var/lib/docker/overlay2/3c4530c7cd077e1d6ec74135679fe7234eddc88fe72ada21f632cebfd26de4f5/diff/test.html123[root@docker ~]# cat /var/lib/docker/overlay2/802018b95e4f9b16e9946e2e827db5c3b0cd8631ac0759c31dffea212ff06d4f/diff/test.html&lt;!DOCTYPE html&gt; 发现两个文件都存在，前面说到了容器在读取文件的时候从上层往下查找，查找到了就返回，但是我的这个Dockerfile里第一个RUN下载了index页面，第二个改了文件内容。 可以证明一个RUN是一层，也证明了之前容器读取文件的逻辑。同时假设我们的目的是最终的123，我们可以俩个RUN合并了，这样就不会有多余的第一个RUN产生的test.html文件。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ARG]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ARG%2F</url>
    <content type="text"><![CDATA[ARG格式有两种123456ARG key----------------ARK key=valueARG key=value \ key2=value2 \ key3=value3 一般来讲第二种用得多，表明build的时候不传入变量设置默认值，无值就是第一种下用户在docker build的时候必须传入值，否则就报错。例如我们可以把nginx官方dockerfile的第一个ENV改成ARG，我们想构建哪个版本直接build的时候传入变量就行了。 当然ARG是唯一一个可以用于FROM前面的指令，例如下面这样我们可以通过命令行传递参数来改变FROM的base镜像。12ARG jdk=1.8xxxxFROM openjdk:$jdk Docker其实也预定了一些ARG方便我们构建的时候使用代理 HTTP_PROXY HTTPS_PROXY FTP_PROXY NO_PROXY]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile MAINTAINER]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-MAINTAINER%2F</url>
    <content type="text"><![CDATA[MAINTAINER已经弃用，推荐使用LABEL，例如nginx dockerfile里的 LABEL maintainer=”NGINX Docker Maintainers &#100;&#111;&#99;&#x6b;&#101;&#x72;&#x2d;&#x6d;&#x61;&#105;&#x6e;&#x74;&#x40;&#x6e;&#x67;&#105;&#110;&#x78;&#x2e;&#99;&#111;&#x6d;“]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile ENV]]></title>
    <url>%2F2019%2F04%2F30%2FDockerfile-ENV%2F</url>
    <content type="text"><![CDATA[ENV写法有两种，后者支持写多个，一般多个的话也是使用后者居多1234567ENV key value-------ENV key=value key2=value2ENV key=value \ key2=value2 \ key3=value3 \ key4=value4 设置一个环境变量，可以被dockerfile里后续的指令使用，也在容器运行过程中保持，支持的指令为:1ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR ONBUILD 可以在dockerhub上发现各种官方镜像的Dockerfile的步骤都是固定的，新版本发布直接改下ENV后构建下即可1234567ENV NGINX_VERSION 1.15.11RUN .... \ &amp;&amp; curl -fSL https://nginx.org/download/nginx-$NGINX_VERSION.tar.gz -o nginx.tar.gz \ ... &amp;&amp; cd /usr/src/nginx-$NGINX_VERSION \ &amp;&amp; ./configure $CONFIG --with-debug \ ... 很多应用镜像启动都是先启动一个脚本，拼接一堆参数最终传递给应用的主进程当作参数，最常见的就是tomcat，或者说很多的应用基于tomcat。下面是之前我修改一个镜像Dockerfile摸索出的启动脚本的运行过程1234567891011+ '[' -r /opt/atlassian/jira/bin/setenv.sh ']'+ . /opt/atlassian/jira/bin/setenv.sh$ cat /opt/atlassian/jira/bin/setenv.sh...JAVA_OPTS="-Xms$&#123;JVM_MINIMUM_MEMORY&#125; -Xmx$&#123;JVM_MAXIMUM_MEMORY&#125; $&#123;JVM_CODE_CACHE_ARGS&#125; $&#123;JAVA_OPTS&#125; $&#123;JVM_REQUIRED_ARGS&#125; $&#123;DISABLE_NOTIFICATIONS&#125; $&#123;JVM_SUPPORT_RECOMMENDED_ARGS&#125; $&#123;JVM_EXTRA_ARGS&#125; $&#123;JIRA_HOME_MINUSD&#125; $&#123;START_JIRA_JAVA_OPTS&#125;"...export JAVA_OPTS...exec java $JAVA_OPTS 其中有一行：1JAVA_OPTS="... $&#123;JAVA_OPTS&#125; ..." 他拼接了自己，如果想给java在最终参数后添加一些固定参数时，可以在构建镜像声明JAVA_OPTS，例如添加时区我们应该在Dockerfile里设置1ENV JAVA_OPTS='-Duser.timezone=GMT+08' docker run可以指定env，ENV指令不一样是给Dockerfile用的，有时候是给容器启动时候用的，我们可以在docker run的时候指定env或者覆盖env达到不需要修改镜像，例如常见的后端需要连接一个mysql，可以在后端代码os.getEnv(“mysql_address”)，我们启动的时候指定mysql_address变量为真实的mysql地址即可。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile FROM]]></title>
    <url>%2F2019%2F04%2F29%2FDockerfile-FROM-1%2F</url>
    <content type="text"><![CDATA[FROM用法1FROM &lt;baseimage&gt; 或者 FROM &lt;baseimage&gt;:&lt;tag&gt; 指定从哪个镜像为基础迭代，如果本地没有镜像则会从仓库拉取，通常是第一行，而scratch是空镜像，是所有rootfs和一些单独可执行文件做镜像的根源，关于scratch后续会说。 例如centos的Dockerfile是下面12345678910FROM scratchADD centos-7-x86_64-docker.tar.xz /LABEL org.label-schema.schema-version="1.0" \ org.label-schema.name="CentOS Base Image" \ org.label-schema.vendor="CentOS" \ org.label-schema.license="GPLv2" \ org.label-schema.build-date="20190305"CMD ["/bin/bash"] 而hello-world为123456FROM scratchCOPY hello /CMD ["/hello"]docker images | grep hellohello-world latest fce289e99eb9 3 months ago 1.84kB nginx:alpine镜像的dockerfile 链接为https://github.com/nginxinc/docker-nginx/blob/7d7c67f2eaa6b2b32c718ba9d93f152870513c7c/mainline/alpine/Dockerfile，大家可以仿照这个经典案例写出自己的Dockerfile。 nginx:alpine既满足运行的最小环境下大小又很小，主要归功于FROM alpine ，现在alpine这个系统和rootfs得益于docker发展非常快，也有很多应用镜像都有alpine版本。 12345678910111213141516FROM alpine:3.9LABEL maintainer="NGINX Docker Maintainers &lt;docker-maint@nginx.com&gt;"ENV NGINX_VERSION 1.15.11RUN ...省略步骤，步骤是下载源码，安装编译需要的依赖，编译安装完删掉源码包和编译的依赖保留运编译出来的nginx二进制和需要的所有so文件COPY nginx.conf /etc/nginx/nginx.confCOPY nginx.vh.default.conf /etc/nginx/conf.d/default.confEXPOSE 80STOPSIGNAL SIGTERMCMD ["nginx", "-g", "daemon off;"]]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像03]]></title>
    <url>%2F2019%2F04%2F29%2FDocker%E9%95%9C%E5%83%8F03%2F</url>
    <content type="text"><![CDATA[构建镜像构建镜像只有两种方式，docker build 和 docker commit。实际上docker build是调用的docker commit。不推荐手动去docker commmit运行的容器成镜像。所以主要讲docker build和dockerfile。 使用docker build 指定Dockerfile来完成一个新镜像的构建。命令格式一般为：1docker build [option] [-t &lt;image&gt;:&lt;tag&gt;] &lt;path&gt; 其中path指向的文件称为context（上下文），context包含docker build镜像过程中需要的Dockerfile以及其他的资源文件。执行build命令后执行流程如下： Docker client端 解析命令行参数，完成对相关信息的设置，Docker client向Docker server发送POST/build的HTTP请求，包含了所需的上下文文件。 Docker server端 创建一个临时目录，并将context指定的文件系统解压到该目录下 读取并解析Dockerfile 根据解析出的Dockerfile遍历其中的所有指令，并分发到不同的模块（parser）去执行 parser为Dockerfile的每一个指令创建一个对应的临时容器，在临时容器中执行当前指令，然后通过commit使用此镜像生成一个镜像层 Dockerfile中所有的指令对应的层的集合，就是此次build后的结果。如果指定了tag参数，便给镜像打上对应的tag。最后一次commit生成的镜像ID就会作为最终的镜像ID返回。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像02]]></title>
    <url>%2F2019%2F04%2F29%2FDocker%E9%95%9C%E5%83%8F02%2F</url>
    <content type="text"><![CDATA[容器是单独的一层读写层一个镜像可以运行无数个容器，容器需要读取文件的场景和对应原理是如下。 在无挂载卷情况下，通过docker cp或exec产生的数据，文件会在读写层里，删除容器则文件也一并删除。 读取文件，从上层往下找到镜像层，找到即可返回，复制到容器层读入内存。 修改文件，从上层往下找到镜像层，找到即可返回，复制到容器层后修改。 删除文件，找到后在容器层记录下删除操作(类似盖层布，后续读取的时候会认为文件不存在) 容器与镜像关系为下图 通过docker ps 的-s选项可以看出容器的size和容器层总大小，这里我用docker命令演示下容器是单独一层读写层和容器被删除后数据消失。 创建一个容器，在容器里写入1g数据，宿主机的可用容量减少1G，docker的overlay2存储目录记录了下这个文件，但是删除后文件也被删除了。在抽象逻辑上一个容器就是单独一个读写层，而删除容器后这层在宿主机上的文件也会被删除。 计算实际占用大小时镜像的大小不会被重复计算，只需要计算一个大小+它起的所有容器大小。目前启动了5个nginx1234567# docker ps -sCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZEac5c0d1f8d6b nginx:alpine "nginx -g 'daemon of…" 7 seconds ago Up 5 seconds 80/tcp web5 2B (virtual 16.1MB)0dd2c0c36084 nginx:alpine "nginx -g 'daemon of…" 11 seconds ago Up 9 seconds 80/tcp web4 2B (virtual 16.1MB)413d9270c702 nginx:alpine "nginx -g 'daemon of…" 15 seconds ago Up 13 seconds 80/tcp web3 2B (virtual 16.1MB)43f8e010f7bb nginx:alpine "nginx -g 'daemon of…" 19 seconds ago Up 17 seconds 80/tcp web2 2B (virtual 16.1MB)610abcfab29d nginx:alpine "nginx -g 'daemon of…" 25 seconds ago Up 23 seconds 80/tcp web1 2B (virtual 16.1MB) 使用exec往容器里写数据123456789101112docker exec web1 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=10'docker exec web2 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=20'docker exec web3 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=30'docker exec web4 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=40'docker exec web5 sh -c 'dd if=/dev/zero of=/test.log bs=1000000 count=50']# docker ps -asCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES SIZEac5c0d1f8d6b nginx:alpine "nginx -g 'daemon of…" 56 seconds ago Up 54 seconds 80/tcp web5 50MB (virtual 66.1MB)0dd2c0c36084 nginx:alpine "nginx -g 'daemon of…" About a minute ago Up 58 seconds 80/tcp web4 40MB (virtual 56.1MB)413d9270c702 nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 80/tcp web3 30MB (virtual 46.1MB)43f8e010f7bb nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 80/tcp web2 20MB (virtual 36.1MB)610abcfab29d nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 80/tcp web1 10MB (virtual 26.1MB) 实际占用量计算123456781 x 16.1MB 只读镜像层1 x 10MB1 x 20MB1 x 30MB1 x 40MB1 x 50MB===========================161.1MB 这样我们可以推导出docker镜像是分层和容器是单独一层只读镜像的。也有部分人不懂这些知识，每次是进容器里安装东西然后commit，导致最后容器越来越大，甚至看到过16g的镜像。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像01]]></title>
    <url>%2F2019%2F04%2F29%2FDocker%E9%95%9C%E5%83%8F01%2F</url>
    <content type="text"><![CDATA[Docker镜像层镜像为什么是有层的？镜像分层是为了解决什么？ 虽然镜像解决了打包，但是实际应用中我们的应用都是基于同一个rootfs来打包和迭代的，难道每个rootfs都会多份吗？ 为此docker利用了存储驱动AUFS，devicemapper，overlay，overlay2的存储技术实现了分层。初期是AUFS，到现在的overlay2驱动（不推荐devicemapper坑很多）。例如一个nginx:alpine和python:alpine镜像可以从分层角度这样去理解。 实际上只有不同的层才占据存储空间，相同的层则是引用关系。抽象地看镜像是一个实体，实际上是/var/lib/docker目录里的分层文件外加一些json和db文件把层联系起来组成了镜像。存储路径是/var/lib/docker/存储驱动类型/。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[proxmox中cloud-init使用方法]]></title>
    <url>%2F2019%2F04%2F18%2Fproxmox%E4%B8%ADcloud-init%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[模版制作标准分区ext4，不添加swap分区，原因下文说。 系统装完后，将网卡配置文件内的onboot打开，清除uuid。关闭selinux和firewalld以及碍事的NetworkManager。123systemctl disable --now firewalld NetworkManagersetenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config 为了让虚拟化层可以重启和关闭虚拟机，必须安装acpid服务；为了使根分区正确调整大小安装cloud-utils-growpart，cloud-init支持下发前设置信息写入。12yum install -y acpid cloud-init cloud-utils-growpartsystemctl enable acpid 禁用默认zeroconf路线1echo "NOZEROCONF=yes" &gt;&gt; /etc/sysconfig/network 防止ssh连接使用dns导致访问过慢12sed -ri '/UseDNS/&#123;s@#@@;s@\s+.+@ no@&#125;' /etc/ssh/sshd_configsystemctl restart sshd cloud-init配置文件:1. ssh_pwauth 为 0 是禁止使用password登陆。2. disable_root：1 是禁止root登陆。3. package-update-upgrade-install会在第一次开机启动时自动yum update -y。123sed -ri '/disable_root/&#123;s#\S$#0#&#125;' /etc/cloud/cloud.cfgsed -ri '/ssh_pwauth/&#123;s#\S$#1#&#125;' /etc/cloud/cloud.cfgsed -ri '/package-update/s@^@#@' /etc/cloud/cloud.cfg 默认cloud-init会创建一个系统类型的用户,可以注释掉。1234567# default_user:# name: centos# lock_passwd: true# gecos: Cloud User# groups: [wheel, adm, systemd-journal]# sudo: ["ALL=(ALL) NOPASSWD:ALL"]# shell: /bin/bash 安装些基础包和预设一些脚本的话就可以关机。12yum install vim git wget -ypoweroff 转换模版12345678root@pve:~# qm list VMID NAME STATUS MEM(MB) BOOTDISK(GB) PID 100 cloud-init stopped 2048 20.00 0 101 k8s-m1 running 2048 20.00 7438root@pve:~# qm set 100 --ide2 local-lvm:cloudinitupdate VM 100: -ide2 local-lvm:cloudinit Using default stripesize 64.00 KiB. Logical volume "vm-100-cloudinit" created. 在Dashboard上可以看到虚拟机的could-init部分已经可以更改属性了。 在Dashboard上把它转换成模板,部署时完整克隆,开机之前双击需要设置的信息即可,否则例如密码不设置默认是模板的密码。也可以通过命令行初始化虚拟机信息。1qm set &lt;vmid&gt; --ipconfig0 ip=10.105.26.x/23,gw=10.105.26.1 备份和恢复虚拟机123456789101112131415161718192021222324252627282930313233root@pve:~# vzdump 100INFO: starting new backup job: vzdump 100INFO: Starting Backup of VM 100 (qemu)INFO: status = stoppedINFO: update VM 100: -lock backupINFO: backup mode: stopINFO: ionice priority: 7INFO: VM Name: cloud-initINFO: include disk 'scsi0' 'local-lvm:vm-100-disk-0' 20GINFO: creating archive '/var/lib/vz/dump/vzdump-qemu-100-2019_04_18-12_48_38.vma'INFO: starting kvm to execute backup taskTotal translation table size: 0Total rockridge attributes bytes: 417Total directory bytes: 0Path table size(bytes): 10Max brk space used 0178 extents written (0 MB)INFO: started backup task 'd65a8f26-20fe-4232-abd3-ec0bcf4623cd'INFO: status: 3% (785645568/21474836480), sparse 1% (395206656), duration 3, read/write 261/130 MB/sINFO: status: 21% (4593876992/21474836480), sparse 19% (4184059904), duration 6, read/write 1269/6 MB/sINFO: status: 34% (7457996800/21474836480), sparse 32% (6929133568), duration 9, read/write 954/39 MB/sINFO: status: 50% (10746396672/21474836480), sparse 46% (10083291136), duration 12, read/write 1096/44 MB/sINFO: status: 61% (13169524736/21474836480), sparse 57% (12349382656), duration 15, read/write 807/52 MB/sINFO: status: 70% (15039004672/21474836480), sparse 64% (13956280320), duration 18, read/write 623/87 MB/sINFO: status: 80% (17196580864/21474836480), sparse 74% (15934279680), duration 21, read/write 719/59 MB/sINFO: status: 89% (19120455680/21474836480), sparse 82% (17667883008), duration 24, read/write 641/63 MB/sINFO: status: 95% (20594622464/21474836480), sparse 88% (18997477376), duration 27, read/write 491/48 MB/sINFO: status: 100% (21474836480/21474836480), sparse 92% (19877691392), duration 28, read/write 880/0 MB/sINFO: transferred 21474 MB in 28 seconds (766 MB/s)INFO: stopping kvm after backup taskINFO: archive file size: 1.49GBINFO: Finished Backup of VM 100 (00:00:32)INFO: Backup job finished 输出路径在：/var/lib/vz/dump/，导入的话使用如下命令1qmrestore vzdump-qemu-xx.vma &lt;vmid&gt;]]></content>
      <categories>
        <category>Proxmox</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s配置secret拉取私有仓库镜像]]></title>
    <url>%2F2019%2F04%2F10%2Fk8s%E9%85%8D%E7%BD%AEsecret%E6%8B%89%E5%8F%96%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[https://kubernetes.io/docs 样例1234567891011# cat ~/.docker/config.json &#123; "auths": &#123; "harbor.station.com": &#123; "auth": "YWRtaW46SGFyYm9yMTIzNDU=" &#125; &#125;, "HttpHeaders": &#123; "User-Agent": "Docker-Client/18.06.1-ce (linux)" &#125;&#125; 这个时候我们虽然可以通过交互式登录，使用docker pull拉取镜像，但无法通过k8s创建Pod时拉取镜像。 生成密钥secret1# kubectl create secret docker-registry harbor --docker-server=x.x.x.x --docker-username=admin --docker-password=Harbor12345 --docker-email=xx@qq.com 1) harbor: 指定密钥的键名称，可自行定义 2）–docker-server：指定docker仓库地址 3）–docker-username：指定docker仓库帐号 4) –docker-password：指定docker仓库密码 5) –docker-email：指定邮件地址（选填）` 查看密钥可以看到当前除了默认的密钥, 还有我们刚才生成的. 另外要注意的是, 该密钥只能在对应namespace使用, 也就是这里的default, 如果需要用到其他namespace, 比如说test, 就需要在生成的时候指定参数 -n test。1234# kubectl get secretsNAME TYPE DATA AGEdefault-token-mzmtj kubernetes.io/service-account-token 3 22mharbor kubernetes.io/dockerconfigjson 1 22m YAML例子其中imagePullSecrets是声明拉取镜像时需要指定密钥, harbor必须和上面生成密钥的键名一致, 另外检查一下pod和密钥是否在同一个namespace, 之后k8s便可以拉取镜像。1234567891011121314151617apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-deploy namespace: harborspec: replicas: 3 template: metadata: labels: app: web_server spec: containers: - name: nginx image: harbor.station.com/library/nginx:latest imagePullSecrets: - name: harbor]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Harbor</tag>
      </tags>
  </entry>
</search>
